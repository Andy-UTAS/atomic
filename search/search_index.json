{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-home-of-atomic-physics","title":"Welcome to the home of atomic physics","text":"<p>This site exists to enhance the distribution of course information and content for the atomic physics component of KYA323: Atomic and Nuclear Physics. All official communication will be though MyLO<sup>1</sup>.</p> A next-generation trap for strontium ions, recently pulished in the journal nature . Trapped ion systems are promising candidates for quantum computation and quantum sensing, and this system, developed by the MIT Lincoln Laboratory demonstrates an important step in the process of making quantum devices suitable for the real-world, namely, making them \"simpler\" and more robust. As part of this course, we will investigate how atomic physics allows us to understand, model, and construct simple, yet extraordinarily powerful quantum systems. After constructing a foundation in the theory of applied quantum mechanics, we will turn our attention to real-world quantum applications and technology.  <p>Course expectations: my expectations of you</p> <p>Quantum mechanics is tough! And this isn't (necessarily) hyperbole: physics is often put in the \"hard\" basket for a number of reasons, not least due to it being seen as irrelevant or mathematically challenging. Whilst the mathematics can indeed by spicy, once can develop coping mechanisms for this, and at its heart physics is usually intuitive: the apple falls from the tree, like charges repel, and so on. At first glance, quantum mechanics can present very differently, and is rarely accused of being intuitive. There are delights are to be had, but like most truly rewarding endeavours, said delights do not come for free. In undertaking this course, it is expected that:</p> <ul> <li>You will attend all face-to-face sessions, and actively contribute to discussions and problem solving</li> <li>Prescribed problems and reading will be undertaken before any face-to-face sessions</li> <li>If you are experiencing difficulties, that you will get in contact as soon as is reasonably possible</li> </ul> <p>Course objectives</p> <p>The purpose of the atomic component of the atomic and nuclear physics course is to provide a deeper exploration of quantum theory, moving beyond toy examples and developing a toolkit for analysing systems which one would actually encounter in a lab. At the conclusion of your journey, you should:</p> <ul> <li>Be familiar with the fundamentals of atomic physics, notably atomic structure and atomic transitions</li> <li>Understand what makes a quantum system behave in a quantum way, and how one can construct experiments to harness the power of quantum mechanics</li> <li>Have studied diverse quantum applications, from how the GPS network functions, to how to discern different squishy bits in an MRI image, and how to build a quantum computer</li> <li>Have had some fun!</li> </ul> <p>Course expectations: my promises to you</p> <p>Rightly, you should have expectations of me. It is my intention that:</p> <ul> <li>I will work to communicate my understanding and insight in the course material</li> <li>I will actively seek input to steer and shape the content discussed, and develop relevant resources</li> <li>I will be available for consultation and discussion</li> <li>I will do my best to cultivate a safe and open forum for discussion</li> </ul> <p>If you think that I am not fulfilling my commitments, or if you have other comments, I would hope that you are comfortable conveying your concerns and/or comments to me, either directly or anonymously. It is my genuine desire to help you navigate learning new concepts and ideas, and I want to do this to the best of my ability, and feedback is the best way to ensure this endeavour proceeds as smoothly as possible.</p> <ol> <li> <p>For the curious traveller, MyLO is The University of Tasmania's learning management system, based on Brightspace as developed D2L.\u00a0\u21a9</p> </li> </ol>"},{"location":"0-1-intro/","title":"0.1 - Atomic physics 101","text":""},{"location":"0-1-intro/#atomic-physics-101","title":"Atomic physics 101","text":"<p>The fundamental laws necessary for the mathematical treatment of a large part of physics and the whole of chemistry are thus completely known, and the difficulty is only that the exact application of these laws leads to equations much too complicated to be soluble.</p> <p> Paul Dirac </p>"},{"location":"0-1-intro/#introduction","title":"Introduction","text":"<p>A motivating anecdote will soon appear here, and you will be enraptured upon reading as to want to learn more about atomic physics.</p> Futurama meme <p>A somewhat different tone, but both too relevant and too classic to pass up (from S03E14: Time Keeps On Slippin').</p> <p> </p> <p>The study of quantum mechanics is a wild ride: it is punctuated by brief periods of understanding and then profound lulls of nothing making sense. It is often stated that quantum mechanics is counter intuitive, or that it cannot be understood, and whilst there one can argue that a deeper level this is true, one can use the scientific method to probe the universe, and formulate theories which accurately predict what will take place (with extraordinary accuracy!). Repeated exposure to commonly encountered systems and physical situations can be used to cultivate an intuition for might be likely to take place. If you come seeking explanations for quantum phenomena through analogy with \"intuitive\" classical systems, you will leave (mostly) empty handed. The best thing one can do to harness the power of quantum mechanics is take one's time: really marinate in the content. Build on the foundations of your existing physics knowledge (e.g. wave mechanics) and embark on a journey to harness the best model we have for the natural world.</p> <p></p> <p>Image credits</p> <p>Header image taken from physics world.</p>"},{"location":"1-1-early/","title":"1.1 - Early atomic physics","text":""},{"location":"1-1-early/#early-atomic-physics","title":"Early atomic physics","text":"<p>Splitting the atom... Well, splitting atomic emissions into a spectrum</p>"},{"location":"1-1-early/#introduction","title":"Introduction","text":"<p>Our journey begins with what may be the most famous experiment in the context of quantum mechanics, which was undertaken in the early part of the twentieth century. The nascent ideas of quantum mechanics were being explored, and a delightfully simple experiment managed to break our understanding of how the world works. We shall explore the rich physics of this foundational experiment and use it as our foundation for introducing quantum mechanics using Dirac notation and matrix mechanics.</p> <p>Expected competencies</p> <p>It is assumed that you have familiarity with the following concepts/techniques:</p> <ul> <li>Basic spectroscopy</li> <li>Bohr's model of the atom</li> </ul> <p>Text reference</p> <p>The material covered here is discussed in section(s) \\(\\S1.1\\) of Quantum Mechanics, A Paradigms Approach (2nd edition)</p>"},{"location":"1-1-early/#the-year-was-1921","title":"The year was 1921...","text":"<p>A touch over 100 years ago, Otto Stern and Walther Gerlach performed the following experiment:</p> <ul> <li>Creating a beam of neutral silver atoms</li> <li>Passing the beam through an inhomogeneous magnetic field</li> <li>Measuring the beam profile after the magnetic field</li> </ul> <p>A schematic of the experiment is shown below:</p> <p> </p> A schematic of the Stern-Gerlach experiment <p>The experiment is conceptually very simple, and despite the experiment being extremely difficult to execute, they were able to obtain the following result:</p> <p> </p> \"Bohr was right after all\", Walther Gerlach <p>Without context, it is somewhat difficult to appreciate the Earth shattering nature of this image, so that is what we are going to explore.</p>"},{"location":"1-1-early/#the-classical-experiment","title":"The classical experiment","text":"<p>Given there is some interaction between the atoms and the magnetic field, this leads to us assuming that the atom possesses a magnetic moment magnetic moment \\boldsymbol{\\mu}. The energy of the interaction is given by</p> \\[ E = -\\boldsymbol{\\mu} \\cdot \\mathbf{B} \\] <p>where \\(\\mathbf{B}\\) is the magnetic field.</p> Question 1.1.1: What is the force due to this interaction? Write an expression for the force in the \\(z\\)-direction <p>The force is the negative gradient of the energy, which in this case yields</p> \\[ \\mathbf{F} = \\nabla \\left(\\boldsymbol{\\mu} \\cdot \\mathbf{B}\\right) \\] <p>and by the design of the experiment, we know the field is primarily in the \\(z\\)-direction and so</p> \\[ \\begin{align} F_z &amp; = \\frac{\\partial}{\\partial z}\\left(\\boldsymbol{\\mu} \\cdot \\mathbf{B}\\right) \\\\ &amp; \\approx \\mu_z \\frac{\\partial B_z}{\\partial z} \\end{align} \\] <p>Consequently, the force is largely perpendicular to the direction of beam propagation, with the amount of deflection being proportional to the component of the magnetic moment in the direction of the field gradient.</p> <p>The origin of a (classical) magnetic moment is either a separation of poles, or loops of current, and explicitly the magnitude of the magnetic moment \\(\\mu\\) is</p> \\[ \\mu = I \\times A \\] <p>where \\(I\\) is the current in the loop and \\(A\\) is the area of the loop. In the early 1920s, atomic model de jour was the Bohr model<sup>1</sup>, where atoms consist of charges in discrete energy shells, and so we can meaningfully talk about a charge \\(q\\) moving at speed \\(v\\) around the loop of a circle of radius \\(r\\).</p> Question 1.1.2: Show that the magnitude of the magnetic moment is given by \\(qrv/2\\) <p>This is plug and play from \\(\\mu = I \\times A\\), requiring only that one the definition of current:</p> \\[ \\begin{align} \\mu &amp; = I \\times A \\\\ &amp; = \\frac{q}{2\\pi r / v} \\times \\pi r^2 \\\\ &amp; = \\frac{qrv}{2} \\end{align} \\] <p>From rotational mechanics, we know the orbital the angular momentum \\(L = mvr\\), so we rewrite</p> \\[ \\mu = \\frac{q}{2m} L. \\] <p>We also take further inspiration from rotational mechanics in the form of an orbiting body can itself rotate (e.g. the earth spinning whilst rotating around the sun), so we assume our charged particle has</p> <ul> <li>Orbital angular momentum \\(\\mathbf{L}\\)</li> <li>Intrinsic angular momentum \\(\\mathbf{S}\\)</li> </ul> <p>both of which will contribute to the magnetic interaction. By analogy with the orbital angular momentum, we can propose a relation between the magnetic moment and the intrinsic angular momentum as</p> \\[ \\boldsymbol{\\mu} = g\\frac{q}{2m} \\mathbf{S} \\] <p>where \\(g\\) is the dimensionless \\(g-\\)factor which contains all of the juicy physics. Arriving at a theoretical value for this constant goes beyond the scope of this course<sup>2</sup>, but it turns out the value of \\(g\\) is approximately -2, and the deviation from -2 is hailed as one of the grand successes of the theory of Quantum Electrodynamics: the predicted value of</p> \\[ g_{\\mathrm{theory}} = -2.002319304363286 \\] <p>matches the experimental value of</p> \\[ g_{\\mathrm{exp}} = \u00e2\u02c6\u20192.00231930436256(35) \\] <p>very well.</p>"},{"location":"1-1-early/#long-shot-silver","title":"Long-shot silver","text":"<p>We must take a brief sojourn from physics fundamentals to look at the practicalities of the experiment, such that we can realistically model the system.</p> Question 1.1.3: How many common isotopes of silver are there and how many neutrons are in each isotope? How does the existence of these isotopes alter our analysis? <p>The common isotopes of silver are \\(^{107}\\)Ag and \\(^{109}\\)Ag, having 60 and 62 neutrons respectively. Impressively, these isotopes occur with near-similar abundances at \\(51.8%\\) and \\(48.2%\\) respectively.</p> <p>In all cases, the magnetic moment depends on the inverse of the particle mass, so for both protons and neutrons which have a mass some thousands of times bigger, we can simply ignore<sup>3</sup> thier contribution to the magnetic moment of the atom.</p> Question 1.1.4: What is the electronic configuration of Silver? How many unpaired electrons are in the outer shell? What are the consequneces for our (the Stern-Gerlach) experiment? Write an expression of the magnetic moment of neutral silver. <p>The electronic confiuration for silver is</p> \\[ \\mathrm{Ag} = 1\\mathrm{s}^2 2\\mathrm{s}^2 2\\mathrm{p}^6 3\\mathrm{s}^2 3\\mathrm{p}^6 4\\mathrm{s}^2 3\\mathrm{d}^{10} 4\\mathrm{p}^6 4\\mathrm{d}^{10} 5\\mathrm{s}^1 \\] <p>or equivalently</p> \\[ \\mathrm{Ag} = [\\mathrm{Kr}] 4\\mathrm{d}^{10} 5\\mathrm{s}^1 \\] <p>meaning that there is only a single unpaired electron in the outer shell, and it is also in the ground state, which has an isotropic distribution and thus no orbital angular momentum, meaning only the intrinsic angular momentum of the electron will contribute to the interaction.</p> <p>The magnetic moment for the silver atom is then</p> \\[ \\boldsymbol{\\mu} = -g\\frac{e}{2 m_e} \\mathbf{S} \\] <p>where \\(e\\) is the magnitude of the electron charge and \\(m_e\\) is the electronic mass.</p> <p>Are we baking in the result for the classical analysis by using what amounts to quantum info?</p> <p>Yes, but this needn't be the case; however, it really simplifies the discussion. The Stern-Gerlach experiment was done with neutral silver, so we seek to analyse this result; but we could look at the case of Hydrogen (which was done afterwards) or the plethora of other systems, but multielectron systems do require a bit of modern knowledge to make the classical analysis fit together. An interesting thought is why don't we do the experiment or analysis with electrons themselves?</p> <p>The force in the the \\(z-\\) direction for a silver atom is thus</p> \\[ \\begin{align} F_z &amp; \\approx \\mu_z \\frac{\\partial B_z}{\\partial z} \\\\ &amp; = - g \\frac{e}{2 m_e} S_z \\frac{\\partial B_z}{\\partial z} \\end{align} \\] <p>and directly, we have the deflection of the beam through the apparatus being a direct measurement of the \\(z\\) component of the spin along the axis of the magnetic field gradient.</p> Question 1.1.5: Make a prediction of the distribution of deflected silver atoms you would expect for a thermal source. <p>A reasonable assumption one can make is that the intrinsic angular momentum for each \\(5\\mathrm{s}\\) electron has the same magnitude, and thus we can write the \\(z-\\)component as</p> \\[ S_z = |\\mathbf{S}|\\cos\\left(\\theta\\right) \\] <p>where \\(\\theta\\) is the angle between the \\(z-\\)axis and the spin vector \\(\\mathbf{S}\\). For a thermal beam, we would expect all values of \\(\\theta\\), with the explicit angular flux to be determined by the apparatus; however, the from of this being largely unimportant save for the fact we expect all values of \\(\\theta\\) to be present. We therefore would expect a continuous range of spin projections, ranging from \\(S_z \\in [-|\\mathbf{S}|, |\\mathbf{S}|]\\)</p>"},{"location":"1-1-early/#the-results","title":"The results","text":"Reprint: the experimental results of the original Stern-Gerlach experiment <p>We can now return to the results as recorded by Stern and Gerlach. If we look at the image without the magnetic field gradient, we see the silver beam form a line (which is due to the geometry of experiment) and when the gradient is switched on, only two projections along the field gradient are observed, which indicates only two values of the \\(z-\\) component of the electron spin are possible. The magnitude of these deflections are consistent with the values of the spin component of</p> \\[ S_z = \\pm \\frac{\\hbar}{2} \\] <p>where the reduced Planck constant is \\(\\hbar = h/2\\pi\\) and Plank's constant is \\(h=6.62607015 \\times 10^{-34}~\\mathrm{J \\cdot Hz^{-1}}\\). The Stern-Gerlach experiment is evidence of the quantisation of the electron spin angular momentum along an axis.</p> <p>The quantisation axis</p> <p>In our working here, we have chosen the \\(z-\\)axis to be the direction along which we measure the spin component, but we could have equally picked any other axis and observed the same result. It is a well-observed convention to define the axis of the magnetic field (gradient) to be along \\(z\\), and we are not going to start rocking the boat.</p>"},{"location":"1-1-early/#a-general-stern-gerlach-experiment","title":"A general Stern-Gerlach experiment","text":"<p>We are going to regroup: with your socks (hopefully) having been blown off by the above result, we are going to move into a generalised framework to consider these kinds of experiments - and yes, should you need to take a minute to collect you socks, please do so now.</p>"},{"location":"1-1-early/#details-in-the-bin","title":"Details in the bin","text":"<p>We are going to strip back the Stern-Gerlach experiment to the core features, which consists of the following:</p> <ul> <li>A beam of atoms</li> <li>A Stern-Gerlach device which analyses the component of spin along a given axis</li> </ul> <p>A schematic of the system is shown below:</p> <p> </p> A schematic of the simplified Stern-Gerlach system <p>We are also going to label the output ports of our analyser: the up and down arrows (\\(\\uparrow\\) and \\(\\downarrow\\)) indicate the possible measurement results for the analyser, which correspond to the measurements</p> \\[ S_z = \\pm \\frac{\\hbar}{2} \\] <p>and as there are only two results, we can refer to these as spin up and spin down. The thing that we are measuring, the projection of \\(\\mathbf{S}\\) onto the \\(z-\\)axis (\\(S_z\\)) is the observable, i.e. the thing we are measuring.</p>"},{"location":"1-1-early/#the-quantum-state","title":"The quantum state","text":"<p>When we talk about the beams in our system, be it the input beam or the beams after the analyser, we are describing quantum states. In the first part of this course, you will have encountered quantum mechanical states in the context of wavefunctions, which are solutions to the Schr\u00c3\u00b6dinger equation. Here we adopt a more general representation of quantum states, which is not limited to the degrees of freedom chosen to represent the wavefunction (e.g. position/momentum space) but rather talk about a state as an object containing all of the information that we can know about the system. Mathematically, we represent states using  Dirac notation; in the case of our spin up and spin down states, we label these \\(|+\\rangle\\) and \\(|-\\rangle\\), where we have introduced a new symbol to demarcate a quantum state, the ket. We will delve deeper into these objects later, but for the moment it sufficient to know that a general state is mathematically described by the ket \\(|\\psi\\rangle\\).</p> <p>Uniqueness of ket labels</p> <p>A quantum state is described by a ket, but the label in the ket is not unique. For example, we might label the spin-up state as</p> <ul> <li>\\(| + \\rangle\\)</li> <li>\\(| S_z = +\\hbar/2 \\rangle\\)</li> <li>\\(| +\\mathbf{\\hat{z}} \\rangle\\)</li> <li>\\(| \\uparrow \\rangle\\)</li> <li>\\(\\ldots\\)</li> </ul> <p>where the label is not important; in all cases, we are talking about the state with a projection of \\(+\\hbar/2\\) through our Stern-Gerlach analyser.</p> <p>Postulate 1</p> <p>The state of a quantum mechanical system - the entirety of information that you can know about it - is represented mathematically by a normalised ket \\(|\\psi\\rangle\\)</p> <p>We now seek to perform a series of experiments which the systems will behave exactly as expected and no strange behaviour will take place.</p> <p>Experiment simulator</p> <p>An online simulation is available to allow you to play along with the following experiments.  </p>"},{"location":"1-1-early/#experiment-one","title":"Experiment one","text":"<p>The following set of experiments are conducted by sending our beam through various analysers. An example of such an experiment from our simple setup above is shown below:</p> <p> </p> The simplified Stern-Gerlach experiment with proportions of atoms detected from the output ports <p>In our first example, we are going to stack two analysers aligned along the \\(z-\\)axis. The beam will be split by the first analyser, and we then take the spin up component of the beam and use this as the input to a second analyser, meaning the spin component is again analysed.</p> <p>Experiment time</p> Experimental setupExperimental results <p> What will the proportion of particles detected at the output ports? </p> <p> </p> <p>This result is perhaps unsurprising, as the first analyser measures an atom to have a \\(z-\\)component of spin \\(S_z = +\\hbar/2\\), and the 2nd analyser also measures \\(S_z = +\\hbar/2\\) for these atoms. It should be emphasised that in this scenario, the first analyser can be considered to be preparing the quantum state: if we have a mixture of spin up and spin down atoms before the analyser, atoms from a given output port will be either spin up or spin down.</p>"},{"location":"1-1-early/#experiment-two","title":"Experiment two","text":"<p>We are now going to preform the same experiment as experiment one with the exception being that we are going to replace the second analyser with an analyser which is aligned to the \\(x-\\)axis, meaning that we now measure the \\(x\\) component of the spin \\(S_x\\) rather than \\(S_z\\).</p> <p>Experiment time</p> Experimental setupExperimental results <p> What will the proportion of particles detected at the output ports? </p> <p> </p> <p>At the top port of the first analyser, we still have atoms in the spin up state \\(|+\\rangle\\) as per the previous example, but following the second analyser, we have atoms which have components \\(S_x = +\\hbar/2\\) and \\(S_x = -\\hbar/2\\), which we denote by \\(|+\\rangle_x\\) and \\(|-\\rangle_x\\) respectively. Points of note from this experiment are</p> <ul> <li>Even though we have changed the orientation of the analyser, there are still only two possible values for the projection of the spin</li> <li>Results for this experiment would be unchanged if we had taken the spin down output from the first analyser</li> <li>Critically: for any given atom, we cannot predict the output of the second analyser. We know that there will be a \\(50\\%\\) probability of an atom exiting via a specific port, but nothing more</li> </ul> <p>Quantum mechanics is inherently probabilistic: we cannot know the outcome of a given measurement without making said measurement. In the development of quantum mechanics, it was postulated that whilst measurements appeared to be probabilistic, there was actually some other variable which was underwriting the system, which if known would allow us to conclusively predict results. This so-called hidden variable theory of quantum mechanics was shown to be incompatible with observed results<sup>4</sup>, something we shall discuss in detail later in the course.</p>"},{"location":"1-1-early/#experiment-three","title":"Experiment three","text":"<p>We shall now extend experiment two with the addition of a third analyser, again aligned along the \\(z-\\)axis, meaning we are measuring the component of the spin along the \\(z\\), \\(x\\), and \\(z\\) axes respectively.</p> <p>Experiment time</p> Experimental setupExperimental results <p> What will the proportion of particles detected at the output ports? </p> <p> </p> <p></p> <p>If this result does not cause a double take, then marinate in it a bit longer until that happens. We have measured \\(S_z = +\\hbar/2\\), then we have taken a measurement of \\(S_x\\), and then immediately taken another measurement of \\(S_z\\), but the result is now a mixture of \\(S_z = +\\hbar/2\\) and \\(S_z = -\\hbar/2\\). Somehow, by measuring \\(S_x\\), we have erased knowledge of \\(S_z\\).</p> <p>A key feature of quantum mechanics is that making a measurement fundamentally alters the system. These experiments are designed to illustrate that there is a fundamental incompatibility between measurements of different spin components, or formally: that \\(S_x\\) and \\(S_z\\) are incompatible observables. This means that we cannot these values simultaneously.</p> <p>Compatible versus incompatible observables</p> <p>In this case we see that \\(S_z\\) is incompatible with \\(S_x\\) (and also \\(S_y\\)), but this does not mean that all pairs of observables are incompatible. It is an important aspect of quantum mechanics that we can make certain measurements without altering other aspects of the system. We shall discuss this in detail later, but for now it is sufficient to say that there are both sets of compatible observables and incompatible observables.</p>"},{"location":"1-1-early/#experiment-four","title":"Experiment four","text":"<p>For our final experiment, we are going to repeat experiment three but we are going to alter the beam as it comes out of analyser number 2: namely change what goes into analyser number three.</p> <p>Experiment time</p> Experimental setupSpin up onlySpin down onlyCombined spin up and spin down <p> What will the proportion of particles detected at the output ports? </p> <p> </p> <p>This is exactly the same as experiment three.</p> <p> </p> <p>This is in effect, exactly the same as experiment three.</p> <p> </p> <p></p> <p>This result is perhaps the strangest of all: by recombining the outputs from the second analyser, we have somehow made the atoms recall their state from the output of analyser number one! Classical probability theory cannot explain this aspect of quantum mechanics.</p> <p>Whilst these results may appear extremely foreign, you already have an intuition for what is going on: consider the canonical double-slit experiment. When light waves pass through slits, each slit produces a nearly uniform illumination of the screen, but when the two slits are allowed to combine, an interference pattern in observed. In order to describe this phenomenon, we consider the complex-valued electric fields from both sources, and ultimately calculate the intensity as the square of the field amplitude, and the nature of complex-valued fields permits interference effects. In this sense, quantum mechanical is no different to any other form of wave mechanics, but we must work to describe and calculate the amplitudes of the quantum mechanical waves.</p>"},{"location":"1-1-early/#conclusions","title":"Conclusions","text":"<p>The Stern-Gerlach experiment demonstrates some foundational concepts of quantum mechanics:</p> <ol> <li>Quantum mechanics is probabilistic</li> <li>Spin measurements are quantised</li> <li>Quantum measurements disturb the system</li> </ol>"},{"location":"1-1-early/#exercises","title":"Exercises","text":""},{"location":"1-1-early/#preliminary-provocations","title":"Preliminary provocations","text":"<ol> <li>Why use an inhomogeneous magnetic field?</li> <li>Why is the experiment done with silver atoms?</li> </ol>"},{"location":"1-1-early/#heavy-hitters","title":"Heavy hitters","text":"<p>1. 2.</p> <p>Image credits</p> <p>Header image taken from the PhD thesis Spinor Bose-Einstein condensates in magnetic field gradients</p> <ol> <li> <p>more accurately, at this time it was the Bohr-Sommerfeld model, which is an extension of the Bohr model to include elliptical orbits which fixed some problems. One will also see in solid-state physics that Sommerfeld has a real talent for tweaking existing theories to make them run a bit better.\u00a0\u21a9</p> </li> <li> <p>A study of the the relativistic wave equation, known as the Dirac equation, is required to fully flesh out this result.\u00a0\u21a9</p> </li> <li> <p>In this case we can, but more generally, the magnetic moment of the nucleus cannot be ignored (see LINK ME! hyperfine splitting)\u00a0\u21a9</p> </li> <li> <p>The 2022 Nobel Prize in physics was awarded for the pioneering work of making these measurements\u00a0\u21a9</p> </li> </ol>"},{"location":"1-2-hydrogen/","title":"1.2 - The hydrogen atom","text":""},{"location":"1-2-hydrogen/#the-hydrogen-atom","title":"The hydrogen atom","text":"<p>The \"simplest\" system</p>"},{"location":"1-2-hydrogen/#introduction","title":"Introduction","text":"<code>Lorem ipsum dolor sit amet</code> <p>Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis.</p>"},{"location":"1-2-hydrogen/#conclusions","title":"Conclusions","text":""},{"location":"1-2-hydrogen/#exercises","title":"Exercises","text":""},{"location":"1-2-hydrogen/#preliminary-provocations","title":"Preliminary provocations","text":"<ol> <li>"},{"location":"1-2-hydrogen/#heavy-hitters","title":"Heavy hitters","text":"<ol> <li> Find the expectation values for  for \\(k=-3,-2,-1, 1, 2\\)</li> </ol> <p>Image credits</p> <p>Header image taken from the paper Hydrogen Atoms under Magnification: Direct Observation of the Nodal Structure of Stark States</p> <ol> <li> <p>more accurately, at this time it was the Bohr-Sommerfeld model, which is an extension of the Bohr model to include elliptical orbits which fixed some problems. One will also see in solid-state physics that Sommerfeld has a real talent for tweaking existing theories to make them run a bit better.\u00a0\u21a9</p> </li> <li> <p>A study of the the relativistic wave equation, known as the Dirac equation, is required to fully flesh out this result.\u00a0\u21a9</p> </li> <li> <p>In this case we can, but more generally, the magnetic moment of the nucleus cannot be ignored (see LINK ME! hyperfine splitting)\u00a0\u21a9</p> </li> </ol>"},{"location":"1-3-methods/","title":"1.3 - Calculation methods for real systems","text":""},{"location":"1-3-methods/#state-vectors","title":"State vectors","text":"<p>Time to do the same experiment lots of times and get different results</p>"},{"location":"1-3-methods/#introduction","title":"Introduction","text":"<code>Lorem ipsum dolor sit amet</code> <p>Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis.</p>"},{"location":"1-3-methods/#conclusions","title":"Conclusions","text":""},{"location":"1-3-methods/#exercises","title":"Exercises","text":""},{"location":"1-3-methods/#preliminary-provocations","title":"Preliminary provocations","text":"<ol> <li>What happens if you happen to choose your variational wavefunction perfectly? That is, you trail wavefunction is an eigenstate of the Hamiltonian?</li> </ol> <p>  as you might expect</p>"},{"location":"1-3-methods/#heavy-hitters","title":"Heavy hitters","text":"<p>Image credits</p> <p>Header image taken from the PhD thesis Spinor Bose-Einstein condensates in magnetic field gradients</p> <ol> <li> <p>more accurately, at this time it was the Bohr-Sommerfeld model, which is an extension of the Bohr model to include elliptical orbits which fixed some problems. One will also see in solid-state physics that Sommerfeld has a real talent for tweaking existing theories to make them run a bit better.\u00a0\u21a9</p> </li> <li> <p>A study of the the relativistic wave equation, known as the Dirac equation, is required to fully flesh out this result.\u00a0\u21a9</p> </li> <li> <p>In this case we can, but more generally, the magnetic moment of the nucleus cannot be ignored (see LINK ME! hyperfine splitting)\u00a0\u21a9</p> </li> </ol>"},{"location":"2-1-hyperfine/","title":"2 1 hyperfine","text":"<pre>\n\nfrom matplotlib import pyplot\nimport numpy as np\n\nfrom common import draw_classic_axes, configure_plotting\n\nconfigure_plotting()\n\n  </pre>"},{"location":"2-1-hyperfine/#state-vectors","title":"State vectors","text":"<p>Time to do the same experiment lots of times and get different results</p>"},{"location":"2-1-hyperfine/#introduction","title":"Introduction","text":"<pre># Data from Einstein's paper\nT = [222.4, 262.4, 283.7, 306.4, 331.3, 358.5, 413.0, 479.2, 520.0, 879.7, 1079.7, 1258.0]\nc = [0.384, 0.578, 0.683, 0.798, 0.928, 1.069, 1.343, 1.656, 1.833, 2.671, 2.720, 2.781]\n\nfig, ax = pyplot.subplots()\nax.scatter(T, c)\nax.set_xlabel('$T [K]$')\nax.set_ylabel('$C/k_B$')\nax.set_ylim((0, 3))\nax.set_title('Heat capacity of diamond')\nfig.show()</pre>"},{"location":"2-1-hyperfine/#conclusions","title":"Conclusions","text":""},{"location":"2-1-hyperfine/#exercises","title":"Exercises","text":""},{"location":"2-1-hyperfine/#preliminary-provocations","title":"Preliminary provocations","text":"<ol> <li>What happens if you happen to choose your variational wavefunction perfectly? That is, you trail wavefunction is an eigenstate of the Hamiltonian?</li> </ol> <p>  as you might expect</p>"},{"location":"2-1-hyperfine/#heavy-hitters","title":"Heavy hitters","text":"<p>Image credits</p> <p>Header image taken from the PhD thesis Spinor Bose-Einstein condensates in magnetic field gradients</p> <ol> <li> <p>more accurately, at this time it was the Bohr-Sommerfeld model, which is an extension of the Bohr model to include elliptical orbits which fixed some problems. One will also see in solid-state physics that Sommerfeld has a real talent for tweaking existing theories to make them run a bit better.\u00a0\u21a9</p> </li> <li> <p>A study of the the relativistic wave equation, known as the Dirac equation, is required to fully flesh out this result.\u00a0\u21a9</p> </li> <li> <p>In this case we can, but more generally, the magnetic moment of the nucleus cannot be ignored (see LINK ME! hyperfine splitting)\u00a0\u21a9</p> </li> </ol>"},{"location":"6-2-molecules/","title":"State vectors","text":""},{"location":"6-2-molecules/#state-vectors","title":"State vectors","text":"<p>Time to do the same experiment lots of times and get different results</p>"},{"location":"6-2-molecules/#introduction","title":"Introduction","text":"<code>Lorem ipsum dolor sit amet</code> <p>Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis.</p> <p>Given this, along with</p> \\[ \\cos \\theta Y_{l m}(\\theta, \\phi)=\\left(\\frac{l^2-m^2}{4 l^2-1}\\right)^{1 / 2} Y_{l-1, m}(\\theta, \\phi)+\\left[\\frac{(l+1)^2-m^2}{4(l+1)^2-1}\\right]^{1 / 2} Y_{l+1, m}(\\theta, \\phi), \\] <p>and</p> \\[ \\int d \\Omega Y_{l^{\\prime} m^{\\prime}}^*(\\theta, \\phi) Y_{l m}(\\theta, \\phi)=\\delta_{l^{\\prime} l} \\delta_{m^{\\prime} m} \\] <p>gives us</p> \\[ \\left\\langle l^{\\prime}, m^{\\prime}|\\cos \\theta| l, m\\right\\rangle=\\delta_{m^{\\prime} m}\\left\\{\\left(\\frac{l^2-m^2}{4 l^2-1}\\right)^{1 / 2} \\delta_{l^{\\prime}, l-1}+\\left[\\frac{(l+1)^2-m^2}{4(l+1)^2-1}\\right]^{1 / 2} \\delta_{l^{\\prime}, l+1}\\right\\} . \\]"},{"location":"6-2-molecules/#conclusions","title":"Conclusions","text":""},{"location":"6-2-molecules/#exercises","title":"Exercises","text":""},{"location":"6-2-molecules/#preliminary-provocations","title":"Preliminary provocations","text":"<ol> <li>What happens if you happen to choose your variational wavefunction perfectly? That is, you trail wavefunction is an eigenstate of the Hamiltonian?</li> </ol> <p>  as you might expect</p>"},{"location":"6-2-molecules/#heavy-hitters","title":"Heavy hitters","text":"<p>Image credits</p> <p>Header image taken from the PhD thesis Spinor Bose-Einstein condensates in magnetic field gradients</p> <ol> <li> <p>more accurately, at this time it was the Bohr-Sommerfeld model, which is an extension of the Bohr model to include elliptical orbits which fixed some problems. One will also see in solid-state physics that Sommerfeld has a real talent for tweaking existing theories to make them run a bit better.\u00a0\u21a9</p> </li> <li> <p>A study of the the relativistic wave equation, known as the Dirac equation, is required to fully flesh out this result.\u00a0\u21a9</p> </li> <li> <p>In this case we can, but more generally, the magnetic moment of the nucleus cannot be ignored (see LINK ME! hyperfine splitting)\u00a0\u21a9</p> </li> </ol>"},{"location":"additional/","title":"Additional resources","text":""},{"location":"additional/#additional-resources","title":"Additional resources","text":"<p>This page is the space for additional resources which may prove to be of some use and/or interest for curious individuals.</p> <p>Websites</p> <ul> <li>Besides being a best-in-class mathematics educator, Grant Sanderson, a.k.a. 3Blue1Brown, has an excellent video series on the essence of linear algebra, which works to give you firm geometric grounding and build intuition in an area which is often taught in a highly abstract way, far removed from its practical application and implications. Given the heavy lifting that is done by linear algebra in atomic physics - and physics more generally - a quick brush-up on the subject, which is taught in such an insightful and refreshing manner, is well worth your time.</li> </ul> <p>Texts</p> <ul> <li>Undergraduate texts for quantum mechanics are common; however, concise, informed, relevant, and entertaining texts are much less common. I think the texts prescribed for both quantum and atomic physics are the best texts for the courses, but other texts which are recommended include<ul> <li>The classic - and increasingly difficult to find - Physics of Atoms and Molecules by B H Bransden and Charles J. Joachain</li> <li>A Modern Approach to Quantum Mechanics, Second Edition by John S. Townsend from Harvey Mudd College</li> <li>Introduction to Quantum Mechanics by David J. Griffiths and Darrell F. Schroeter from Reed College</li> <li>Quantum Mechanics by Daniel A. Steck from the University of Oregon</li> <li>Harry Potter and the methods of rationality by Eliezer Eudkowsky. Who know that Harry Potter fan fiction could provide such insight. As a jumping off point: Petunia married a biochemist, and Harry grew up reading science and science fiction. Then came the Hogwarts letter, and a world of intriguing new possibilities to exploit.</li> </ul> </li> </ul> <p>Previous course notes</p> <p>Atomic physics at UTAS is taught every second (even-numbered) year, with the previous course outing having been taught by Krzysztof Bolejko and Ross Turner for atomic physics and nuclear physics respectively. The notes from the 2022 course are posted here as a reference, but it should be emphasised that the course structure is vastly different, and consequently one's mileage may vary.</p> <p>Atomic physics</p>"},{"location":"maths/","title":"Mathematical marvels","text":""},{"location":"maths/#mathematical-marvels","title":"Mathematical marvels","text":"<p> Physical constants and mathematical formulae</p> <p> Clebsch-Gordan coefficients</p>"},{"location":"maths/#diagonalization-of-a-3-times-3-matrix","title":"Diagonalization of a \\(3 \\times 3\\) Matrix","text":"<p>Given a matrix \\(A\\):</p> \\[ A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{pmatrix} \\] <ol> <li>Find the eigenvalues by solving the characteristic equation:  </li> <li>Find the eigenvectors for each eigenvalue \\(\\lambda\\) by solving:  </li> <li>Form the matrix \\(P\\) using the eigenvectors as columns:  </li> <li>Construct the diagonal matrix \\(D\\) with the eigenvalues on the diagonal:  </li> <li>\\textbf{Verify} that \\(A = PDP^{-1}\\).</li> </ol>"},{"location":"particulars/","title":"Course particulars","text":""},{"location":"particulars/#course-information","title":"Course information","text":""},{"location":"particulars/#administration","title":"Administration","text":"<p>The atomic physics component of the atomic and nuclear physics course will run for six weeks, beginning in week one and concluding at the end of week six. In previous years, atomic physics acted to reenforce the content discussed in the advanced wave mechanics and quantum mechanics course, spending time on trying to understand atomic structure in a piecewise manner. In contrast, this course begins where the quantum mechanics course left off: the conclusion for most introductory quantum courses is the full solution to the Schr\u00c3\u00b6dinger equation for the hydrogen atom, and poking at the structure of helium. We take these foundations, and construct a framework for understanding and predicting the structure of more complex systems, and importantly, the kind of systems would encounter in wild. Moreover, we will explicitly discuss how one designs and executes experiments in a real-world context with the aim to cultivate an appreciation for quantum systems actually being accessible, and not just problems which appear on whiteboards.</p> <p>Prerequisite knowledge</p> <p>The content covered in atomic physics is testing, and without the firm bedrock of requisite knowledge and associated competencies, attempts to construct additional structures may be compromised. It is critical that one is comfortable with the following:</p> <ul> <li>The principles and machinery of wave mechanics. Explicitly, an understanding of how physical systems and their evolution are modelled using the wave equation, along with a fluency in common examples (plane-wave solutions, travelling waves, etc.).</li> <li>The foundations of quantum mechanics, including the (time dependent) Schr\\(\\\"{o}\\)dinger equation and its solutions for common physical systems, and importantly, you must be comfortable with the physical concepts which underpin the mathematics. Fortunately, you have just completed an introductory quantum mechanics course, and it will be assumed that you are comfortable with the content.</li> </ul> <p>It is my intention that you will be required to call upon many of the other tools from the toolbox that you have been developing during your studies, with the explicit aim of further honing these tools, and maybe adding a few to the kit.</p> <p>This looks familiar</p> <p>If you are experiencing a sense of d\u00e9j\u00e0 vu, that might be because you have encountered one of the other websites which are part of the quantum family of sites:</p> <ul> <li>Quantum mechanics</li> <li>Solid-state physics</li> </ul> <p>In particular, much of the content from the quantum site is directly relevant to this course, and it could well be considered a sister site.</p>"},{"location":"particulars/#delivery-of-content","title":"Delivery of content","text":"<p>The course will be run in a traditional.</p>"},{"location":"particulars/#subject-matter","title":"Subject matter","text":"<p>The content for this course draws heavily from a set of reference texts:</p> <ol> <li>The excellent text Quantum Mechanics, A Paradigms Approach (2nd edition) by David H. McIntyre</li> <li>The go-to reference for basic atomic physics, aptly named Atomic Physics by Christopher J. Foot</li> <li>The intensely thorough Quantum and Atom Optics by Daniel A. Steck</li> </ol> <p>With Foot being the prescribed text for the course, that is, it is assumed that you will have access to this book. This was chosen as McIntyre is the prescribed text for Quantum mechanics, and the two are somewhat complimentary. Steck is a glorious reference - and freely available - but our atomic physics journey only scratches the surface of the material covered in these notes, and as such it is not appropriate as a prescribed text. With this trilogy of titles, you will be well placed to understand why atoms are the way they are, and how to make them do what you want them to do. It is also one of the best introductory texts on the subject, so whether you fall deeply into the quantum rabbit hole or pack it all in to cultivate vanilla in Madagascar, your will be able to polish up on the basics thanks to the text.  </p> <p>Wait, are you actually prescribing textbooks?</p> <p>Yes. And no, you haven't just been transported to 1993; I endeavour to employ evidence-based, best-practice methods for teaching, and this includes deploying many modern teaching aids, and also includes ensuring a glorious reference is prescribed. I will be working from hardcopies, and I encourage you to do the same, although softcopies can be a more cost-effective option.</p>"},{"location":"particulars/#course-outline","title":"Course outline","text":"<p>Make yourself at home for our journey into atomic and nuclear physics. In this course we are going to study fundamental physical systems, which, on their surface can often appear simple, but are delightfully rich, engaging, and powerful. A testament to the foundational nature, utility, and continuing importance of the topics covered in this course is that during the period between 1995 - 2017, the fields of atomic and particle physics accounted for 25% of all Nobel prizes in science that were awarded. It is with this backdrop that we shall set out and seek to understand the fundamental properties and interactions of atoms and nuclei.</p> <p>Course summary</p> <p>This subject is designed to be a meaningful introduction to atomic physics. Hopefully your foray into the world of quantum mechanics was enjoyable, but it is almost inevidable that it left you with more questions than answers. Notably, most introductory quantum mechancis courses finish at the same spot: with the full-blown calculation of the energy eigenstates of the Hamiltonian for the hydrogen atom (in the nonrelativistic case). Whilst such a calculation is a triumph in its own right, the first question that you may ask is: \"does the hydrogen emission spectrum match my prediction\". The answer is no, and there are many distinct ways in which it does not match, due to different phenomena which arise from considering the atom in more detail. It is not the intention of this course the completely describe the hydrogen atom, on the contrary, we want to be able to describe all the atoms! This will mean developing theories and methods of calculation for multielectron systems, ensembles of atoms, and even touching on what happens with multiple nuclei. If your quantum mechanics course was a taster for what is out there in the quantum world; atomic physics narrows the focus of quantum mechanics to the measurement, manipulation, and evolution of atoms and their internal electronic transitions, our understanding of which provides our best tests of how the universe works, along with tests of how well we can predict how the universe works. The building blocks we shall study are basic atomic physics, angular momentum coupling, systems of indistinguishable particles, enesmbles of atoms, modern applications of atomic physics and molecular physics. It is worth noting that I am an experimental physicist by training, and it is my duty to ensure that experimental details permiate all that we study, because at the end of the day, we do need to actually make measurements to test our scientific theories: without this, one lives in a world of fiction.</p> <p>A rough outline of the course is as follows:</p> <ol> <li>Early atomic physics, the hydrogen atom, computational methods</li> <li>Hyperfine structure, angular momentum coupling</li> <li>Identical particles, transitions</li> <li>The density matrix</li> <li>Applications</li> <li>Molecular physics</li> </ol> <p>with approximately one week devoted to each topic, but with the natural ebb and flow ultimately dictating the timeline.</p>"},{"location":"particulars/#the-notes","title":"The notes","text":"<p>The notes on this site are designed to be consumed in concert the content from class, with certain aspects highlighted differently in the different media; in extreme cases, different paths are used to arrive at the same destination. One of my aims is to expose you to different ways of thinking about problems, not just have the same method and then just \"press play\".</p> <p>Expected competencies</p> <p>Content has been constructed with the assumption that the material is consumed sequentially, with sections often explicitly relying on results from previous work. I have also to placed expected competency markers at the beginning of each section, outlining knowledge that I expect you to have seen in other areas of study, and importantly, these are cumulative from section to section.</p> <p>Text reference</p> <p>You will also encounter text references at the beginning of each section, relating to the relevant content in the course texts</p> <p>Computational content</p> <p>Computational content, which is normally just the jupyter notebook associated with the section, also appears at the top of the page, but may also contain links to other software or pertinent computational material.</p>"},{"location":"particulars/#slides","title":"Slides","text":"<p>In the first iteration of this class, content will be deliverd with a mixture of curated presentation material and whiteboarding. Below you can find slides for the in-class presentations, but note that content is delibrately missing from these collections - usually many whole slides or sections - and this content will be covered in class.</p> <ul> <li> <p> Week 1</p> <p>Introductory atomic physics, the hydrogen atom, and computational methods for real systems</p> <p> Week 1</p> </li> <li> <p> Week 2</p> <p>Hyperfine structure, angular momentum, and the coupling of angular momentum</p> <p> Week 2</p> </li> <li> <p> Week 3</p> <p>Identical particles, multielectron atoms, and atomic transitions</p> <p> Week 3</p> </li> <li> <p> Week 4</p> <p>The density matrix, the optical bloch equations, Rabi oscillations</p> <p> Week 4</p> </li> <li> <p> Week 5</p> <p>Atomic timekeeping, quantum sensing, mechanical effects of quantum interactions</p> <p> Week 5</p> </li> <li> <p> Week 6</p> <p>Molecular physics, quantum chemistry</p> <p> Week 6</p> </li> </ul>"},{"location":"particulars/#support","title":"Support","text":"You are not on this journey alone: there are many avenues available to you to help you on the journey (depending on your inclination to watch fantasy movies in the 2000s, you may or not take comfort in the support Harry Potter received by those around him). <p>The course materials as consumed through the content download components are necessarily an individual effort, but in all other facets I strongly encourage collaboration. The structure of content unpacking sessions is deliberately geared towards discussion, the exchange of ideas, and collective problem solving moreso than the execution of a solution finding program.  </p> <p>Quote</p> <p>... there's something in science like the shine of the Patronus Charm, driving back all sorts of darkness and madness ...</p> <p></p><p> Eliezer Eudkowsky (Less Wrong), Harry Potter and the methods of rationality</p>"},{"location":"particulars/#computational-resources","title":"Computational resources","text":"<p>As part of the course, it will be expected that you perform calculation and computations. You are welcome to do this in which ever language you prefer, but it is strongly recommended that you use <code>Python</code>, and indeed, this is the only language that will be supported. In order to ensure equitable and easy access to Python computing resources, a Jupyter Notebook server has been established, which allows for one to write and execute code via a web browser. The server is named Jove<sup>1</sup>, and access is through the JupyterHub portal. You will need to create an account to start using the server, but beyond this is should be click and go. Should you experience any problems getting this up and running, please see the computation section of POLUS.</p> <p>Should you have a machine upon which you already have, or you wish to deploy, your own instance of <code>Python</code>, this is perfectly acceptable, but note that you will have to manually import the distributed materials into Jupyter. This can be effectively accomplished using the clone functionality of GitHub as this is where the course content is hosted.</p>"},{"location":"particulars/#bug-catcher","title":"Bug catcher","text":"<p> Finally, this is more of a request than anything else, but should you find any errors in the content - this site, the distributed notebooks, whatever - please let me know so I can correct the content, which is a major boon for everyone involved. Thanks!</p> <ol> <li> <p>For those wondering, Jove is an alternate name for the Roman god Jupiter.\u00a0\u21a9</p> </li> </ol>"},{"location":"s-a-1/","title":"Assignment 1","text":"<pre>\n\nfrom matplotlib import pyplot\nimport numpy as np\n\nfrom common import draw_classic_axes, configure_plotting\n\nconfigure_plotting()\n\n  </pre>"},{"location":"s-a-1/#assignment-one","title":"Assignment one","text":"<p>The first assignment covers the content from weeks 1 and 2, which includes topics such as early atomic physics, the hydrogen atom, stationary perturbation theory, the variational principle, angular momentum coupling, and the Hyperfine effect.</p> <p> A pdf version of the assignment as distributed     </p>"},{"location":"s-a-1/#question-1","title":"Question 1","text":"<p>Tritium</p> <p>Tritium is an isotope of hydrogen, with a nucleus comprising one proton and two neutrons. The tritium nucleus (triton) is radioactive, decaying via \\(\\beta\\) emission to a helium-3 nucleus which comprises two protons and one neutron. An electron is initially in the ground state of a tritium atom. After the (instantaneous) \\(\\beta\\) decay, what is the probability that the electron is in the ground state of the new atom?</p> <p>The ground state of tritium (\\(Z=1\\)) is</p> \\[     \\psi_{T,100}(r, \\theta, \\phi) = \\frac{1}{\\sqrt{\\pi a_0^3}} e^{-r/a_0} \\] <p>and the ground state of helium-3 (\\(Z=2\\)) is</p> \\[     \\psi_{He,100}(r, \\theta, \\phi) = \\sqrt{\\frac{8}{\\pi a_0^3}} e^{-2r/a_0}. \\] <p>The wavefunction is unchanged during the decay, so we can compute the probability via the usual way, namely:</p> \\[     \\mathscr{P}(\\psi = \\psi_{He,100}) = \\left\\rvert \\left\\langle \\psi_{He,100} \\rvert \\psi_{T,100} \\right\\rangle \\right\\rvert^2. \\] <p>The inner product is</p> \\[\\begin{align*}     \\left\\langle \\psi_{He,100} \\rvert \\psi_{T,100} \\right\\rangle &amp; = \\int \\psi_{He,100} \\, \\psi_{T,100} \\, dV \\\\     &amp; = \\int_0^\\infty \\int_0^{2\\pi} \\int_0^\\pi \\sqrt{\\frac{8}{\\pi a_0^3}} e^{-2r/a_0} \\frac{1}{\\sqrt{\\pi a_0^3}} e^{-r/a_0} r^2 \\sin(\\theta) \\, d\\theta \\, d\\phi \\, dr \\\\     &amp; = \\frac{\\sqrt{8}}{\\pi a_0^3} (4\\pi) \\int_0^\\infty r^2 e^{-3r/a_0} dr = \\frac{4\\sqrt{8}}{a_0^3} \\left(\\frac{2a_0^3}{27}\\right) \\\\     &amp; = \\frac{8\\sqrt{8}}{27} \\end{align*}\\] <p>and therefore the probability is</p> \\[     \\mathscr{P}(\\psi = \\psi_{He,100}) = \\left\\rvert  \\frac{8\\sqrt{8}}{27} \\right\\rvert^2 \\approx 0.702 \\]"},{"location":"s-a-1/#question-2","title":"Question 2","text":"<p>Muonic hyrdogen</p> <p>Muons are effectively heavy electrons. They rapidly decay into electrons with a lifetime of approximately \\(2.2~\\mu\\textrm{s}\\) - see the lifetime of the muon experiment in third year labs - but through some technical wizardry, it is possible to produce muonic hydrogen and perform precision spectroscopy on these atoms before they decay. Indeed, said spectroscopy has led to a still-unanswered problem, the so-called proton radius problem.</p> <p>As muons are much heavier than electrons, muonic hydrogen is a much smaller atom the vanilla hydrogen. As such, the finite size of the nucleus plays an more significant role in the energy-level structure of the system. The effective Coulomb potential can be approximated as</p> \\[     V(r) =     \\begin{cases}         -\\frac{Z e^2}{r} &amp; r \\geq R \\\\          -\\frac{Z e^2}{r}\\left( \\frac{3}{2}-\\frac{1}{2}\\frac{r^2}{R^2} \\right) &amp; r \\leq R     \\end{cases} \\] <p>where \\(R\\) is the region over which the nuclear charge is distributed.</p> <ol> <li>Express the Hamiltonian for muonic hydrogen as a perturbation of a hydrogenic system.          </li> <li>Posit how the energies for all the states with \\(n=1, 2,\\) and \\(3\\) will be shifted. How will the levels be shifted relative to absolutely, and relatively? Draw an energy-level diagram, indicating the unperturbed states, the perturbed states, and discuss the physical origins of any differences between states.</li> <li>Calculate the first-order change in energy for the ground state of muonic hydrogen. You can make the approximation that \\(R \\ll a_\\mu\\), where \\(a_\\mu\\) is the Bohr radius for the muon (1).</li> <li>The charge radius \\(R\\) is often measured by probing the \\(2s \\rightarrow 2p\\) transition. Find an expression for the (angular) frequency of this transition, thus showing one can indeed measure \\(R\\) from measurement of this transition.</li> <li>Can you suggest reasons that the \\(2s \\rightarrow 2p\\) transition is used to measure \\(R\\) instead of directly measuring the ground state energy?</li> </ol> <ol> <li>Hint: make this approximation early!</li> </ol>"},{"location":"s-a-1/#21","title":"2.1","text":"<p>Express the Hamiltonian for muonic hydrogen as a perturbation of a hydrogenic system.  </p> <p>If the system were ideal, that is, the proton charge radius was zero, the potential would be \\(V_0 = -\\frac{Z e^2}{r}\\) for all \\(r\\) - which we know how to solve. Our perturbation can then be constructed by considering \\(H'=V - V_0\\), or explicitly</p> \\[     H^\\prime =     \\begin{cases}         0 &amp; r \\ge R \\\\         Z e^2 \\left[\\frac{1}{r}-\\frac{1}{R}\\left(\\frac{3}{2} - \\frac{1}{2}\\frac{r^2}{R^2}\\right)\\right] &amp; r \\le R     \\end{cases} \\]"},{"location":"s-a-1/#22","title":"2.2","text":"<p>Posit how the energies for all the states with \\(n=1, 2,\\) and \\(3\\) will be shifted. How will the levels be shifted relative to absolutely, and relatively? Draw an energy-level diagram, indicating the unperturbed states, the perturbed states, and discuss the physical origins of any differences between states.</p> <p>When \\(r &lt; R\\), \\(H^\\prime &gt; 0\\) and the energy levels shift upwards, which is best understood through Gauss' law: inside the charge radius, there is less enclosed charge, so the interaction will be lessened and therefore the muon will be more weakly bound. This effect will be greatest for states which have a higher likelihood of being closer to the nucleus (\\(n=1\\)) and the effect will diminish with increasing angular momentum \\(\\ell\\), as states are further from the origin. We therefore expect the greatest effects for low \\(n\\) and \\(l\\), with the perturbation diminishing for higher \\(n\\) and \\(l\\).</p> <p> </p> The energy levels of muonic hydrogen"},{"location":"s-a-1/#23","title":"2.3","text":"<p>Calculate the first-order change in energy for the ground state of muonic hydrogen. You can make the approximation that \\(R \\ll a_\\mu\\), where \\(a_\\mu\\) is the Bohr radius for the muon (1).</p> <ol> <li>Hint: make this approximation early!</li> </ol> <p>The first-order correction to the ground state is given by</p> \\[     E_{1s}^{(1)} = \\left\\langle 1s \\left\\rvert H^\\prime \\right\\rvert 1s \\right\\rangle \\] <p>The ground state in position space is given by \\(\\psi_{\\mu,100} = \\frac{1}{\\sqrt{\\pi a_\\mu^3}} e^{-r/a_{\\mu}}\\) so making the approximation that \\(e^{-r/a_\\mu}\\approx 1\\) since \\(R \\ll a_\\mu\\) (I told you to make the approximation early!)</p> \\[   E_{1s}^{(1)} = \\int \\psi_{\\mu,100}^* \\, H' \\, \\psi_{\\mu,100} d\\mathbf{r} = \\frac{1}{\\pi a_\\mu^3} (4\\pi) \\int_0^R  Z e^2 \\left[\\frac{1}{r}-\\frac{1}{R}\\left(\\frac{3}{2} - \\frac{1}{2}\\frac{r^2}{R^2}\\right)\\right] r^2 dr \\] <p>where we note that the angular integral simply evaluates to \\(4\\pi\\) as there is no dependence on \\(\\theta\\) or \\(\\phi\\). Cranking the handle:</p> \\[\\begin{align*}     \\frac{ 4 Z e^2 }{a_\\mu^3} \\left(\\frac{R^2}{2}-\\frac{R^3}{2R} + \\frac{R^5}{10 R^3} \\right) &amp; = \\frac{ 4 Z e^2 }{a_\\mu^3} \\left(\\frac{R^2}{2}-\\frac{R^2}{2} + \\frac{R^2}{10} \\right) \\\\     &amp; = \\frac{ 2 R^2 Z e^2 }{5 a_\\mu^3} \\end{align*}\\]"},{"location":"s-a-1/#24","title":"2.4","text":"<p>The charge radius \\(R\\) is often measured by probing the \\(2s \\rightarrow 2p\\) transition. Find an expression for the (angular) frequency of this transition, thus showing one can indeed measure \\(R\\) from measurement of this transition.</p> <p>We follow essentially an identical procedure to that above, except using the states \\(\\psi_{\\mu,200}\\) and \\(\\psi_{\\mu,210}\\). Once you have played the game long enough, you can be sneaky, identifying that in integral over \\(\\psi_{\\mu,210}\\) is going to vanish, as the only angular dependence of the wave function is \\(\\cos(\\theta)\\), and the perturbing Hamiltonian does not have angular dependence, so we will effectively compute the integral of \\(\\cos^2(\\theta)\\sin(\\theta)\\) which when integrated over \\([0,2\\pi]\\) will average to zero, so we can immediately say that \\(E_{2p}^{(1)}=0\\).</p> <p>Computing \\(E_{2s}^{(1)}\\), we have</p> \\[ \\psi_{\\mu,200} = \\frac{1}{\\sqrt{\\pi}}\\left(\\frac{1}{2 a_\\mu}\\right)^{3/2} \\left[1-\\frac{r}{2 a_\\mu}\\right] e^{-r/a_{\\mu}} \\] <p>and again, making the approximation that \\(e^{-r/a_\\mu}\\approx 1\\) since \\(R \\ll a_\\mu\\) means we end up computing effectively the same as above</p> \\[\\begin{align*}     E_{2s}^{(1)} = \\int \\psi_{\\mu,200}^* \\, H' \\, \\psi_{\\mu,200} d\\mathbf{r} &amp; =  \\frac{1}{8 \\pi a_\\mu^3} (4\\pi) \\int_0^R  Z e^2 \\left[\\frac{1}{r}-\\frac{1}{R}\\left(\\frac{3}{2} - \\frac{1}{2}\\frac{r^2}{R^2}\\right)\\right] r^2 dr \\\\     &amp; = \\frac{Ze^2}{2 a_\\mu^3} \\frac{R^2}{10} \\\\     &amp; = \\frac{R^2 Z e^2 }{20 a_\\mu^3} \\end{align*}\\] <p>And so the perturbation to the energy of the transition is given by</p> \\[ \\Delta E = E_{2p}^{(1)} - E_{2s}^{(1)} \\approx E_{2s}^{(1)} \\] <p>and the total energy of the transition \\(E=\\hbar\\omega=E_{2p}-E_{2s}\\) to first-order only has the correction \\(E_{2s}^{(1)}\\)</p>"},{"location":"s-a-1/#25","title":"2.5","text":"<p>Can you suggest reasons that the \\(2s \\rightarrow 2p\\) transition is used to measure \\(R\\) instead of directly measuring the ground state energy?</p> <p>This is very much an experimentally-oriented question, and there are a few main considerations. The first is what happens when when measures a ground-state energy: when you do spectroscopy, you put in a photon with enough energy to drive the transition. By definition, the ground-state energy is the same as the ionisation energy of the atom, so you will generate muons in the spectroscopy process, which has the disadvantage of destroying your hard-won muonic hydrogen, but also means that you now have charge available to perturb the energy levels of surrounding atoms. There is also the practical consideration of when we perform spectroscopy, we like microwaves and optical frequencies, as we can control the frequencies of both of these very well. In muonic hydrogen, the energies involved are much larger than those of hydrogen - see the energy-level diagram below - and producing monochromatic radiation at those wavelengths is hard! Explicitly, experiments which deal even with the \\(1s\\rightarrow2p\\) states in hydrogen are hard work, as you require a laser in the UV, and these are both hard to make and difficult to use. In the case of muonic hydrogen, you require 2keV X-rays for the same transition, and coherent sources of X-rays are not readily available.</p> <p> </p> The energy levels of both electronic hydrogen and muonic hydrogen, taken from the paper The proton size which gives a good account of the *proton radius puzzle*. <p>For the \\(2s\\rightarrow2p\\) transition in hydrogen, we can use microwaves to drive transitions, which corresponds to deep infrared transitions for muonic hydrogen, but whilst this is not the most convenient laser wavelegnth, we can make it, and therefore do precision spectroscopy of muonic hydrogen. Well, for the time it exists before the muon decays!</p>"},{"location":"s-a-1/#question-3","title":"Question 3","text":"<p>Variational method</p> <p>Electrons in many-electron atoms generally experience a screened potential, that is, a potential that is smaller in magnitude than that due solely to its interaction with the nucleus due to other electrons shielding the nuclear charge.</p> <p>In atomic units, we can model this screened potential using</p> \\[     V(r) = -\\frac{Z}{r} + \\frac{1 - e^{-\\mu r}}{r} \\] <p>where \\(\\mu\\) (1) is a constant determined by atomic properties. We are going to use our intuition and propose a trial wave function for the ground state of the form</p> \\[ \\psi_{trial}(\\mathbf{r}) \\propto e^{-\\lambda Z r} Y_0^0(\\theta, \\phi) \\] <p>You are going to use the variational method to find a bound on the ground-state energy.</p> <ol> <li>Determined the normalisation constant for the trail wave function \\psi_{trial}(\\mathbf{r})</li> <li>Calculate the expectation value \\(\\left\\langle \\psi_{trial} \\rvert T \\rvert\\psi_{trial} \\right\\rangle\\), where \\(T\\) is the kinetic energy operator</li> <li>Calculate the expectation value \\(\\left\\langle \\psi_{trial} \\rvert V_0 \\rvert \\psi_{trial} \\right\\rangle\\), where \\(V_0\\) is the normal Coulomb potential</li> <li>Calculate the expectation value \\(\\left\\langle \\psi_{trial} \\rvert V^\\prime \\rvert \\psi_{trial} \\right\\rangle\\), where \\(V^\\prime\\) is perturbation from the normal Coulomb potential</li> <li>Using the above, calculate a bound on the ground state energy of the system, assuming a screening potential of \\(\\mu = 0.1~\\text{a.u.}^{-1}\\). You may solve non-trivial polynomials computationally (indeed, it is encouraged) but ensure to include executable code in your response.</li> </ol> <ol> <li>The symbol \\(\\mu\\) here is convention, and is unfortunate given the question above, but the two \\(\\mu\\) are very much unrelated!</li> </ol>"},{"location":"s-a-1/#31","title":"3.1","text":"<p>Determined the normalisation constant for the trail wave function \\(\\psi_{trial}(\\mathbf{r})\\)</p> <p>If we have our trial wave function</p> \\[     \\psi_{trial}(\\mathbf{r}) = N e^{-\\lambda Z r} Y_0^0(\\theta, \\phi) = \\frac{N}{\\sqrt{4\\pi}} e^{-\\lambda Z r} \\] <p>then normailsation demands that \\(\\left \\langle \\psi_{trial} \\rvert \\psi_{trial} \\right\\rangle = 1\\). This means that</p> \\[     \\begin{align*}           1 = \\left \\langle \\psi_{trial} \\rvert \\psi_{trial} \\right\\rangle &amp; = \\frac{N^2}{4\\pi}\\int_0^\\infty 4\\pi r^2 e^{-2\\lambda Z r} dr \\\\           &amp; = N^2\\frac{2!}{(2\\lambda Z)^3} = \\frac{N^2}{4(\\lambda Z)^3} \\\\           &amp; \\Rightarrow N^2 = 4(\\lambda Z)^3     \\end{align*} \\]"},{"location":"s-a-1/#32","title":"3.2","text":"<p>Calculate the expectation value \\(\\left\\langle \\psi_{trial} \\rvert T \\rvert\\psi_{trial} \\right\\rangle\\), where \\(T\\) is the kinetic energy operator</p> <p>To calculate the expectation value \\(\\langle T \\rangle\\), we must compute \\(\\left \\langle \\psi_{trial} \\rvert T \\rvert \\psi_{trial} \\right\\rangle\\). As we are in atomic units, this means out kinetic energy operator for a wave function which depends only on \\(r\\) has the form</p> \\[     T=- \\frac{1}{2} \\nabla^2 = -\\frac{1}{2}\\frac{1}{r^2}\\frac{\\partial}{\\partial r}\\left(r^2\\frac{\\partial}{\\partial r}\\right) \\] <p>which when applied to our wave function</p> \\[   \\begin{align*}       T\\rvert \\psi_{trial} \\rangle &amp; = -\\frac{1}{2}\\frac{1}{r^2}\\frac{\\partial}{\\partial r}\\left(r^2\\frac{\\partial}{\\partial r} \\frac{N}{\\sqrt{4\\pi}} e^{-\\lambda Z r} \\right) \\\\       &amp; = \\frac{\\lambda Z N}{2\\sqrt{4\\pi}} \\frac{1}{r^2} \\frac{\\partial}{\\partial r} \\left( r^2 e^{-\\lambda Z r} \\right) \\\\       &amp; = \\frac{\\lambda Z N}{2\\sqrt{4\\pi}} \\frac{1}{r^2}\\left(2re^{-\\lambda Z r} - \\lambda Z r^2 e^{-\\lambda Z r}\\right) \\\\       \\Rightarrow \\left\\langle \\psi_{trial} \\rvert T \\rvert \\psi_{trial} \\right\\rangle &amp; = \\frac{\\lambda Z N^2}{2\\cdot4\\pi} \\int_0^\\infty (4\\pi) \\left(2re^{-\\lambda Z r} - \\lambda Z r^2 e^{-\\lambda Z r}\\right) dr \\\\       &amp; = \\frac{\\lambda Z N^2}{2}\\left(2\\frac{1!}{(2\\lambda Z)^2} - \\lambda Z \\frac{2!}{(2\\lambda Z)^3}\\right) \\\\       &amp; = \\frac{\\lambda Z N^2}{2}\\frac{1}{4(\\lambda Z)^2} = \\frac{N^2}{8\\lambda Z} \\\\       &amp; \\left( = \\frac{(\\lambda Z)^2}{2} \\right)   \\end{align*} \\] <p>where I normally not substitute \\(N\\) until the computation is complete, but I put the result there if those that use a wave function with a normalisation constant. This is probably worth more than two marks...</p>"},{"location":"s-a-1/#33","title":"3.3","text":"<p>Calculate the expectation value \\(\\left\\langle \\psi_{trial} \\rvert V_0 \\rvert \\psi_{trial} \\right\\rangle\\), where \\(V_0\\) is the normal Coulomb potential</p> <p>After the work above, this one is easy! In atomic units, we need only compute \\(\\left\\langle \\psi_{trial} \\rvert -Z/r \\rvert \\psi_{trial} \\right\\rangle\\) which we can do directly, in the same way as the integrals above:</p> \\[\\begin{align*}     \\left\\langle \\psi_{trial} \\rvert V_0 \\rvert \\psi_{trial} \\right\\rangle &amp; = \\frac{N^2}{4\\pi}(4\\pi)\\int_0^\\infty -Z r e^{-2\\lambda Z r}dr \\\\     &amp; = - N^2 Z \\frac{1!}{(2\\lambda Z)^2} \\\\     &amp; = - \\frac{ZN^2}{(2\\lambda Z)^2} \\end{align*}\\]"},{"location":"s-a-1/#34","title":"3.4","text":"<p>Calculate the expectation value \\(\\left\\langle \\psi_{trial} \\rvert V^\\prime \\rvert \\psi_{trial} \\right\\rangle\\), where \\(V^\\prime\\) is perturbation from the normal Coulomb potential</p> <p>Hopefully it was clear that this didn't want you to calculate the expectation value of the full screening potential, just the perturbation, namely</p> \\[ V^\\prime = \\frac{1 - e^{-\\mu r}}{r} \\] <p>The expectation value is then</p> \\[\\begin{align*}     \\left\\langle \\psi_{trial} \\rvert V^\\prime \\rvert \\psi_{trial} \\right\\rangle &amp; = \\frac{N^2}{4\\pi}(4\\pi)\\int_0^\\infty r^2 e^{-2\\lambda Z r}  \\frac{1 - e^{-\\mu r}}{r} dr \\\\     &amp; = N^2 \\left( \\int_0^\\infty r e^{-2\\lambda Z r} dr - \\int_0^\\infty r e^{-(2\\lambda Z+\\mu) r} dr \\right) \\\\     &amp; = N^2 \\left( \\frac{1}{(2\\lambda Z)^2} - \\frac{1}{(2\\lambda Z+\\mu)^2}  \\right) \\\\ \\end{align*}\\] <p>This could be rearranged, but that is what we are going to do in the final part of this question.</p>"},{"location":"s-a-1/#35","title":"3.5","text":"<p>Using the above, calculate a bound on the ground state energy of the system, assuming a screening potential of \\(\\mu = 0.1~\\textrm{a.u.}^{-1}\\). You may solve non-trivial polynomials computationally (indeed, it is encouraged) but ensure to include executable code in your response.</p> <p>Incorrect value of \\(\\mu\\)</p> <p>It should be noted that there was an error in this question: the exponent in the value of \\(\\mu\\) was not rendered in the pdf version of the assignment. Given typical values are of the order 0.01 to 0.1 in atomic units, a value of \\(10~\\textrm{m}^{-1}\\) is silly. In any case, for the bulk of the question \\(mu\\) is just a symbol, and for any computations, there will be no penalty if the stated value - or any other - is used.   </p> <p>To find a bound on the energy, we can apply the variational principle, which states that the ground state energy will be less than the expectation value</p> \\[     \\left\\langle \\psi_{trial} \\rvert H \\rvert \\psi_{trial} \\right\\rangle \\] <p>Our Hamiltonian is composed by \\(H = T + V_0 + V^\\prime\\), and as taking the expectation value is a linear operation, the required expectation value is the sum of the individual expectation values, which we have just computed. Therefore</p> \\[\\begin{align*}     \\left\\langle H \\right\\rangle &amp; = N^2 \\left( \\frac{1}{8\\lambda Z} - \\frac{Z}{(2\\lambda Z)^2} + \\frac{1}{(2\\lambda Z)^2} - \\frac{1}{(2\\lambda Z+\\mu)^2} \\right) \\\\     &amp; = 4(\\lambda Z)^3 \\left( \\frac{1}{8\\lambda Z} + \\frac{1-Z}{(2\\lambda Z)^2} - \\frac{1}{(2\\lambda Z+\\mu)^2} \\right) \\end{align*}\\] <p>We now seek to minimise this with respect to our variational parameter \\(\\lambda\\), which means that we need to compute the derivative with respect to \\(\\lambda\\) and set it to zero. As each instance of \\(\\lambda\\) appears in the form of \\(\\lambda Z\\), I am going to define a new variable \\(\\lambda^\\prime = \\lambda Z\\) just to streamline things. We could have actually stated the calculation with this redefinition, as this means that our variational parameter would directly be an effective charge, rather than the correction factor to take the full charge to the effective charge. In any case, computing the derivative:</p> \\[\\begin{align*}     \\frac{\\partial \\left\\langle H \\right\\rangle}{\\partial \\lambda^\\prime} &amp; = \\frac{\\partial}{\\partial \\lambda^\\prime} \\left( \\frac{\\lambda^\\prime}{2} + (1-Z)\\lambda^\\prime - \\frac{4\\lambda^\\prime}{(2\\lambda^\\prime + \\mu)^2} \\right) \\\\     &amp; = \\lambda^\\prime + (1-Z) + \\frac{4{\\lambda^\\prime}^2(2\\lambda^\\prime+3\\mu)}{(2\\lambda^\\prime+\\mu)^3} \\end{align*}\\] <p>and we need to find the roots. Ain't nobody got time for that analytically, so let go to the computer. Normally I would only ever discuss <code>python</code>, but for a little exposure, I am going to highlight that sometimes <code>Mathematica</code> can be very useful - provided that you can get access to it.</p> <p>With that said, using <code>Mathematica</code>, one is able to get an analytic form to the roots by simply asking it to find them for you:</p> <pre><code>sols = Solve[x + (1 - Z) + (4 x^2 (2 x + 3 m))/(2 x + m)^3 == 0, x] /. {m -&gt; 0.1}\n</code></pre> <p>and one will be treated to a torrent of terms. Notably, there will be multiple solutions - this is fine, we are most interested in the term which will minimise the energy. <code>Mathematica</code> can tell you the energy of these states if you specify the expectation value, e.g.</p> <pre><code>4 x^2 (1/(8 x) + (1 - Z)/(2 x)^2  - 1/(2 x - 0.1)^2) /. {sols}\n</code></pre> <p>Once again, you will be treated to a bunch of garbage, but these are your solutions. This becomes more meaningful once on actually specifies a value for \\(Z\\), so for the sake of completeness, let us look at Helium, where \\(Z=2\\), which returns a minimum value of \\(E = -3.14~\\textrm{a.u.} = -85.4~\\textrm{eV}\\). This shielding parameter was just chosen at random with the correct order of magnitude (lol) so the result is not the meaningful by itself.</p> <p>If one wanted to use <code>python</code>, a good bet would be to use the <code>solve</code> function from the <code>SciPy.optimize</code> module:</p> <pre><code>import numpy as np\nfrom scipy.optimize import fsolve\n\n# Define the expectation value derivative\ndef expectation_min(lambda_prime, Z, mu):\n   return lambda_prime + (1-Z) + (4*lambda_prime**2*(2*lambda_prime + 3*mu)) / ((2*lambda_prime + mu)**3)\n\n# Set values of Z and mu\nZ_value = 2  # Helium\nmu_value = 0.1  # screening constant [a.u.]\n\n# Initial value for lambda_prime\nlambda_guess = 100 # The solution shouldn't be overly sensitive to this\n\n# Solve and print the solution\nnumerical_solution = fsolve(expectation_min, lambda_guess, args=(Z_value, mu_value))\nprint(numerical_solution)\n</code></pre> <p>which yield the identical result to that of <code>Mathematica</code>.</p>"},{"location":"s-a-1/#question-4","title":"Question 4","text":"<p>Angular momentum</p> <p>In class, we used the fact that the electron and proton spin observables \\(\\mathbf{S}^2\\) and \\(\\mathbf{I}^2\\) commute with the hyperfine Hamiltonian \\(H_{hf}^\\prime = A\\mathbf{S}\\cdot\\mathbf{I}/\\hbar^2\\), and stated that the component observables \\(S_z\\) and \\(I_z\\) do not.</p> <ol> <li>Explicitly show that \\(\\mathbf{S}^2\\) and \\(\\mathbf{I}^2\\) commute with the hyperfine Hamiltonian</li> <li>Explicitly show that \\(S_z\\) and \\(I_z\\) do not commute with the hyperfine Hamiltonian</li> </ol> <p>Commutation relations</p> <p>For both questions, we make use of the standard angular momentum commutation relations:</p> \\[\\begin{gather*}     \\left[ \\mathbf{J}^2, J_{x,y,z} \\right] = 0 \\\\     \\left[J_i, J_j\\right] = i\\hbar\\varepsilon_{ijk}J_k \\end{gather*}\\] <p>where \\(\\varepsilon_{ijk}\\) is the Levi-Civita symbol.</p>"},{"location":"s-a-1/#41","title":"4.1","text":"<p>Explicitly show that \\(\\mathbf{S}^2\\) and \\(\\mathbf{I}^2\\) commute with the hyperfine Hamiltonian</p> <p>To show the operators commute, we crank the handle. Starting with \\(\\mathbf{S}^2\\):</p> \\[   \\begin{align*}       \\left[H_{h f}^{\\prime}, \\mathbf{S}^2\\right] &amp; =\\left[\\frac{A}{\\hbar^2} \\mathbf{S} \\cdot \\mathbf{I}, \\mathbf{S}^2\\right]=\\frac{A}{\\hbar^2} \\mathbf{I}\\left[\\mathbf{S}, \\mathbf{S}^2\\right]=\\frac{A}{\\hbar^2} \\mathbf{I} \\cdot\\left[S_x \\mathbf{i}+S_y \\mathbf{j}+S_z \\mathbf{k}, \\mathbf{S}^2\\right] \\\\       &amp; = \\frac{A}{\\hbar^2} \\mathbf{I} \\cdot\\left\\{\\mathbf{i}\\left[S_x, \\mathbf{S}^2\\right]+\\mathbf{j}\\left[S_y, \\mathbf{S}^2\\right]+\\mathbf{k}\\left[S_z, \\mathbf{S}^2\\right]\\right\\}=0 \\\\   \\end{align*} \\] <p>and then \\(\\mathbf{I}^2\\):</p> \\[   \\begin{align*}       {\\left[H_{h f}^{\\prime}, \\mathbf{I}^2\\right] } &amp; =\\left[\\frac{A}{\\hbar^2} \\mathbf{S} \\cdot \\mathbf{I}, \\mathbf{I}^2\\right]=\\frac{A}{\\hbar^2} \\mathbf{S} \\cdot\\left[\\mathbf{I}, \\mathbf{I}^2\\right]=\\frac{A}{\\hbar^2} \\mathbf{S} \\cdot\\left[I_x \\mathbf{i}+I_y \\mathbf{j}+I_z \\mathbf{k}, \\mathbf{I}^2\\right]= \\\\       &amp; =\\frac{A}{\\hbar^2} \\mathbf{S} \\cdot\\left\\{\\mathbf{i}\\left[I_x, \\mathbf{I}^2\\right]+\\mathbf{j}\\left[I_y, \\mathbf{I}^2\\right]+\\mathbf{k}\\left[I_z, \\mathbf{I}^2\\right]\\right\\}=0 \\\\   \\end{align*} \\]"},{"location":"s-a-1/#42","title":"4.2","text":"<p>Explicitly show that \\(S_z\\) and \\(I_z\\) do not commute with the hyperfine Hamiltonian</p> <p>In the same way as above, we can show the operators do not commute directly:</p> \\[   \\begin{align*}       {\\left[H_{h f}^{\\prime}, S_z\\right] } &amp; =\\left[\\frac{A}{\\hbar^2} \\mathbf{S} \\cdot \\mathbf{I}, S_z\\right]=\\frac{A}{\\hbar^2}\\left[S_x I_x+S_y I_y+S_z I_z, S_z\\right] \\\\       &amp; =\\frac{A}{\\hbar^2}\\left\\{I_x\\left[S_x, S_z\\right]+I_y\\left[S_y, S_z\\right]+I_z\\left[S_z, S_z\\right]\\right\\} \\\\       &amp; =\\frac{A}{\\hbar^2}\\left\\{-i \\hbar S_y I_x+i \\hbar S_x I_y\\right\\}=\\frac{-i A}{\\hbar}\\left\\{S_y I_x-S_x I_y\\right\\} \\neq 0 \\\\       {\\left[H_{h f}^{\\prime}, I_z\\right] } &amp; =\\left[\\frac{A}{\\hbar^2} \\mathbf{S} \\cdot \\mathbf{I}, I_z\\right]=\\frac{A}{\\hbar^2}\\left[S_x I_x+S_y I_y+S_z I_z, I_z\\right] \\\\       &amp; =\\frac{A}{\\hbar^2}\\left\\{S_x\\left[I_x, I_z\\right]+S_y\\left[I_y, I_z\\right]+S_z\\left[I_z, I_z\\right]\\right\\} \\\\       &amp; =\\frac{A}{\\hbar^2}\\left\\{-i \\hbar I_y S_x+i \\hbar I_x S_y\\right\\}=\\frac{-i A}{\\hbar}\\left\\{I_y S_x-I_x S_y\\right\\} \\neq 0   \\end{align*} \\]"},{"location":"s-a-1/#question-5","title":"Question 5","text":"<p>Positronium</p> <p>In February 2024, positronium was laser cooled, heralding the era for precision measurements involving antimatter. Positronium atom is a hydrogen-like atom with a positron (\\(m = m_e,\\,q = +e\\), spin 1/2)  as the nucleus and a bound electron. The hyperfine structure in the ground state of positronium is described by a perturbation Hamiltonian \\(H^\\prime = A \\mathbf{S}_1 \\cdot \\mathbf{S}_2/\\hbar^2\\) where \\(\\mathbf{S}_i\\) are the spins of the electron and positron.</p> <ol> <li>What is the Bohr energy of the ground state of positronium (you can ignore the hyperfine structure for this one)?</li> <li>The electron and positron spins can be coupled to form the total spin \\(\\mathbf{S}\\) of the atom. Write down the spin states of the coupled and uncoupled bases and how they relate to each other.</li> <li>Express the hyperfine Hamiltonian in the ground state as a matrix in both the coupled and uncoupled spin bases.</li> <li>Determine the effect of the hyperfine perturbation interaction on the ground state of positronium. Draw an energy level diagram to illustrate your results.</li> </ol>"},{"location":"s-a-1/#51","title":"5.1","text":"<p>What is the Bohr energy of the ground state of positronium (that is, you can ignore the hyperfine structure)?</p> <p>The energy structure for hydrogen is given by \\(E_n = - \\alpha^2 \\mu c^2 / 2n\\) where \\(\\mu\\) is the reduced mass, which most of the time can be approximated by \\(m_e\\), but for positronium, as the nuclear mass and electron mass are the same, the reduced mass will no longer be well approximated by \\(m_e\\). Explicitly</p> \\[     \\mu = \\frac{m_e m_n}{m_e + m_n} = \\frac{m_e^2}{2m_e} = \\frac{m_e}{2} \\] <p>so the energy levels in hydrogen are essentially half those of hydrogen, meaning that the ground state energy is</p> \\[     E_1 = -\\frac{13.6}{2} = - 6.8~\\textrm{eV} \\]"},{"location":"s-a-1/#52","title":"5.2","text":"<p>The electron and positron spins can be coupled to form the total spin \\(\\mathbf{S}\\) of the atom. Write down the spin states of the coupled and uncoupled bases and how they relate to each other.</p> <p>In the same way as the electron and proton in hydrogen are two spin-1/2 particles, as are the electron and positron in positronium.</p> <p>The uncoupled basis states \\(\\rvert \\frac{1}{2} \\frac{1}{2} m_{s_1} m_{s_2} \\rangle\\) are</p> \\[   \\left\\rvert ++ \\right\\rangle, \\left\\rvert +- \\right\\rangle, \\left\\rvert -+ \\right\\rangle, \\, \\mathrm{and} \\, \\left\\rvert -- \\right\\rangle \\] <p>and the coupled states are</p> \\[   \\begin{alignedat}{2}       &amp;\\left.\\begin{aligned}           &amp; |11\\rangle = |++\\rangle \\\\           &amp; |10\\rangle = \\frac{1}{\\sqrt{2}}\\left[|+-\\rangle+|-+\\rangle\\right] \\\\           &amp; |1,-1\\rangle = |--\\rangle       \\end{aligned}       \\quad \\right\\rbrace       &amp; \\quad \\text{Triplet state} \\\\[10pt]       &amp;\\left.\\begin{aligned}           &amp; |00\\rangle = \\frac{1}{\\sqrt{2}}\\left[|+-\\rangle-|-+\\rangle\\right]       \\end{aligned}       \\right\\rbrace       &amp; \\quad \\text{Singlet state}   \\end{alignedat} \\]"},{"location":"s-a-1/#53","title":"5.3","text":"<p>Express the hyperfine Hamiltonian in the ground state as a matrix in both the coupled and uncoupled spin bases.</p> <p>The hyperfine Hamiltonian is</p> \\[   \\begin{align*}       \\mathbf{S} &amp; = \\mathbf{S}_1 + \\mathbf{S}_2 \\\\       \\mathbf{S}^2 &amp; = (\\mathbf{S}_1 + \\mathbf{S}_2)^2 = \\mathbf{S}_1^2 + \\mathbf{S}_2^2 + 2 \\mathbf{S}_1 \\cdot \\mathbf{S}_2 \\\\       \\Rightarrow \\mathbf{S}_1 \\cdot \\mathbf{S}_2 &amp; = \\frac{1}{2} \\left( \\mathbf{S}^2 - \\mathbf{S}_1^2 - \\mathbf{S}_2^2 \\right) = \\frac{1}{2}\\hbar^2 \\left( S(S+1) - 3/4 \\right) \\\\       H^\\prime &amp; = A \\mathbf{S}_1\\cdot\\mathbf{S_2} = \\frac{A}{2}\\hbar^2 \\left( S(S+1) - 3/4 \\right)   \\end{align*} \\] <p>The matrix in the coupled basis is then</p> \\[     H^\\prime = \\frac{A\\hbar^2}{4}     \\begin{pmatrix}         1 &amp; 0 &amp; 0 &amp; 0 \\\\         0 &amp; 1 &amp; 0 &amp; 0 \\\\         0 &amp; 0 &amp; 1 &amp; 0 \\\\         0 &amp; 0 &amp; 0 &amp; -3     \\end{pmatrix}     \\begin{matrix}         \\ket{11} \\\\         \\ket{10} \\\\         \\ket{1,-1} \\\\         \\ket{00}     \\end{matrix} \\] <p>To calculate the matrix in the uncoupled basis, we can express the \\(x\\) and \\(y\\) components of \\(\\mathbf{S}_1\\) and \\(\\mathbf{S}_2\\) in terms of ladder operators</p> \\[     \\mathbf{S}_1 \\cdot \\mathbf{S}_2 = S_{1x}S_{2x} + S_{1y}S_{2y} + S_{1z}S_{2z} = \\frac{1}{2}\\left(S_{1+}S_{2-}+S_{1-}S_{2+}\\right) + S_{1z}S_{2z} \\] <p>and so</p> \\[     H^\\prime = \\frac{A\\hbar^2}{4}     \\begin{pmatrix}         1 &amp; 0 &amp; 0 &amp; 0 \\\\         0 &amp; -1 &amp; 2 &amp; 0 \\\\         0 &amp; 2 &amp; -1 &amp; 0 \\\\         0 &amp; 0 &amp; 0 &amp; 1     \\end{pmatrix}     \\begin{matrix}         \\left\\rvert ++ \\right\\rangle \\\\         \\left\\rvert +- \\right\\rangle \\\\         \\left\\rvert -+ \\right\\rangle \\\\         \\left\\rvert -- \\right\\rangle     \\end{matrix} \\]"},{"location":"s-a-1/#54","title":"5.4","text":"<p>Determine the effect of the hyperfine perturbation interaction on the ground state of positronium. Draw an energy level diagram to illustrate your results.</p> <p>Since the Hamiltonian in the coupled basis is diagonal</p> \\[   H^\\prime = A \\mathbf{S}_1\\cdot\\mathbf{S_2} = \\frac{A}{2}\\hbar^2 \\left( S(S+1) - 3/4 \\right) \\] <p>we can directly compute the energy eigenvalues</p> \\[    \\langle S^\\prime M^\\prime \\rvert H^\\prime \\rvert SM \\rangle = \\frac{1}{2} A \\hbar^2 \\left(S(S+1) - \\frac{3}{2}\\right) \\delta_{SS^\\prime} \\delta_{MM^\\prime}    =\\left\\{\\begin{array}{cc}            -\\frac{3}{4} A \\hbar^2 &amp; S=0 \\\\            \\frac{1}{4} A \\hbar^2 &amp; S=1            \\end{array}\\right. \\] <p>and so the energy-level structure is</p> <p> </p> A schematic of the Stern-Gerlach experiment"},{"location":"s-a-2/","title":"Assignment 2","text":"<pre>\n\nfrom matplotlib import pyplot\nimport numpy as np\n\nfrom common import draw_classic_axes, configure_plotting\n\nconfigure_plotting()\n\n  </pre>"},{"location":"s-a-2/#assignment-two","title":"Assignment two","text":"<p>The second assignment covers the content from weeks 3 and 4, which includes topics such as identical particles, transitions, the density matrix, and the optical Bloch equations.</p> <p> A .pdf version of the assignment as distributed     </p>"},{"location":"s-a-2/#question-1","title":"Question 1","text":"<p>Multiparticle harmonic oscillator</p> <p>Consider two non-interacting particles of mass \\(m\\) in the harmonic oscillator potential well. For the case with one particle in the single-particle state \\(\\ket{n}\\) and the other in state \\(\\ket{k}\\) (where \\(n \\ne k\\)), we are going to calculate the expectation value of the squared interparticle spacing: \\(\\left\\langle \\left( x_1 - x_2 \\right)^2 \\right\\rangle\\). Do this for the cases where the particles are:</p> <ul> <li>distinguishable</li> <li>spin-0</li> <li>spin-1/2 in a spin triplet state</li> </ul> <p>In all cases, calculate the expected interpatricle spacing and explain whether the results are consistent with your expectations, and why (1).</p> <ol> <li>Hint: Use Dirac notation. With the correct application, integration is not required for the above calculations. Go forth and harness the power of state vectors to improve your quality of life!</li> </ol> <p>For the astute observer, this problem is extremely similar to problem 4 from tutorial 3. Indeed, it is so similar that I can cut and paste a large amount of the \\(\\LaTeX\\) which I prepared for that solution.</p> <p><code>\\begin{Tutorial 3 problem 4 solution}</code></p> <p>We need to look at the three cases of the 2-particle wave function, which is a product of the two single-particle wave functions which are</p> <ul> <li>not symmetrised in the case of distinguishable particles</li> <li>symmetrised in the case of bosons</li> <li>antisymmetrised in the case of fermions</li> </ul> <p>with respect to exchange of the two particles. In the three cases, we have</p> \\[\\begin{align*}     &amp; \\left|\\psi_D\\right\\rangle=|n k\\rangle \\equiv|n\\rangle_1|k\\rangle_2 \\\\     &amp; \\left|\\psi_B\\right\\rangle=\\frac{1}{\\sqrt{2}}[|n k\\rangle+|k n\\rangle] \\\\     &amp; \\left|\\psi_F\\right\\rangle=\\frac{1}{\\sqrt{2}}[|n k\\rangle-|k n\\rangle] \\end{align*}\\] <p>where the single particle wave function \\(\\ket{n}=\\phi_n(x)=\\sqrt{\\frac{2}{L}}\\sin\\left(\\frac{n \\pi x}{L}\\right)\\).</p> <p>For distinguishable particles</p> \\[     \\left\\langle x_1^2\\right\\rangle_D=\\left\\langle n k\\left|x_1^2\\right| n k\\right\\rangle=\\left\\langle n\\left|x_1^2\\right| n\\right\\rangle\\langle k | k\\rangle=\\left\\langle x^2\\right\\rangle_n \\] <p>At this point, we can write down the integral, so</p> \\[     \\left\\langle x^2\\right\\rangle_n=\\int_0^L \\varphi_n^{+}(x) x^2 \\varphi_n(x) d x=\\int_0^L x^2\\left|\\varphi_n(x)\\right|^2 d x \\] <p>is a single-particle expectation value for \\(x^2\\). Likewise, we get</p> \\[   \\begin{gathered}       \\left\\langle x_2^2\\right\\rangle_D=\\left\\langle n k\\left|x_2^2\\right| n k\\right\\rangle=\\langle n | n\\rangle\\left\\langle k\\left|x_2^2\\right| k\\right\\rangle=\\left\\langle x^2\\right\\rangle_k \\\\       \\left\\langle x_1 x_2\\right\\rangle_D=\\left\\langle n k\\left|x_1 x_2\\right| n k\\right\\rangle=\\left\\langle n\\left|x_1\\right| n\\right\\rangle\\left\\langle k\\left|x_2\\right| k\\right\\rangle=\\langle x\\rangle_n\\langle x\\rangle_k \\\\       \\left\\langle\\left(x_1-x_2\\right)^2\\right\\rangle_D=\\left\\langle x^2\\right\\rangle_n+\\left\\langle x^2\\right\\rangle_k-2\\langle x\\rangle_n\\langle x\\rangle_k   \\end{gathered} \\] <p>So really, we need only perform the integrals to compute \\(\\left\\langle x^2\\right\\rangle_n\\) and \\(\\langle x\\rangle_n\\). But before we actually integrate, lets look at which integrals we need to compute in the case of fermions and bosons.</p> \\[\\begin{align*}       \\left\\langle x_1^2\\right\\rangle_{B, F} &amp; =\\frac{1}{\\sqrt{2}}\\left[\\langle n k| \\pm\\langle k n|\\right] x_1^2 \\times \\frac{1}{\\sqrt{2}}\\left[|n k\\rangle \\pm|k n\\rangle\\right] \\\\       &amp; =\\frac{1}{2}\\left[\\left\\langle n\\left|x_1^2\\right| n\\right\\rangle\\langle k | k\\rangle \\pm\\left\\langle n\\left|x_1^2\\right| k\\right\\rangle\\langle k | n\\rangle \\pm\\left\\langle k\\left|x_1^2\\right| n\\right\\rangle\\langle n | k\\rangle +\\left\\langle k\\left|x_1^2\\right| k\\right\\rangle\\langle n | n\\rangle\\right] \\\\       &amp; =\\frac{1}{2}\\left[\\left\\langle n\\left|x_1^2\\right| n\\right\\rangle \\pm 0 \\pm 0+\\left\\langle k\\left|x_1^2\\right| k\\right\\rangle\\right]=\\frac{1}{2}\\left[\\left\\langle x^2\\right\\rangle_n+\\left\\langle x^2\\right\\rangle_k\\right] \\\\       \\left\\langle x_1 x_2\\right\\rangle_{B, F} &amp; =\\frac{1}{\\sqrt{2}}\\left[\\langle n k| \\pm\\langle k n|\\right] x_1 x_2 \\times \\frac{1}{\\sqrt{2}}\\left[|n k\\rangle \\pm|k n\\rangle\\right] \\\\       &amp; = \\frac{1}{2}\\left[\\left\\langle n\\left|x_1\\right| n\\right\\rangle\\left\\langle k\\left|x_2\\right| k\\right\\rangle \\pm\\left\\langle n\\left|x_1\\right| k\\right\\rangle\\left\\langle k\\left|x_2\\right| n\\right\\rangle \\pm\\left\\langle k\\left|x_1\\right| n\\right\\rangle\\left\\langle n\\left|x_2\\right| k\\right\\rangle+\\left\\langle k\\left|x_1\\right| k\\right\\rangle\\left\\langle n\\left|x_2\\right| n\\right\\rangle\\right] \\\\       &amp; = \\frac{1}{2}\\left[\\langle x\\rangle_n\\langle x\\rangle_k \\pm\\langle x\\rangle_{n k}\\langle x\\rangle_{k n} \\pm\\langle x\\rangle_{k n}\\langle x\\rangle_{n k}+\\langle x\\rangle_k\\langle x\\rangle_n\\right]=\\langle x\\rangle_n\\langle x\\rangle_k \\pm\\left|\\langle x\\rangle_{n k}\\right|^2 \\end{align*}\\] <p>where</p> \\[   \\langle x\\rangle_{n k}=\\langle n|x| k\\rangle=\\int_0^L \\varphi_n^*(x) x \\varphi_k(x) d x . \\] <p>Combining this, we have</p> \\[     \\left\\langle\\left(x_1-x_2\\right)^2\\right\\rangle_{B, F}=\\left\\langle x^2\\right\\rangle_n+\\left\\langle x^2\\right\\rangle_k-2\\langle x\\rangle_n\\langle x\\rangle_k \\mp 2\\left|\\langle x\\rangle_{n k}\\right|^2 \\] <p>meaning we need only compute one extra integral.</p> <p><code>\\end{Tutorial 3 problem 4 solution}</code></p> <p>Now the hint promises that one need not compute any integrals, and this is because in the above solution, we need to change the single particle wave functions to that of the harmonic oscillator rather than the square well. But the secret sauce to this problem is recalling that the position (and momentum, and therefore Hamiltonian) can be expressed in terms of the ladder operators \\(a\\) and \\(a^\\dagger\\). Explicitly, we have</p> \\[     x = \\sqrt{\\frac{\\hbar}{2m\\omega}}\\left(a^\\dagger+a\\right) \\] <p>which means we can evaluate the required expectation values via</p> \\[\\begin{align*}         \\left\\langle x^2\\right\\rangle_n &amp; =\\left\\langle n\\left|x^2\\right| n\\right\\rangle=\\frac{\\hbar}{2 m \\omega}\\left\\langle n\\left|\\left(a^{\\dagger}+a\\right)^2\\right| n\\right\\rangle=\\frac{\\hbar}{2 m \\omega}\\left\\langle n\\left|\\left(a^{\\dagger}\\right)^2+a^{\\dagger} a+a a^{\\dagger}+a^2\\right| n\\right\\rangle \\\\         &amp; =\\frac{\\hbar}{2 m \\omega}\\left\\langle n\\left|a^{\\dagger} a+a a^{\\dagger}\\right| n\\right\\rangle=\\frac{\\hbar}{2 m \\omega}\\langle n|\\sqrt{n} \\sqrt{n}+\\sqrt{n+1} \\sqrt{n+1}| n\\rangle \\\\         &amp; =\\frac{\\hbar}{2 m \\omega}(2 n+1)=\\frac{\\hbar}{m \\omega}\\left(n+\\frac{1}{2}\\right) \\end{align*}\\] <p>Notice the appearance of the number operator and it conjugate from tutorial 2.</p> \\[\\begin{align*}         \\langle x\\rangle_n &amp; =\\langle n|x| n\\rangle=\\sqrt{\\frac{\\hbar}{2 m \\omega}}\\left\\langle n\\left|a^{\\dagger}+a\\right| n\\right\\rangle \\\\         &amp; =\\sqrt{\\frac{\\hbar}{2 m \\omega}}\\left[\\left\\langle n\\left|a^{\\dagger}\\right| n\\right\\rangle+\\langle n|a| n\\rangle\\right]=\\sqrt{\\frac{\\hbar}{2 m \\omega}}[\\langle n|\\sqrt{n+1}| n+1\\rangle+\\langle n|\\sqrt{n}| n-1\\rangle] \\\\         &amp; =\\sqrt{\\frac{\\hbar}{2 m \\omega}}[\\sqrt{n+1}\\langle n | n+1\\rangle+\\sqrt{n}\\langle n | n-1\\rangle]=0 \\text { since }\\langle n | m\\rangle=\\delta_{n m} \\end{align*}\\] <p>which is to be expected: where will be find the particle? At the bottom of the well, which is centred on \\(x=0\\). The final term to calculate are the matrix elements \\(x_nk\\), which wouldn't you know it, we calculated in tutorial 2:</p> \\[\\begin{align*}         \\langle x\\rangle_{n k} &amp; =\\langle n|x| k\\rangle=\\sqrt{\\frac{\\hbar}{2 m \\omega}}\\left\\langle n\\left|a^{\\dagger}+a\\right| k\\right\\rangle \\\\         &amp; =\\sqrt{\\frac{\\hbar}{2 m \\omega}}\\left(\\left\\langle n\\left|a^{\\dagger}\\right| k\\right\\rangle+\\langle n|a| k\\rangle\\right)=\\sqrt{\\frac{\\hbar}{2 m \\omega}}(\\langle n|\\sqrt{k+1}| k+1\\rangle+\\langle n|\\sqrt{k}| k-1\\rangle) . \\\\         &amp; =\\sqrt{\\frac{\\hbar}{2 m \\omega}}(\\sqrt{k+1}\\langle n | k+1\\rangle+\\sqrt{k}\\langle n | k-1\\rangle)=\\sqrt{\\frac{\\hbar}{2 m \\omega}}\\left(\\sqrt{n} \\delta_{n, k+1}+\\sqrt{k} \\delta_{n, k-1}\\right) \\end{align*}\\] <p>Putting everything together</p> \\[\\begin{align*}         \\left\\langle\\left(x_1-x_2\\right)^2\\right\\rangle_D &amp; =\\left\\langle x^2\\right\\rangle_n+\\left\\langle x^2\\right\\rangle_k-2\\langle x\\rangle_n\\langle x\\rangle_k=\\frac{\\hbar}{m \\omega}\\left(n+\\frac{1}{2}\\right)+\\frac{\\hbar}{m \\omega}\\left(k+\\frac{1}{2}\\right) \\\\         &amp; =\\frac{\\hbar}{m \\omega}(n+k+1) \\\\ \\end{align*}\\] <p>and</p> \\[\\begin{align*}         \\left\\langle\\left(x_1-x_2\\right)^2\\right\\rangle_{B, F}&amp;=\\left\\langle x^2\\right\\rangle_n+\\left\\langle x^2\\right\\rangle_k-2\\langle x\\rangle_n\\langle x\\rangle_k \\mp 2\\left|\\langle x\\rangle_{n k}\\right|^2 \\\\         &amp; =\\frac{\\hbar}{m \\omega}(n+k+1) \\mp \\frac{\\hbar}{m \\omega}\\left(n \\delta_{n, k+1}+k \\delta_{n, k-1}\\right) \\end{align*}\\] <p>For the lowest energy state \\((n=0, k=1)\\) the expected interparticle spacings are</p> \\[\\begin{align*}         &amp; \\sqrt{\\left\\langle\\left(x_1-x_2\\right)^2\\right\\rangle_D}=2\\frac{\\hbar}{m\\omega} \\\\         &amp; \\sqrt{\\left\\langle\\left(x_1-x_2\\right)^2\\right\\rangle_B}=2\\frac{\\hbar}{m\\omega} - \\frac{\\hbar}{m\\omega} = \\frac{\\hbar}{m\\omega}  \\\\         &amp; \\sqrt{\\left\\langle\\left(x_1-x_2\\right)^2\\right\\rangle_F}=2\\frac{\\hbar}{m\\omega} + \\frac{\\hbar}{m\\omega} = 3\\frac{\\hbar}{m\\omega} \\end{align*}\\] <p>This shows, as expected, that bosons tend to attract and fermions tend to repel as enforced by the symmetrisation postulate and associated exchange interaction.</p>"},{"location":"s-a-2/#question-2","title":"Question 2","text":"<p>Helium ground state energy</p> <ol> <li>Compute the direct integral for the ground state of helium and show the first-order correction to the energy is \\(E_{1s,1s}^{(1)} = \\frac{5}{2} \\mathrm{Ry} = 34~\\mathrm{eV}\\).</li> </ol> <p>Hint: For no reason, here is the spherical harmonic addition theorem:</p> <p> </p> <p>where \\(r_{&gt;}\\)/\\(r_{&lt;}\\) denoting the larger/smaller of the two distances \\(r_1\\) and \\(r_2\\).</p> <ol> <li>Imagine that we are back in week one, where the variational method was introduced. If we had guessed that the wave function of helium was roughly</li> </ol> <p> </p> <p>where \\(\\varphi_{nlm}(\\mathbf{r})\\) are the usual hydrogenic wave functions, explain why we would have calculated the identical result to that above, assuming that the interaction potential between electrons was the same. You can incorporate into your answer why I am nice for not having asked you to calculate the ground state energy for a trail wave function not of that form, for example, if \\(\\varphi(\\mathbf{r})=\\exp\\left(-\\alpha(r_1 - r_2)^2\\right)\\).</p>"},{"location":"s-a-2/#21","title":"2.1","text":"<p>Compute the direct integral for the ground state of helium and show the first-order correction to the energy is \\(E_{1s,1s}^{(1)} = \\frac{5}{2} \\mathrm{Ry} = 34~\\mathrm{eV}\\).</p> <p>Hint: For no reason, here is the spherical harmonic addition theorem:</p> \\[ \\frac{1}{\\left\\rvert \\mathbf{r_1} - \\mathbf{r_2} \\right\\rvert} = \\sum_{\\ell = 0}^{\\infty} \\sum_{m=-\\ell}^{\\ell} \\frac{4\\pi}{2\\ell+1} \\frac{r_{&lt;}^\\ell}{r_{&gt;}^{\\ell+1}} Y_\\ell^m {}^*    \\left(\\theta_1, \\phi_1\\right) Y_\\ell^m \\left(\\theta_2, \\phi_2\\right) \\] <p>where \\(r_{&gt;}\\)/\\(r_{&lt;}\\) denoting the larger/smaller of the two distances \\(r_1\\) and \\(r_2\\).</p> <p>The first order correction is calculated using perturbation theory, stating that</p> \\[     E_{1s,1s}^{(1)} = \\left\\langle \\psi_{1s,1s}^{SA} \\rvert H' \\rvert \\psi_{1s,1s}^{SA}\\right\\rangle \\] <p>which we saw in class defines the direct integral \\(J_{11}\\). At the time, I emphasised that such integrals had physical meanings, which was the important bit, and the actual numbers that popped out were just a computational exercise. Well, here we are.</p> <p>The direct integral of the ground state of helium is</p> \\[     E_{1s,1s}^{(1)} = \\int\\int\\psi_{100}^*\\left(\\mathbf{r}_1\\right)\\psi_{100}^*\\left(\\mathbf{r}_2\\right)\\frac{e^2}{4\\pi\\varepsilon_0 \\left\\rvert \\mathbf{r_1} - \\mathbf{r_2} \\right\\rvert}\\psi_{100}\\left(\\mathbf{r}_1\\right)\\psi_{100}\\left(\\mathbf{r}_2\\right) \\] <p>where the ground state wave function is</p> \\[     \\psi_{100}\\left(\\mathbf{r}\\right) = \\psi_{100}\\left(r,\\theta,\\phi\\right)=\\sqrt{\\frac{Z^3}{\\pi a_0^3}}e^{-Zr/a_0} \\] <p>As the hint states, to make this problem tractable, we can use the spherical harmonic addition theorem, which yields</p> \\[\\begin{align*}         E_{1 s, 1 s}^{(1)} &amp; =\\frac{Z^6 e^2}{4 \\pi^3 \\varepsilon_0 a_0^6} \\iint e^{-2 Z_1 / a_0} e^{-2 Z r_2 / a_0} \\sum_{\\ell=0}^{\\infty} \\sum_{m=-\\ell}^{\\ell} \\frac{4 \\pi}{2 \\ell+1} \\frac{r_{&lt;}^{\\ell}}{r_{&gt;}^{\\ell+1}} Y_\\ell^m{}^*\\left(\\theta_1, \\phi_1\\right) Y_\\ell^m\\left(\\theta_2, \\phi_2\\right) d^3 \\mathbf{r}_1 d^3 \\mathbf{r}_2 \\\\         &amp; =\\frac{Z^6 e^2}{4 \\pi^3 \\varepsilon_0 a_0^6} \\sum_{\\ell=0}^{\\infty} \\sum_{m=-\\ell}^{\\ell} \\frac{4 \\pi}{2 \\ell+1} \\iint e^{-2 Z\\left(r_1+r_2\\right) / a_0} \\frac{r_{&lt;}^{\\ell}}{r_{&gt;}^{\\ell+1}} Y_\\ell^m{}^*\\left(\\theta_1, \\phi_1\\right) Y_\\ell^m\\left(\\theta_2, \\phi_2\\right) d^3 \\mathbf{r}_1 d^3 \\mathbf{r}_2 \\end{align*}\\] <p>Now plugging in \\(\\ell = m = 0\\):</p> \\[\\begin{align*}         E_{1 s, 1 s}^{(1)}=\\frac{Z^6 e^2}{4 \\pi^3 \\varepsilon_0 a_0^6} &amp; (4 \\pi)^2 \\int_0^{\\infty} \\int_0^{\\infty} e^{-2 Z\\left(r_1+r_2\\right) / a_0} \\frac{1}{r_{&gt;}} r_1^2 d r_1 r_2^2 d r_1 \\\\         &amp; \\times \\int Y_{0}^{0}{}^*\\left(\\theta_1, \\phi_1\\right) Y_0^0\\left(\\theta_1, \\phi_1\\right) d \\Omega_1 \\int Y_0^0{}^*\\left(\\theta_2, \\phi_2\\right) Y_{0}^0\\left(\\theta_2, \\phi_2\\right) d \\Omega_2 \\end{align*}\\] <p>The spherical harmonics are orthonormal, so the angular integral evaluates to 1 and we are left with</p> \\[     E_{1 s, 1 s}^{(1)}=\\frac{4 Z^6 e^2}{\\pi \\varepsilon_0 a_0^6} \\int_0^{\\infty} \\int_0^{\\infty} e^{-2 Z\\left(r_1+r_2\\right) / a_0} \\frac{1}{r_{&gt;}} r_1^2 d r_1 r_2^2 d r_2 \\] <p>The trick here is to split the integral with \\(r_&gt;\\) into two parts, and with that, the rest is just cranking the handle:</p> \\[\\begin{align*}         E_{1 s, 1 s}^{(1)} &amp; =\\frac{4 Z^6 e^2}{\\pi \\varepsilon_0 a_0^6} \\int_0^{\\infty} e^{-2 Z r_1 / a_0} r_1^2 d r_1\\left[\\frac{1}{r_1} \\int_0^{r_1} e^{-2 Z {r_2} / a_0} r_2^2 d r_2+\\int_{r_1}^{\\infty} e^{-2 Z r_2 / a_0} \\frac{1}{r_2} r_2^2 d r_2\\right] \\\\         &amp; =\\frac{4 Z^6 e^2}{\\pi \\varepsilon_0 a_0^6} \\int_0^{\\infty} e^{-2 Z r_1 / a_0} r_1^2 d r_1 \\times \\bigg[ \\frac{1}{r_1}\\left(\\frac{a_0}{4 Z^3}\\right)\\left(a_0^2-e^{-2 Z_1 / a_0}\\left\\{a_0^2+2 a_0 r_1 Z+2 r_1^2 Z^2\\right\\}\\right) \\\\         &amp;\\hspace{5cm} +\\left(\\frac{a_0}{4 Z^2}\\right) e^{-2 Z_{r_1} / a_0}\\left\\{a_0+2 r_1 Z\\right\\} \\bigg] \\\\         &amp; =\\frac{Z^3 e^2}{\\pi \\varepsilon_0 a_0^5}\\bigg[         \\int_0^{\\infty}\\left(a_0^2 e^{-2 Z_1 / a_0}-e^{-4 Z_{\\mathrm{i}} / a_0}\\left\\{a_0^2+2 a_0 r_1 Z+2 r_1^2 Z^2\\right\\}\\right) r_1 d r_1 \\\\         &amp;\\hspace{2cm} + \\int_0^{\\infty} e^{-4 Z_{\\mathrm{i}} / a_0}\\left(a_0 Z+2 r_1 Z^2\\right) r_1^2 d r_1 \\bigg] \\\\         &amp; =\\frac{Z^3 e^2}{\\pi \\varepsilon_0 a_0^5}\\left[\\frac{a_0^4}{4 Z^2}-\\frac{a_0^4}{16 Z^2}-\\frac{a_0^4}{16 Z^2}-\\frac{3 a_0^4}{64 Z^2}+\\frac{a_0^4}{32 Z^2}+\\frac{3 a_0^4}{64 Z^2}\\right] \\\\         &amp; =\\frac{5 Z e^2}{32 \\pi \\varepsilon_0 a_0}=\\frac{5}{8}\\left(\\frac{Z e^2}{4 \\pi \\varepsilon_0 a_0}\\right) \\\\ \\end{align*}\\] <p>Plugging in the values for helium we get</p> \\[     E_{1 s, 1 s}^{(1)}=\\frac{5}{8} Z\\left(\\frac{e^2}{4 \\pi \\varepsilon_0 a_0}\\right)=\\frac{5}{8} 2(2 \\mathrm{Ry})=\\frac{5}{2} 13.6 \\mathrm{eV}=34~\\mathrm{eV} \\]"},{"location":"s-a-2/#22","title":"2.2","text":"<p>Imagine that we are back in week one, where the variational method was introduced. If we had guessed that the wave function of helium was roughly</p> \\[   \\psi_{He} = \\varphi_{nlm}(\\mathbf{r}_1)\\varphi_{nlm}(\\mathbf{r}_2) \\] <p>where \\(\\varphi_{nlm}(\\mathbf{r})\\) are the usual hydrogenic wave functions, explain why we would have calculated the identical result to that above, assuming that the interaction potential between electrons was the same. You can incorporate into your answer why I am nice for not having asked you to calculate the ground state energy for a trail wave function \\emph{not} of that form, for example, if \\(\\varphi(\\mathbf{r})=\\exp\\left(-\\alpha(r_1 - r_2)^2\\right)\\).</p> <p>In order to calculate the ground state energy using the variational method, we need to compute the expectation value</p> \\[     E \\le \\left\\langle \\psi | H | \\psi \\right\\rangle \\] <p>It is usual to split this into kinetic, potential and interaction terms, where</p> \\[     H = T + V + V_{int} = ((T_1 + V_1) + (T_2 + V_2)) + V_{int}. \\] <p>As we compute these expectation values, we have \\(\\varphi_{nlm}(\\mathbf{r}_{1,2})\\) as eigenstates of \\(T_1 + V_1\\) and \\(T_2 + V_2\\) respectively, meaning that each of their contributions to the expectation value would be</p> \\[   -Z^2\\,\\mathrm{Ry} \\] <p>or \\(-108.8~\\mathrm{eV}\\) in total. The final component of the expectation value would be calculate as</p> \\[   \\left\\langle \\psi | V_{int} | \\psi \\right\\rangle \\] <p>which is the direct integral that was computed above.</p> <p>The reason that I am nice is because this is only true because \\(\\varphi_{nlm}(\\mathbf{r}_{1,2})\\) are eigenstates; had I given you something like the Gaussian listed (as I had been planning), you would have had to explicitly compute the expectation values for \\(T_{1,2} + V_{1,2}\\), meaning lots of nasty integration!</p>"},{"location":"s-a-2/#question-3","title":"Question 3","text":"<p>Density of states</p> <p>In class, it was stated that the density of states \\(g(E)\\) can be calculated as the Fourier transform of the emitted field, and for an exponentially decaying excited state, the density of states is</p> \\[ g(E) = \\frac{\\hbar A_{21}/2\\pi}{\\left(E-\\hbar\\omega_{12}\\right)^2 + \\left( \\frac{\\hbar A_{21}}{2}\\right)^2} \\] <p>We are going to show exactly this.</p> <ol> <li>Explain why that given a population which decays exponentially with time dependence \\(e^{-t/\\tau}\\), the field decays with the a time dependence \\(e^{-t/2\\tau}\\).</li> <li>Calculate the Fourier transform of the emitted field,            and hence find the frequency spectrum of the radiated power in spontaneous emission(1).</li> <li>Convert this frequency spectrum to an energy spectrum, and normalise it (to 1) and voil\u00c3\u00a0, you should arrive at the density of states above (2).</li> </ol> <ol> <li>Hint: The power spectral density is computed as the square of the absolute value of the Fourier transform.</li> <li>Hint: You will need to make the (very valid) approximation that the linewidth \\(A_{21}\\) is much less than the resonance frequency \\(\\omega_{21}\\)</li> </ol>"},{"location":"s-a-2/#31","title":"3.1","text":"<p>Explain why that given a population which decays exponentially with time dependence \\(e^{-t/\\tau}\\), the field decays with the a time dependence \\(e^{-t/2\\tau}\\).</p> <p>The radiated power exhibits the same time dependence, but the power is calculated as the field squared, hence the factor of two.</p>"},{"location":"s-a-2/#32","title":"3.2","text":"<p>Calculate the Fourier transform of the emitted field,</p> \\[     E(t) = \\begin{cases}             0 &amp; t &lt; 0 \\\\             E_0 e^{-t/2\\tau} e^{-i\\omega_{21}t} &amp; t\\geq 0            \\end{cases} \\] <p>and hence find the frequency spectrum of the radiated power in spontaneous emission(1).</p> <ol> <li>Hint: The power spectral density is computed as the square of the absolute value of the Fourier transform.</li> </ol> <p>The Fourier transform of \\(E(t)\\) is</p> \\[\\begin{align*}     \\mathscr{F}\\{E(t); t\\rightarrow \\omega\\} &amp;\\equiv\\hat{E}(\\omega) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} E(t) e^{-i \\omega t} dt \\\\     &amp; = \\frac{1}{\\sqrt{2\\pi}}\\int_{0}^{\\infty} E_0 e^{-t/2\\tau} e^{i\\omega_{21}t} e^{-i \\omega t} dt \\\\     &amp; = \\frac{E_0}{\\sqrt{2\\pi}}\\int_{0}^{\\infty} e^{-i(\\omega - \\omega_{21})t - t/2\\tau} dt \\\\     &amp; = \\frac{E_0}{\\sqrt{2\\pi}} \\left. \\frac{e^{-i(\\omega - \\omega_{21})t - t/2\\tau}}{-i(\\omega - \\omega_{21}) - 1/2\\tau} \\right\\rvert_0^\\infty \\\\     &amp; = \\frac{E_0}{\\sqrt{2\\pi}} \\frac{1}{i(\\omega - \\omega_{21}) + 1/2\\tau} \\end{align*}\\] <p>The power is then</p> \\[\\begin{align*}     P(\\omega) &amp; = \\left\\rvert\\hat{E}(\\omega)\\right\\rvert^2 \\propto \\left\\rvert \\frac{1}{i(\\omega - \\omega_{21}) + 1/2\\tau} \\right\\rvert^2 \\\\     &amp; \\propto \\frac{1}{(\\omega - \\omega_{21})^2 + (1/2\\tau)^2} \\\\     &amp; = \\frac{1}{(\\omega - \\omega_{21})^2 + (A_{21}/2)^2} \\end{align*}\\] <p>where we have used the definition of the Einstein \\(A\\) coefficient as \\(1/\\tau\\), and have binned the factor out the front as we are going to normalise the spectrum in the next part.</p>"},{"location":"s-a-2/#33","title":"3.3","text":"<p>Convert this frequency spectrum to an energy spectrum, and normalise it (to 1) and voil\u00c3\u00a0, you should arrive at the density of states above (1).</p> <ol> <li>Hint: You will need to make the (very valid) approximation that the linewidth \\(A_{21}\\) is much less than the resonance frequency \\(\\omega_{21}\\)</li> </ol> <p>Converting to an energy spectrum, we have</p> \\[ g(E) \\propto \\frac{1}{(E - \\hbar\\omega_{21})^2 + (\\hbar A_{21}/2)^2} \\] <p>which we need to normalise. Doing so:</p> \\[ \\int_0^\\infty g(E) dE = \\int_0^\\infty \\frac{C}{(E - \\hbar\\omega_{21})^2 + (A_{21}/2)^2} dE = 1 \\] <p>Making the substitution \\(x = E-\\hbar\\omega_{21}\\)</p> \\[\\begin{align*}   1 &amp; = \\int_{-\\hbar\\omega_{21}}^\\infty \\frac{C}{x^2 + (A_{21}/2)^2} dx \\\\   &amp; = C\\left.\\frac{\\tan^{-1}(2x/\\hbar A_21)}{\\hbar A_{21}/2}\\right\\rvert_{-\\hbar\\omega_{21}}^{\\infty} \\\\   &amp; = \\frac{2C}{\\hbar A_{21}}\\left(\\frac{\\pi}{2}-\\tan^{-1}\\left(-\\frac{\\omega_{21}}{A_{21}}\\right)\\right) \\end{align*}\\] <p>Now making the assumption that \\(A_{21}\\ll\\omega_{21}\\) gives</p> \\[     1=\\frac{2\\pi C}{\\hbar A_{21}} \\Rightarrow C = \\frac{\\hbar A_{21}}{2\\pi} \\] <p>and ultimately the desired result</p> \\[     g(E) = \\frac{\\hbar A_{21}/2\\pi}{\\left(E-\\hbar\\omega_{12}\\right)^2 + \\left( \\frac{\\hbar A_{21}}{2}\\right)^2} \\]"},{"location":"s-a-2/#question-4","title":"Question 4","text":"<p>Absorption of light</p> <p>Light of frequency \\(\\omega\\) propagates in the \\(z\\) direction and is incident on an ensemble of two-level atoms. In the steady state, we can model the absorption process by assuming that the intensity falls off as \\(I(z)=I_0e^{-\\alpha z}\\).</p> <ol> <li>Show that \\(\\alpha = k\\,\\mathrm{Im}\\left[\\chi\\right]\\) where k is the wavenumber</li> <li>Show that in the case of homogeneous broadening, the absorption per unit length is given by  </li> <li>Assuming resonant conditions, by introducing the on-resonance saturation parameter, \\(s_0=\\frac{2\\Omega^2}{\\Gamma^2}\\), derive the \\emph{Beer-Lambert law}:  where \\(\\frac{I}{I_{sat}}=s_0\\).</li> <li>In the high-intensity limit (\\(s\\rightarrow\\infty\\)), this reduces to  Comment on the significance of this, in particular, what happens to the transmission as a function of light as a function of incident intensity?</li> </ol>"},{"location":"s-a-2/#41","title":"4.1","text":"<p>Show that \\(\\alpha = k\\,\\mathrm{Im}\\left[\\chi\\right]\\) where k is the wavenumber</p> <p>The electric field of the light field in the ensemble (with refractive index \\(n\\)) is given by</p> \\[\\begin{equation*}     \\mathbf{E}(z,t)=\\mathbf{E}_0e^{i\\left(k n_R z-\\omega t\\right)}e^{-k n_I z}. \\end{equation*}\\] <p>We know that the intensity is related to the electric field through</p> \\[\\begin{align*}     I(z,t)&amp;=\\frac{1}{2}\\epsilon_0 c |\\mathbf{E}(z,t)|^2 \\\\     &amp;= \\frac{\\epsilon_0 c}{2}|\\mathbf{E}_0|^2 e^{-2k n_I z} \\end{align*}\\] <p>when we plug in the electric field above. Comparing this to the form \\(I(z)=I_0e^{-\\alpha z}\\) is clear that</p> \\[\\begin{align*}     \\alpha&amp;=2k n_I \\\\     &amp;=k\\,\\mathrm{Im}\\left[\\chi\\right] \\end{align*}\\] <p>since \\(n=\\sqrt{1+\\chi}\\approx1+\\frac{\\chi}{2}\\) for small \\(\\chi\\).</p>"},{"location":"s-a-2/#42","title":"4.2","text":"<p>Show that in the case of homogeneous broadening, the absorption per unit length is given by  </p> <p>The light absorbed by a slab of atoms with thickness \\(dz\\) is</p> \\[\\begin{equation*}     dI=-\\alpha dz I \\end{equation*}\\] <p>which implies that</p> \\[\\begin{equation*}     \\frac{dI}{dz}=-k\\,\\mathrm{Im}\\left[\\chi\\right]I. \\end{equation*}\\] <p>We showed in class that the susceptibility is related to the slowly varying coherence \\(\\sigma_{12}\\) via</p> \\[\\begin{equation*}     \\chi=\\frac{2nd_{12}^2}{\\epsilon_0 \\hbar \\Omega}\\sigma_{12}. \\end{equation*}\\] <p>We also saw that in the steady state, the slowly varying coherence \\(\\sigma_{12}\\) evaluated for \\(t\\rightarrow\\infty\\) is given by</p> \\[\\begin{equation*}     \\sigma_{12}\\bigg\\rvert_{t\\rightarrow\\infty}=\\frac{i\\Omega}{2\\gamma_{\\perp}} \\frac{1-\\frac{i\\Delta}{\\gamma_{\\perp}}}{1+\\left(\\frac{\\Delta}{\\gamma_{\\perp}}\\right)^2+\\frac{\\Omega^2}{\\gamma_{\\perp}\\Gamma}} \\end{equation*}\\] <p>which in a medium with homogeneous broadening (\\(\\gamma_{\\perp}=\\frac{\\Gamma}{2}\\)) reduces to</p> \\[\\begin{equation*}     \\sigma_{12}\\bigg\\rvert_{t\\rightarrow\\infty}=\\frac{i\\Omega}{\\Gamma}\\frac{1-i\\frac{2\\Delta}{\\Gamma}}{1+\\left(\\frac{2\\Delta}{\\Gamma}\\right)^2+2\\left(\\frac{\\Omega}{\\Gamma}\\right)^2}. \\end{equation*}\\] <p>With this, it is simply a matter of plugging and playing:</p> \\[\\begin{align*}     \\frac{dI}{dz}&amp;=-\\frac{k I 2n d_{12}^2}{\\epsilon_0\\hbar\\Gamma} \\frac{1}{1+\\left(\\frac{2\\Delta}{\\Gamma}\\right)^2+2\\left(\\frac{\\Omega}{\\Gamma}\\right)^2}\\\\     &amp;=-\\frac{k I nd_{12}^2}{\\epsilon_0\\hbar}\\frac{\\Gamma}{2} \\frac{1}{\\Delta^2+\\frac{\\Gamma^2}{4}+\\frac{\\Omega^2}{2}} \\end{align*}\\] <p>where we pulled out a factor of \\(\\frac{4}{\\Gamma^2}\\) from the denominator of \\(\\mathrm{Im}\\left[\\sigma_{12}\\bigg\\rvert_{t\\rightarrow\\infty}\\right]\\) to achieve the desired form.</p>"},{"location":"s-a-2/#43","title":"4.3","text":"<p>Assuming resonant conditions, by introducing the on-resonance saturation parameter, \\(s_0=\\frac{2\\Omega^2}{\\Gamma^2}\\), derive the Beer-Lambert law:  where \\(\\frac{I}{I_{sat}}=s_0\\).</p> <p>As we are now talking about an resonant system, we are dealing with a detuning of zero (\\(\\Delta=0\\)). Next, as the on-resonance saturation parameter is given by \\(s_0=\\frac{2\\Omega^2}{\\Gamma^2}\\), it is wise to write everything in terms of \\(\\frac{2\\Omega^2}{\\Gamma^2}\\):</p> \\[\\begin{equation*}     \\frac{dI}{dz}=-\\frac{k I nd_{12}^2}{\\epsilon_0\\hbar} \\frac{\\Gamma}{2} \\frac{2}{\\Omega^2} \\frac{\\frac{2\\Omega^2}{\\Gamma^2}}{1+\\frac{2\\Omega^2}{\\Gamma^2}}. \\end{equation*}\\] <p>Now</p> \\[\\begin{equation*}     I d_{12}^2 = \\frac{1}{2}\\epsilon_0 c\\frac{|\\mathbf{E}_0|^2 d_{12}^2}{\\left(\\hbar\\Omega\\right)^2} \\end{equation*}\\] <p>which implies</p> \\[\\begin{equation*}     \\frac{dI}{dz}=-k c n \\hbar \\frac{\\Gamma}{2} \\frac{s_0}{1+s_0} \\end{equation*}\\] <p>and as \\(k=\\frac{\\omega}{c}\\), we have</p> \\[\\begin{equation*}     \\frac{dI}{dz}=-n \\hbar \\omega \\hbar \\frac{\\Gamma}{2} \\frac{s_0}{1+s_0}. \\end{equation*}\\] <p>Substituting \\(s_0=\\frac{I}{I_{sat}}\\), we arrive at the Beer-Lambert law:</p> \\[\\begin{equation*}     \\frac{1}{I}\\frac{dI}{dz}=-\\frac{n\\hbar\\omega}{I_{sat}}\\frac{\\Gamma}{2}. \\end{equation*}\\]"},{"location":"s-a-2/#44","title":"4.4","text":"<p>In the high-intensity limit (\\(s\\rightarrow\\infty\\)), this reduces to  Comment on the significance of this, in particular, what happens to the transmission as a function of light as a function of incident intensity?</p> <p>The Beer-Lambert law as derived above returns the behavior that we expect, \\(I(z)=I_0e^{-\\alpha z}\\), but only for small intensities. Explicitly, we identify that the intensity decays exponentially as light propagates through the medium and is absorbed by the two-level system.</p> <p>In the high-intensity limit we have  which has the solution \\(I(z)=I_0 - n\\hbar\\omega\\frac{\\Gamma}{2}z\\). We now identify that there is a linear absorption in the medium. But why?</p> <p>As we discussed in class, transitions experience saturation. The physical origin of saturation is intuitive: a system can only scatter so many photons per unit time. So once the system is in a regime whereby the medium cannot scatter any more light, we observe an increase in transmission. This simple principle has some extremely practical applications, including a spectroscopic technique used to stabalise laser systems to atomic transitions with an incredible degree of accuracy, so-called saturated absorption spectroscopy. Understanding of saturation and related spectroscopy was rewarded with the Nobel Prize in physics in 1981.</p>"},{"location":"s-a-2/#question-5","title":"Question 5","text":"<p>The density matrix and OBEs</p> <p>Here we are going to compute some density matrices, and then we are going to solve the optical Bloch equations.</p> <ol> <li>What is the density matrix for the state \\(\\psi = 0.577\\ket{+} + 0.577(1+i)\\ket{-}\\) in the \\(\\ket{\\pm}\\) basis?</li> <li>What is the expectation value for \\(S_y\\)?</li> <li>What is a mixed state which would give the same probabilities of measuring the system in states \\(\\ket{+}\\) and \\(\\ket{-}\\)</li> <li>Explain why we care about the difference between pure and mixed states, and what differences one might observe in experiments when using either a pure or mixed state</li> </ol> <p><code>\\begin{Computational content}</code></p> <p>The computation of density matrices can be tedious, especially once we have larger systems. Consider the state</p> \\[    \\ket{\\psi} = \\frac{1}{\\sqrt{10}}\\left( 3\\ket{++} + 1\\ket{+-} + 4 \\ket{-+} + 2\\ket{--} \\right) \\] <p>in the basis \\(|J_1\\,J_2\\,m_{J_1}\\, m_{J_2}\\rangle\\), where \\(J_1, J_2 = 1/2\\) and have been dropped from the state labels.</p> <p>Using <code>Python</code> (or equivalent) 5. Compute the density matrix \\(\\rho\\) for the above state \\(\\ket{\\psi}\\) 6. Compute the expectation value \\(\\langle \\mathbf{F}^2 \\rangle\\) for this state</p> <p>We now turn to the optical Bloch equations. We have seen that in the rotating wave approximation, we can solve them analytically, but such approximations are not always valid, and more generally, in large systems the analytical solutions become nightmarish (e.g. see appendix D of my Ph.D thesis. Consequently, we seek to solve them numerically.</p> <p>Using <code>Python</code> 7. Compute and plot the evolution of the matrix elements \\(\\rho_{11}, \\rho_{12}, \\rho_{21},\\) and \\(\\rho_{22}\\) as a function of time for realistic experimental parameters(1). 8. Use the computational package <code>[QuTiP](https://qutip.readthedocs.io/en/master/index.html)</code> to produce plots of the same matrix elements(2).</p> <p>As computational experience, competence, and confidence will vary across the class, I have provided a jupyter notebook to provide a few hot tips.</p> <ol> <li>Hint: This means that you will need to figure out what realistic parameters are.</li> <li>Hint: This is an exercise to demonstrate that using established libraries can make complex tasks simple, and thus should not require any complex computation \\emph{per se}</li> </ol>"},{"location":"s-a-2/#51","title":"5.1","text":"<p>What is the density matrix for the state \\(\\psi = 0.577\\ket{+} + 0.577(1+i)\\ket{-}\\) in the \\(\\ket{\\pm}\\) basis?</p>"},{"location":"s-a-2/#52","title":"5.2","text":"<p>What is the expectation value for \\(S_y\\)?</p>"},{"location":"s-a-2/#53","title":"5.3","text":"<p>What is a mixed state which would give the same probabilities of measuring the system in states \\(\\ket{+}\\) and \\(\\ket{-}\\)</p>"},{"location":"s-a-2/#54","title":"5.4","text":"<p>Explain why we care about the difference between pure and mixed states, and what differences one might observe in experiments when using either a pure or mixed state</p>"},{"location":"s-a-2/#55","title":"5.5","text":"<p>Compute the density matrix \\(\\rho\\) for the above state \\(\\ket{\\psi}\\)</p>"},{"location":"s-a-2/#56","title":"5.6","text":"<p>Compute the expectation value \\(\\langle \\mathbf{F}^2 \\rangle\\) for this state</p>"},{"location":"s-a-2/#57","title":"5.7","text":"<p>Compute and plot the evolution of the matrix elements \\(\\rho_{11}, \\rho_{12}, \\rho_{21},\\) and \\(\\rho_{22}\\) as a function of time for realistic experimental parameters(1).</p> <ol> <li>Hint: This means that you will need to figure out what realistic parameters are.</li> </ol>"},{"location":"s-a-2/#58","title":"5.8","text":"<p>Use the computational package <code>[QuTiP](https://qutip.readthedocs.io/en/master/index.html)</code> to produce plots of the same matrix elements(1).</p> <ol> <li>Hint: This is an exercise to demonstrate that using established libraries can make complex tasks simple, and thus should not require any complex computation per se</li> </ol> <pre><code>\n</code></pre> <p> </p> A schematic of the Stern-Gerlach experiment"},{"location":"s-a-3/","title":"Assignment 3","text":"<pre>\n\nfrom Atomic import *\n\nfrom common import draw_classic_axes, configure_plotting\n\nconfigure_plotting()\n\n  </pre> <pre><code>Atomic.py - the package for atomic physics content for use in KYA323 has been loaded.\n\nCore package versions are as follows:\nMatplotlib 3.7.1\nNumPy 1.25.0\n</code></pre>"},{"location":"s-a-3/#assignment-three","title":"Assignment three","text":"<p>The third assignment covers the content from weeks 5 and 6, which includes topics such as ensembles, laser cooling and trapping, dressed states (eigenstates of a coupled system), and Ramsey interferometry.</p> <p> A .pdf version of the assignment as distributed     </p>"},{"location":"s-a-3/#question-1","title":"Question 1","text":"<p>Stern-Gerlach: measurement edition</p> <p>Consider an ensemble of silver atoms incident on an inhomogeneous magnetic field (i.e. the OG Stern-Gerlach experiment with spin-1/2 particles).</p> <ol> <li>Suppose we have measured \\(\\expectation{S_x}\\), \\(\\expectation{S_y}\\) and \\(\\expectation{S_z}\\) but we do not know the initial state of the system. Is it possible to know the density matrix of the system, \\(\\rho\\)?</li> <li>Now suppose that we know that the system is pure. Is it possible to determine \\(\\rho\\) with fewer measurements?</li> </ol>"},{"location":"s-a-3/#11","title":"1.1","text":"<p>Suppose we have measured \\(\\expectation{S_x}\\), \\(\\expectation{S_y}\\) and \\(\\expectation{S_z}\\) but we do not know the initial state of the system. Is it possible to know the density matrix of the system, \\(\\rho\\)?</p> <p>The general form of the density matrix for an ensemble of spin-1/2 particles is</p> \\[\\begin{equation*}     \\rho = \\begin{pmatrix} \\rho_{11} &amp; \\rho_{12} \\\\ \\rho_{21} &amp; \\rho_{22} \\end{pmatrix} \\end{equation*}\\] <p>Essentially, we want to see if the expectation values \\(\\expectation{S_x}\\), \\(\\expectation{S_y}\\) and \\(\\expectation{S_z}\\) can uniquely constrain \\(\\rho_{11}, \\rho_{12}, \\rho_{21}\\) and \\(\\rho_{22}\\). We do not know anything about our system (for example, we cannot assume that it is pure) nor can we assume that the \\(\\rho\\) is diagonal in our basis of choice. It is worth noting that \\(\\rho\\) will be diagonal in a basis, but currently we do not know what is that basis that. We do know that \\(\\rho\\) is both normalised and Hermitian, that is \\(\\mathrm{tr}\\left(\\rho\\right)=1\\) and \\(\\rho=\\rho^{\\dagger}\\), so it is convenient to write</p> \\[\\begin{equation*}     \\rho = \\begin{pmatrix} \\frac{1}{2}+c &amp; a-ib \\\\ a+ib &amp; \\frac{1}{2}-c \\end{pmatrix}. \\end{equation*}\\] <p>We now have three independent parameters which describe \\(\\rho\\). Recalling the Pauli matrices</p> \\[\\begin{equation*}     \\sigma_x=\\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix},\\, \\sigma_y=\\begin{pmatrix} 0 &amp; -i \\\\ i &amp; 0 \\end{pmatrix},\\, \\sigma_z=\\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix} \\end{equation*}\\] <p>we identify that</p> \\[\\begin{equation*}     \\rho = \\frac{1}{2}\\mathbbm{1} + a\\sigma_x + b\\sigma_y + c\\sigma_z \\end{equation*}\\] <p>and we are really cooking with gas now. The in \\(z\\) basis, we know the spin operators \\(S_{x,y,z}\\) act to return \\(\\frac{\\hbar}{2}\\sigma_{x,y,z}\\). Combining this with knowledge \\(\\mathrm{Tr}\\left(\\sigma_i\\sigma_j\\right)=2\\delta_{ij}\\), we have</p> \\[\\begin{align*}     \\expectation{S_x}&amp;=\\mathrm{Tr}\\left(\\rho\\,\\frac{\\hbar\\sigma_x}{2}\\right)=\\frac{\\hbar}{2}\\mathrm{Tr}\\left(\\frac{1}{2}\\sigma_x + a\\mathbbm{1} + b\\sigma_y\\sigma_x + c\\sigma_z\\sigma_x \\right)\\\\     &amp;= \\hbar a \\end{align*}\\] <p>and similarly</p> \\[\\begin{align*}     \\expectation{S_y} &amp;=\\hbar b\\\\     \\expectation{S_z} &amp;=\\hbar c. \\end{align*}\\] <p>Life is good: now we have</p> \\[\\begin{align*}     \\rho &amp;= \\frac{1}{2}\\mathbbm{1} + \\frac{\\expectation{S_x}}{\\hbar}\\sigma_x + \\frac{\\expectation{S_y}}{\\hbar}\\sigma_y + \\frac{\\expectation{S_z}}{\\hbar}\\sigma_z\\\\     &amp;= \\frac{1}{\\hbar}\\begin{pmatrix} \\frac{\\hbar}{2}+\\expectation{S_z} &amp; \\expectation{S_x}-i\\expectation{S_y} \\\\ \\expectation{S_x}+i\\expectation{S_y} &amp; \\frac{\\hbar}{2}-\\expectation{S_z} \\end{pmatrix} \\end{align*}\\] <p>which shows that yes, it is possible to construct \\(\\rho\\) with knowledge of \\(\\expectation{S_x}\\), \\(\\expectation{S_y}\\) and \\(\\expectation{S_z}\\).\\</p>"},{"location":"s-a-3/#12","title":"1.2","text":"<p>Now suppose that we know that the system is pure. Is it possible to determine \\(\\rho\\) with fewer measurements?</p> <p>If the state is pure, we know that \\(\\mathrm{Tr}\\left(\\rho^2\\right)=1\\). The question then becomes can I uniquely express \\(\\expectation{S_z}\\) in terms of \\(\\expectation{S_x}\\), \\(\\expectation{S_y}\\) and thus reduce the number of independent parameters in the density matrix? The condition \\(\\mathrm{Tr}\\left(\\rho^2\\right)=1\\) means that</p> \\[\\begin{equation*}     \\mathrm{Tr}\\left(\\rho^2\\right)=\\mathrm{Tr}\\left(\\frac{1}{4}\\mathbbm{1}+\\frac{\\expectation{S_x}^2}{\\hbar^2}\\sigma_x^2 + \\frac{\\expectation{S_y}^2}{\\hbar^2}\\sigma_y^2 + \\frac{\\expectation{S_z}^2}{\\hbar^2}\\sigma_z^2 + \\cdots\\right) \\end{equation*}\\] <p>where \\(\\cdots\\) hides the cross terms which have zero trace. Then  </p> <p>as all Pauli matrices have trace 2 when squared, and so</p> \\[\\begin{equation*}     \\expectation{S_z}=\\pm\\hbar\\sqrt{\\frac{1}{4}-\\frac{\\expectation{S_x}^2}{\\hbar^2}-\\frac{\\expectation{S_y}^2}{\\hbar^2}}. \\end{equation*}\\] <p>This shows that knowing the system is pure almost allows us to capture \\(\\rho\\) with just two free parameters, but without further information it is not possible. Clearly, if we also know the sign of \\(\\expectation{S_z}\\), we would be in business.</p>"},{"location":"s-a-3/#question-2","title":"Question 2","text":"<p>Laser cooling</p> <p>We are going to perform some calculations based on the simplest laser-cooling system, namely a laser beam incident on a source of thermal atoms.</p> <ol> <li>Find the distance required to stop a room-temperature rubidium atom with the resonant scattering force</li> <li>Show that rubidium atoms with velocity \\(v=350~\\mathrm{m/s}\\) are resonant with a counter-propagating laser with a frequency detuning of \\(450\\unit{MHz}\\)</li> <li>In class, we looked at using a magnetic field to deal with the changing Doppler shift of the light as seen by the atom. An alternative method is to change the laser frequency, known as chirping the frequency. What is the maximum rate at which one could chirp the laser frequency to cool rubidium atoms? Assuming a Doppler width of \\(1\\unit{GHz}\\), how long would it take to slow the atoms?</li> <li>In laser cooling experiments, it turns out to be necessary to have (at least) two laser beams to actually cool atoms; in the case of rubidium, only two are required. Suggest what is the reason that multiple lasers are required, and describe what properties (e.g. intensity, frequency, polarisation) that these laser beams should have, and why?</li> </ol> <p>The Magneto-Optical Trap (MOT) is the workhorse of atomic physics, having revolutionised how ensembles of atoms can be prepared and interrogated.</p> <ol> <li>Explain what a MOT is, and how it works</li> <li>Imagine that you have a MOT: it is completely standard except you altered the beams which propagate along the vertical axis to have a frequency difference between thnormally, all six beams have the same detuning \\(\\Delta \\approx - \\Gamma/2\\), but here the beams in the horizontal plane have the detuning \\(\\Delta\\), but the vertical beams each ha detuning \\(\\Delta \\pm \\delta\\). What will happen?</li> <li>Atoms trapped in a MOT are typically imaged using absorption imaging, that is, illuminating the atom cloud with an on-resonant laser beam, and then imaging the profile of laser beam, which will have a shadow where the atoms were. What effect will this imaging have on the atoms?</li> <li>Assuming that the system can be described in the steady state, what would be the effect of imaging with an slightly detuned laser?</li> </ol>"},{"location":"s-a-3/#21","title":"2.1","text":"<p>Find the distance required to stop a room-temperature rubidium atom with the resonant scattering force</p> <p>The acceleration experienced by the rubidium atom will be</p> \\[\\begin{align*}     a_{\\max } &amp; =\\frac{F_{\\max }}{M} = \\frac{\\hbar k}{2 M \\tau} = \\frac{h}{2 M \\lambda \\tau}=\\frac{(h c) c}{2 M c^2 \\lambda \\tau} \\\\     &amp; =\\frac{(1240 \\mathrm{eV~nm})\\left(3 \\times 10^8 \\mathrm{~m} / \\mathrm{s}\\right)}{2(85 \\times 931 \\mathrm{MeV})(780 \\mathrm{~nm})\\left(27 \\times 10^{-9} \\mathrm{~s}\\right)}=1.12 \\times 10^5 \\mathrm{~m} / \\mathrm{s}^2 \\end{align*}\\] <p>and from basic kinematics, the distance to stop the atom with an initial velocity \\(v_0\\) is</p> \\[     v^2=v_0^2-2 a z \\Rightarrow z=\\frac{v_0^2}{2 a} \\] <p>We can calculate \\(v_0\\) from the thermal distribution, since the most probable velocity will be \\(M v^2=2 k_B T\\), so the stopping distance for a rubidium atom ( \\(M=85 \\unit{amu}, \\lambda=780 \\unit{~nm}, \\tau=27 \\unit{~ns})\\) is</p> \\[\\begin{align*}     z &amp; =\\frac{v_0^2}{2 a}=\\frac{2 k_B T}{2 M} \\frac{2 M \\lambda \\tau}{h}=\\frac{2 k_B T \\lambda \\tau}{h} \\\\     &amp; =\\frac{2\\left(8.62 \\times 10^{-5} \\mathrm{eV} / \\mathrm{K}\\right)(300 \\mathrm{~K})\\left(780 \\times 10^{-9} \\mathrm{~m}\\right)\\left(27 \\times 10^{-9} \\mathrm{~s}\\right)}{\\left(4.14 \\times 10^{-15} \\mathrm{eVs}\\right)}=0.26 \\mathrm{~m} \\end{align*}\\]"},{"location":"s-a-3/#22","title":"2.2","text":"<p>Show that rubidium atoms with velocity \\(v=350~\\mathrm{m/s}\\) are resonant with a counter-propagating laser with a frequency detuning of \\(450\\unit{MHz}\\)</p> <p>The Doppler shifted frequency of a laser beam with wave vector \\(\\mathbf{k}\\) as seen be an atom with velocity \\(\\mathbf{v}\\) is given by</p> \\[     \\omega_{atom} = \\omega_{laser} - \\mathbf{k}\\cdot\\mathbf{v} \\] <p>This means that the frequency detuning must match the doppler term, i.e.</p> \\[     2\\pi \\left( f_{laser} - f_0 \\right) = \\mathbf{k}\\cdot\\mathbf{v} \\] <p>and so for a laser which is colinear with the atomic velocity</p> \\[     f_{laser} - f_0 = \\Delta = \\frac{kv}{2\\pi} = \\frac{v}{\\lambda} \\] <p>and thus for an atom at \\(350\\unit{m/s}\\)</p> \\[     \\Delta = \\frac{v}{\\lambda} = \\frac{350}{780\\times10^{-9}} = 449 \\unit{MHz} \\]"},{"location":"s-a-3/#23","title":"2.3","text":"<p>In class, we looked at using a magnetic field to deal with the changing Doppler shift of the light as seen by the atom. An alternative method is to change the laser frequency, known as chirping the frequency. What is the maximum rate at which one could chirp the laser frequency to cool rubidium atoms? Assuming a Doppler width of \\(1\\unit{GHz}\\), how long would it take to slow the atoms?</p> <p>From the previous question, we have the condition that</p> \\[     \\Delta = \\frac{v}{\\lambda}. \\] <p>The chirp rate is the rate of change of the frequency</p> \\[     \\frac{df_{laser}}{dt} = \\frac{d}{dt} \\frac{v}{\\lambda} = \\frac{a}{\\lambda} \\] <p>which for the condition of maximum cooling (which would allow the maximum chirp rate) would have \\(a=a_{max}\\) and so</p> \\[     \\left. \\frac{df_{laser}}{dt} \\right\\rvert_{max} = \\frac{a_max}{\\lambda} = \\frac{h}{2 M \\lambda^2 \\tau}=\\frac{(h c) c}{2 M c^2 \\lambda \\tau}. \\] <p>In the case of rubidium, this evaluates to</p> \\[\\begin{align*}     \\left. \\frac{df_{laser}}{dt} \\right\\rvert_{max} &amp; = \\frac{(1240 \\mathrm{eVnm})\\left(3 \\times 10^8 \\mathrm{~m} / \\mathrm{s}\\right)}{2(85 \\times 931 \\mathrm{MeV})(780 \\mathrm{~nm})\\left(780 \\times 10^{-9} \\mathrm{~m}\\right)\\left(27 \\times 10^{-9} \\mathrm{~s}\\right)} \\\\     &amp; =1.43 \\times 10^{11} \\mathrm{~s}^{-2}=143 \\mathrm{MHz} / \\mathrm{ms} \\end{align*}\\] <p>and for a velocity distribution of order \\(1\\unit{GHz}\\), we would require \\(7\\unit{ms}\\) to sweep the frequency.</p>"},{"location":"s-a-3/#24","title":"2.4","text":"<p>In laser cooling experiments, it turns out to be necessary to have (at least) two laser beams to actually cool atoms; in the case of rubidium, only two are required. Suggest what is the reason that multiple lasers are required, and describe what properties (e.g. intensity, frequency, polarisation) that these laser beams should have, and why?</p> <p>Multiple lasers are required as the cooling of atoms requires that they cycle on a particular transition, and loose momentum by scattering photons, and if they are not scattering photons, they are not being cooled. So one must ask, why might they no longer scatter photons? * They might be out of resonance. This will happen, as the atoms slow and thus the Doppler shift changes, leading to a decrease in absorption; however, we have discussed ways of dealing with this: the Zeeman effect, or frequency chirping * They might not remain in the correct state. It is the latter point which must be corrected for: we want to ensure that our atom stays cycling on a given transition. Whilst in many cases, the two-level atom picture is sufficient for understanding population dynamics in multi-level atoms as there is little atomic response for light that is off-resonant, they decay mechanisms can really mess this up! For example, the \\(5S_{1/2}\\) ground state of Rb-85 has two hyperfine substates (\\(F=2,3\\)) and the excited state \\(5P_{3/2}\\) has four hyperfine substates (\\(F=1, 2, 3, 4\\))(1). If we had a laser coupling the \\(F = 2 \\rightarrow F^\\prime = 3\\) states the excited electron could decay back to the \\(F=2\\), but it could also decay into the \\(F=3\\) state, and then it would be lost to us!(2). So how can we solve this issue? Well, firstly, have selection rules operate in your favour: if we had \\(\\sigma^+\\) light coupling the stretched states \\(F = 3 \\rightarrow F^\\prime = 4\\), in principle, the electron would be cycling between the \\(M_F = 3 \\rightarrow M_F^\\prime = 4\\) substates, where decay is dipole forbidden to any other state. But even still, this is not enough, as there are small probabilities for other decay pathways, and if one cycles enough times (which occurs at the Rabi frequency) the atom will eventually find itself in a \"dark\" state, i.e. in a state which we aren't coupled to. In the case of a laser coupling the \\(5S_{1/2}~F = 3 \\rightarrow 5P_{3/2} ~ F^\\prime = 4\\) states, decay to the \\(5S_{1/2}~F = 2\\) is inevitable. So, in order to return atoms from the dark state, one must introduce a laser to return the atoms to the system so they can be cooled. It doesn't really matter too much what states are coupled, provided there is a reasonable decay pathway back to the cooling transition, for example a typical \"repump\" laser would couple the \\(5S_{1/2}~F = 2 \\rightarrow 5P_{3/2} ~ F^\\prime = 3\\) states. The polarisation should be set to maximise the chance of decay to the cooling transition state(s) and the frequency must match any Doppler shift of the atoms. The intensity shouldn't be too large, otherwise the repump laser will begin to alter the energy levels we care about! 1. Question: what is the nuclear spin \\(I\\) of the Rb-85 nucleus? 2. Question: what would be the probability this would happen?</p>"},{"location":"s-a-3/#25","title":"2.5","text":"<p>Explain what a MOT is, and how it works</p> <p>A magneto-optical trap is an arrangement of lasers and a magnetic field which is designed to both cool and trap atoms. Laser cooling is applied by individual laser beams which propagate along the \\(\\pm x, \\pm y, \\pm z\\) axes, meaning an atom which propagates along any of those axes is slowed. The secret sauce in the introduction of a magnetic field which is created by a pair of coils in an anti-Helmholtz configuration, resulting in a zero at the centre of the trap, which increases with distance from the trap centre. With careful control over the frequncy, the balance between detuning, Doppler shift (atomic velocity), and Zeeman shift (distance from the trap centre) can be used to both cool and trap atoms via the scattering of light. It should be noted that whilst there is a trapping magnetic field, this is distinct from a magnetic trap, which directly traps atoms due to the interaction between the magnetic dipole of the atoms and the field: one needs very large fields to do this.</p>"},{"location":"s-a-3/#26","title":"2.6","text":"<p>Imagine that you have a MOT: it is completely standard except you altered the beams which propagate along the vertical axis to have a frequency difference between thnormally, all six beams have the same detuning \\(\\Delta \\approx - \\Gamma/2\\), but here the beams in the horizontal plane have the detuning \\(\\Delta\\), but the vertical beams each ha detuning \\(\\Delta \\pm \\delta\\). What will happen?</p> <p>The MOT (in 1D) can be well understood by looking at the scattering force from a single laser beam</p> \\[     F_{scatt} = \\hbar k \\frac{\\Gamma}{2}\\frac{I/I_{sat}}{1+I/I_{sat}+(4\\Delta/\\Gamma)^2} \\] <p>when it is applied in both \\(\\pm\\) directions</p> \\[     F_{MOT} = F_{scatt}^{\\sigma^+}\\left(\\omega-kv-(\\omega_0 + \\beta z)\\right) - F_{scatt}^{\\sigma^-}\\left(\\omega+kv-(\\omega_0 - \\beta z)\\right) \\] <p>where</p> \\[     \\beta z = \\frac{g \\mu_{\\mathrm{B}}}{\\hbar}\\frac{dB}{dz}z \\] <p>is the Zeeman shift at point \\(z\\). If the horizontal (\\(x\\) and \\(y\\)) beams all have a detuning of \\(\\Delta\\), this is no different to a standard MOT, so the cooling and trapping would look identical. The spice in the mix here is the vertical beams having a relative detuning. We can make this detuning disappear with either the addition of a height-dependent magnetic field (not helpful), or with a Doppler shift (very helpful). This means that the \\(F_{MOT}\\) would look familiar if we shifted into a reference frame with Doppler shift \\(kv = \\delta\\), which means the MOT would function as normal, but cool atoms in a moving reference frame - thus making a beam of atoms!</p> <p>Building something to do this may or may not have been my Honours project.</p>"},{"location":"s-a-3/#27","title":"2.7","text":"<p>Atoms trapped in a MOT are typically imaged using absorption imaging, that is, illuminating the atom cloud with an on-resonant laser beam, and then imaging the profile of laser beam, which will have a shadow where the atoms were. What effect will this imaging have on the atoms?</p> <p>If we have cold atoms that are trapped in a MOT, this means that their Doppler shift is essentially negligible and therefore the will strongly absorb resonant light. When something strongly absorbs light, it scatters many photons, which ultimately exerts a radiation pressure and the associated acceleration, which will push them out of the trap. In essence, you could call this laser heating, but colloquially this is called blasting your atom trap: all your trapped atoms will be kicked out.</p>"},{"location":"s-a-3/#28","title":"2.8","text":"<p>Assuming that the system can be described in the steady state, what would be the effect of imaging with an slightly detuned laser?</p> <p>This is a tad complicated, but recall that in the steady state, we arrived at the result that the susceptibility of an atom was</p> \\[     \\chi = \\frac{2nd_{12}^2}{\\varepsilon_0\\hbar\\Omega}\\sigma_{12} \\] <p>and</p> \\[     \\sigma_{12} = \\frac{i\\Omega}{2\\gamma_\\perp} \\frac{1 - \\frac{i\\Delta}{\\gamma_\\perp}}{1+\\left(\\frac{\\Delta}{\\gamma_\\perp}\\right)^2 + \\frac{\\Omega^2}{\\gamma_\\perp \\Gamma}}. \\] <p>The key part of this is that the real component of the refractive index (\\(\\chi\\)) represents the phase accumulated in the medium, whereas the imaginary component quantifies the absorption. The plot below tells a better story:</p> <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the atomic coherence\ndef sigma21(Omega, Delta, Gamma, gamma):\n    \"\"\"\n    Calculate the blackbody radiation spectrum using Planck's law.\n\n    Parameters:\n    Omega (float): Rabi frequency\n    Delta (array): Detuning\n    Gamma (float): Linewidth\n    gamma (flat): Decay rate of coherence\n\n    Returns:\n    array: off-daigonal density matrix element\n    \"\"\"\n\n    term1 = - 1j * Omega / (2 * gamma)\n    term2 = 1 + ( 1j * Delta / gamma)\n    term3 = 1 + (Delta / gamma) ** 2 + ((Omega ** 2) / (gamma * Gamma))\n\n    return term1 * term2 / term3\n\ndef chi(Omega, Delta, Gamma, gamma, n, d):\n    \"\"\"\n    Calculate the blackbody radiation spectrum using Planck's law.\n\n    Parameters:\n    Omega (float): Rabi frequency\n    Delta (array): Detuning\n    Gamma (float): Linewidth\n    gamma (flat): Decay rate of coherence\n    n (flat): number density\n    d (flat): dipole matrix element\n\n    Returns:\n    array: complex susceptibility\n    \"\"\"\n\n    factor = (2 * n * d **2) / (epsilon_0 * hbar * Omega)\n\n    return factor * sigma21(Omega, Delta, Gamma, gamma)\n\nOmega = 0.1\nGamma = 1\ngamma = 1\nn = 1\nd = 1\n\n# Set up parameters for plot\ndet = np.linspace(-7.5, 7.5, 500) # Detuning values\nchifull = chi(Omega, det, Gamma, gamma, n, d)\nchi_im = np.imag(chifull)\nchi_re = np.real(chifull)\nchi_0 = np.imag(chi(Omega, 0, Gamma, gamma, n, d))\n\n# Make the plot                  \nplt.figure()\nplt.plot(det, chi_im / chi_0, label = '$Im[\\chi]$: absorption')\nplt.plot(det, chi_re / chi_0, label = '$Re[\\chi]$: dispersion')\nplt.hlines(0, det.min(), det.max(), color = 'k', linestyle = '--')\nplt.xlabel('Detuning [$\\Gamma$]')\nplt.ylabel('Susceptibility')\n\nplt.xlim(det.min(), det.max())\nplt.legend()\n\nplt.show()</pre> <p></p> <p>which shows that with a detuning from resonance, absorption (and therefore heating) can be reduced, and a phase will be imprinted on the light field. In the best case, you could imagine being able to perform phase imaging so one could image the atoms without having them be kicked out of the trap.</p> <p>But the question is why does this happen physically? Ultimately it comes down to what the real and imaginary components of \\(\\sigma_{12}\\) represent: the in-phase add in-quadrature components of the system's response to the coupling field. When we are driving an atom with an electric dipole field, there is a delay between the field and the atomic response. When we are perfectly on resonance, this delay is \\(\\pi/2\\), and tells us that the energy exchange between the atom and light field is as efficient as it can be, and there is no dispersion in the medium. Conversely, any in-phase component tells us that energy is going into the system, but not in its most efficient form (which would be stimulated absorption and emission) but is perhaps modulating the polarisation of the atom. A good analogy of this is imagining a driven classical oscillator. When you drive a classical oscillator, there is always a phase lag between the driving field and the response of the system, and when the driving frequency matches the natural frequency of the system, there is efficient energy transfer between the driving field and the system, but when the frequency is different, the system will oscillate at the driving frequency, and this phase will change, in the extreme limit being driving purely in phase as the system is not responding to the driving field.      </p>"},{"location":"s-a-3/#question-3","title":"Question 3","text":"<p>Density of states</p> <p>We have looked extensively at the two-level atom, that is, a ground state \\(\\rvert 1 \\rangle\\) and an excited state \\(\\rvert 2 \\rangle\\) which have an energy difference \\(\\hbar\\omega_0\\), and we expose the system to radiation of frequency \\(\\omega\\). In our description of this system, states \\(\\rvert 1 \\rangle\\) and \\(\\rvert 2 \\rangle\\) are energy eigenstates of some Hamiltonian \\(H_0\\) (e.g. the hydrogen atom) and we have treated the dipole interaction as a perturbation, \\(H'\\), to this Hamiltonian.</p> <ol> <li>Are the states \\(\\rvert 1 \\rangle\\) and \\(\\rvert 2 \\rangle\\)  eigenstates of the full Hamiltonian \\(H = H_0 + H^\\prime\\)?</li> <li>Using the pure state</li> </ol> \\[ \\rvert \\psi \\rangle = c_1 \\rvert 1 \\rangle + c_2 \\rvert 2 \\rangle, \\] <p>the Schr\\\"{o}dinger equation, and the rotating wave approximate, show that</p> \\[ \\frac{d}{dt}\\begin{pmatrix}             \\bar{c}_2 \\\\             c_1             \\end{pmatrix}             = - i \\begin{pmatrix}             -\\Delta &amp; \\Omega/2 \\\\             \\Omega/2 &amp; 0             \\end{pmatrix}             \\begin{pmatrix}             \\bar{c}_2 \\\\             c_1             \\end{pmatrix} \\] <p>where \\(\\bar{c}_2\\) is the slowly-varying variable in the rotating wave approximation.</p> <ol> <li>Using the above result, or otherwise, show that the eigenvalues \\(\\lambda_{\\pm}\\) and the associated eigenvectors \\(\\rvert \\pm \\rangle\\) of this system are</li> </ol> \\[ \\lambda_{\\pm} = -\\frac{\\hbar\\Delta}{2} \\pm \\frac{\\hbar\\Omega^\\prime}{2} \\] <p>and</p> \\[\\begin{align*}     \\rvert + \\rangle &amp; = \\sin(\\theta) \\rvert 1 \\rangle + \\cos(\\theta) \\rvert 2 \\rangle \\\\     \\rvert - \\rangle &amp; = \\cos(\\theta) \\rvert 1 \\rangle - \\sin(\\theta) \\rvert 2 \\rangle \\end{align*}\\] <p>where \\(\\tan(2\\theta) = -\\Omega/\\Delta\\)(1).</p> <ol> <li> <p>The time has come: interpret the above mathematical results physically. Given this is hard, you should make a plot of the eigenvalues as a function of \\(\\Delta\\), first for the case  \\(\\Omega = 0\\), and then make additional plots for \\(\\Omega \\ne 0\\) and comment on what happens. What is happening in our system, and why? (2)</p> </li> <li> <p>Perhaps the most surprising result of this work is how this system will respond to a frequency chirp (i.e. changing \\(\\Delta\\)). Let us imagine we start in the ground state, and we sweep our coupling field frequency, beginning such that \\(\\Delta \\ll \\Omega\\) and ending with \\(\\Delta \\gg \\Omega\\), and we do this sufficiently slowly such that no funny business will occur (effects that we have not considered). Explain physically what will have transpired, and why it happens. Note: This is an advanced problem, insomuch as it knocks on the door of the kind of quantum effects which start to crop up everywhere once you start looking, but are not routinely covered in undergraduate courses because it is hard to interpret classically. As such, this question is only worth one point, but a golden point, meaning there is an additional bonus (which will not alter your mark) for providing a correct answer.  </p> </li> </ol> <ol> <li>Hint: computing the ratio of the coefficients \\(c_1\\) and \\(\\bar{c}_2\\) might prove useful.</li> <li>Hint: Think hard about what eigenvalues and eigenvectors mean physically</li> </ol>"},{"location":"s-a-3/#31","title":"3.1","text":"<p>Are the states \\(\\rvert 1 \\rangle\\) and \\(\\rvert 2 \\rangle\\)  eigenstates of the full Hamiltonian \\(H = H_0 + H^\\prime\\)?</p> <p>No, they are not: the eigenstates of \\(H_0\\) are \\(\\rvert 1 \\rangle\\) and \\(\\rvert 2 \\rangle\\), and we are adding additional terms to our Hamiltonian in such a way that</p> \\[     H\\rvert 1 \\rangle \\ne H_0\\rvert 1 \\rangle \\ne E_0 \\rvert 1 \\rangle. \\] <p>It is worth stating that rarely would one want to bring along the computational machinery of time-dependent perturbation theory just to compute something that doesn't change the energy!</p>"},{"location":"s-a-3/#32","title":"3.2","text":"<p>Using the pure state</p> \\[ \\rvert \\psi \\rangle = c_1 \\rvert 1 \\rangle + c_2 \\rvert 2 \\rangle, \\] <p>the Schr\\\"{o}dinger equation, and the rotating wave approximate, show that</p> \\[ \\frac{d}{dt}\\begin{pmatrix}            \\bar{c}_2 \\\\            c_1            \\end{pmatrix}            = - i \\begin{pmatrix}            -\\Delta &amp; \\Omega/2 \\\\            \\Omega/2 &amp; 0            \\end{pmatrix}            \\begin{pmatrix}            \\bar{c}_2 \\\\            c_1            \\end{pmatrix} \\] <p>where \\(\\bar{c}_2\\) is the slowly-varying variable in the rotating wave approximation.</p> <p>Note that the second exercise of tutorial 5 has \\textit{much} common content with this question. Almost like it was designed that way, hey?!</p> <p>The Schr\\\"{o}dinger equation for the pure state gives</p> \\[\\begin{align*}    i\\hbar\\frac{\\partial}{\\partial t} \\rvert \\psi \\rangle &amp; = H \\rvert \\psi \\rangle \\\\    \\Rightarrow \\frac{\\partial c_1}{\\partial t} \\rvert 1 \\rangle + \\frac{\\partial c_2}{\\partial t} \\rvert 2 \\rangle &amp; = -i\\omega_0 c_2 \\rvert 1 \\rangle - i\\frac{\\Omega}{2}e^{i\\omega t} c_2 \\rvert 1  \\rangle - i\\frac{\\Omega}{2}e^{-i\\omega t} c_1 \\rvert 2  \\rangle \\end{align*}\\] <p>where we have used the fact all the time-dependence is stored in the coefficients, and the Hamiltonian is constructed via</p> \\[\\begin{align*}    H_0 &amp; = \\hbar\\omega_0 \\left\\rvert 2 \\right\\rangle \\left\\langle 2 \\right\\rvert \\\\    H^\\prime &amp; = \\frac{\\hbar \\Omega}{2} \\left(e^{i\\omega t} \\left\\rvert 1 \\right\\rangle \\left\\langle 2 \\right\\rvert +e^{-i\\omega t} \\left\\rvert 2 \\right\\rangle \\left\\langle 1 \\right\\rvert \\right) \\end{align*}\\] <p>which was the result shown in tutorial 5. But with the Schr\\\"{o}dinger equation in that form, how can we make it more tractable? How about using the orthogonality of \\(\\rvert 1 \\rangle\\) and  \\(\\rvert 2 \\rangle\\)? Projecting the the Schr\\\"{o}dinger equation onto both \\(\\rvert 1 \\rangle\\) and  \\(\\rvert 2 \\rangle\\) (that is, applying the bras \\(\\langle 1 \\rvert\\) and \\(\\langle 2 \\rvert\\))</p> \\[\\begin{align*}    \\frac{\\partial c_1}{\\partial t} &amp; = -i \\frac{\\Omega}{2} c_2 e^{i\\omega t} \\\\    \\frac{\\partial c_2}{\\partial t} &amp; = -i\\omega_0 c_2 - i \\frac{\\Omega}{2} c_1 e^{-i\\omega t} \\end{align*}\\] <p>we get two coupled equations. Now we define the slowly-varying excited-state amplitude</p> \\[    \\bar{c}_2 \\equiv c_2 e^{-i\\omega t} \\] <p>and our coupled equations reduce to</p> \\[\\begin{align*}    \\frac{\\partial c_1}{\\partial t} &amp; = -i \\frac{\\Omega}{2} \\bar{c}_2 \\\\    \\frac{\\partial \\bar{c}_2}{\\partial t} &amp; = -i\\Delta \\bar{c}_2 -i \\frac{\\Omega}{2} c_1 \\end{align*}\\] <p>which can be rewritten as</p> \\[    \\frac{d}{dt}\\begin{pmatrix}                \\bar{c}_2 \\\\                c_1                \\end{pmatrix}                = - i \\begin{pmatrix}                -\\Delta &amp; \\Omega/2 \\\\                \\Omega/2 &amp; 0                \\end{pmatrix}                \\begin{pmatrix}                \\bar{c}_2 \\\\                c_1                \\end{pmatrix} \\]"},{"location":"s-a-3/#33","title":"3.3","text":"<p>Using the above result, or otherwise, show that the eigenvalues \\(\\lambda_{\\pm}\\) and the associated eigenvectors \\(\\rvert \\pm \\rangle\\) of this system are</p> \\[ \\lambda_{\\pm} = -\\frac{\\hbar\\Delta}{2} \\pm \\frac{\\hbar\\Omega^\\prime}{2} \\] <p>and</p> \\[\\begin{align*}     \\rvert + \\rangle &amp; = \\sin(\\theta) \\rvert 1 \\rangle + \\cos(\\theta) \\rvert 2 \\rangle \\\\     \\rvert - \\rangle &amp; = \\cos(\\theta) \\rvert 1 \\rangle - \\sin(\\theta) \\rvert 2 \\rangle \\end{align*}\\] <p>where \\(\\tan(2\\theta) = -\\Omega/\\Delta\\)(1).</p> <ol> <li>Hint: computing the ratio of the coefficients \\(c_1\\) and \\(\\bar{c}_2\\) might prove useful.</li> </ol> <p>The result</p> \\[     \\frac{d}{dt}\\begin{pmatrix}                 \\bar{c}_2 \\\\                 c_1                 \\end{pmatrix}                 = - i \\begin{pmatrix}                 -\\Delta &amp; \\Omega/2 \\\\                 \\Omega/2 &amp; 0                 \\end{pmatrix}                 \\begin{pmatrix}                 \\bar{c}_2 \\\\                 c_1                 \\end{pmatrix} \\] <p>tells us that our Hamiltonian, defined through \\(i\\hbar \\frac{d}{dt} \\ket{\\psi} = H \\ket{\\psi}\\) must have the form</p> \\[     H = \\hbar \\begin{pmatrix}                 -\\Delta &amp; \\Omega/2 \\\\                 \\Omega/2 &amp; 0                 \\end{pmatrix} \\] <p>and so to compute the eigenvalues, we must solve $\\mathrm{det}(H - \\lambda I) = 0</p> \\[\\begin{align*}     \\mathrm{det}(H-\\lambda I) &amp; = \\begin{pmatrix}                 -\\hbar\\Delta - \\lambda &amp; \\hbar\\Omega/2 \\\\                 \\hbar\\Omega/2 &amp; - \\lambda                 \\end{pmatrix} \\\\                 &amp; = \\hbar\\left( \\lambda(\\hbar\\Delta + \\lambda) - \\left(\\frac{\\hbar\\Omega}{2}\\right)^2 \\right) \\end{align*}\\] <p>which yields a quadratic in \\(\\lambda\\) with roots that can be found using the quadratic formula</p> \\[\\begin{align*}     \\lambda &amp; = \\frac{-\\hbar\\Delta \\pm \\sqrt{\\hbar^2\\Delta^2 - 4\\left(\\frac{\\hbar\\Omega}{2}\\right)^2}}{2} \\\\     \\lambda &amp; = -\\frac{\\hbar\\Delta}{2} \\pm \\hbar\\frac{\\sqrt{\\Delta^2 - \\Omega^2}}{2} = -\\frac{\\hbar\\Delta}{2} \\pm \\frac{\\hbar\\Omega^\\prime}{2} \\end{align*}\\] <p>from the definition of the generalised Rabi frequency \\(\\Omega^\\prime\\). To find the eigenvectors we need to find the vectors which satisfy the relation</p> \\[     H \\begin{pmatrix}             \\bar{c}_2\\\\             c_1         \\end{pmatrix}         = \\lambda         \\begin{pmatrix}             \\bar{c}_2\\\\             c_1         \\end{pmatrix} \\] <p>which gives the coupled equations</p> \\[\\begin{align*}     -\\hbar \\Delta \\bar{c}_2 + \\frac{\\hbar\\Omega}{2} c_1 &amp; = \\lambda \\bar{c}_2\\\\     \\frac{\\hbar\\Omega}{2} \\bar{c}_2&amp; = \\lambda c_1. \\end{align*}\\] <p>We now plug in our values for \\(\\lambda\\) into the 2nd equation:</p> \\[     \\frac{\\hbar\\Omega}{2} \\bar{c}_2 = \\left(-\\frac{\\hbar\\Delta}{2} \\pm \\frac{\\hbar\\Omega^\\prime}{2} \\right) c_1 \\] <p>which gives the ratio</p> \\[     \\frac{\\bar{c}_2}{c_1} = \\frac{-\\Delta \\pm \\Omega^\\prime}{\\Omega} \\] <p>which is consistent with the first equation. This means our eigenvectors will be of the form</p> \\[     \\ket{\\pm} = \\begin{pmatrix}             \\bar{c}_2/c_1 \\\\             1         \\end{pmatrix} \\] <p>although these should be normalised. However, these are not in terms of trigonmetric functions: we need to introduce the angular relation - called the St\\\"{u}kelberg angle - which can be defined via</p> \\[\\begin{align*}     \\cos\\left(\\theta\\right) &amp; = -\\frac{\\Delta}{\\Omega^\\prime} \\\\     \\sin\\left(\\theta\\right) &amp; = \\frac{\\Omega}{\\Omega^\\prime}. \\end{align*}\\] <p>It is worth noting that there are a few different conventions for this, and I should have told you which one I was using, as this is required to get the correct result. We also shift to considering the cases of \\(\\lambda = -\\frac{\\hbar\\Delta}{2} \\pm \\frac{\\hbar\\Omega^\\prime}{2}\\) separately - for the ease of computation/manipulation. Considering \\(\\lambda_+\\), we can use the angular relations above to construct the relation</p> \\[     -\\Delta + \\Omega^\\prime = \\Omega^\\prime\\left( 1 + \\cos\\left(\\theta\\right)\\right) \\] <p>which combined with our definition of \\(\\sin\\left(\\theta\\right)\\) allows us to re-express the quotient</p> \\[\\begin{align*}     \\frac{\\bar{c}_2}{c_1} &amp; = \\frac{-\\Delta + \\Omega^\\prime}{\\Omega} \\\\     &amp; = \\frac{\\Omega^\\prime \\left( 1 + \\cos\\left(\\theta\\right) \\right)}{\\Omega} \\\\     &amp; = \\frac{\\Omega}{\\sin\\left(\\theta\\right)} \\frac{\\Omega^\\prime \\left( 1 + \\cos\\left(\\theta\\right)\\right)}{\\Omega} \\\\     &amp; = \\frac{1 + \\cos\\left(\\theta\\right)}{\\sin\\left(\\theta\\right)} = \\cot\\left(\\theta/2\\right) \\end{align*}\\] <p>using a half-angle formula for the (co)tangent function. This means our eigenvector is</p> \\[     \\ket{+} = \\begin{pmatrix}             \\bar{c}_2/c_1 \\\\             1         \\end{pmatrix}         = \\begin{pmatrix}             \\cot\\left(\\theta/2\\right) \\\\             1         \\end{pmatrix} \\] <p>but this must be normalised, so it must be scaled by</p> \\[     \\sqrt{1 + \\cot^2\\left(\\theta/2\\right)} = \\frac{1}{\\sin\\left(\\theta/2\\right)} \\] <p>giving the final result</p> \\[     \\ket{+} = \\begin{pmatrix}             \\cos\\left(\\theta/2\\right) \\\\             \\sin\\left(\\theta/2\\right)         \\end{pmatrix} \\] <p>For the case of \\(\\lambda_-\\), the calculation is more-or-less identical, except one finds that</p> \\[\\begin{align*}     \\frac{\\bar{c}_2}{c_1} &amp; = \\frac{-\\Delta - \\Omega^\\prime}{\\Omega} \\\\     &amp; = \\frac{\\Omega^\\prime \\left(\\cos\\left(\\theta\\right) - 1 \\right)}{\\Omega} \\\\     &amp; = \\frac{\\Omega}{\\sin\\left(\\theta\\right)} \\frac{\\Omega^\\prime \\left( \\cos\\left(\\theta\\right) - 1 \\right)}{\\Omega} \\\\     &amp; = \\frac{\\cos\\left(\\theta\\right) - 1}{\\sin\\left(\\theta\\right)} = -\\tan\\left(\\theta/2\\right) \\end{align*}\\] <p>and then normalising the associated eigenvector, one ends up dividing by a factor of \\(1/\\cos\\left(\\theta / 2\\right)\\) which gives</p> \\[     \\ket{-} = \\begin{pmatrix}             -\\sin\\left(\\theta/2\\right) \\\\             \\cos\\left(\\theta/2\\right)         \\end{pmatrix} \\] <p>which are the results quoted - although in terms of \\(\\theta\\) and \\(\\theta/2\\) rather than \\(2\\theta\\) and \\(\\theta\\).</p>"},{"location":"s-a-3/#34","title":"3.4","text":"<p>The time has come: interpret the above mathematical results physically. Given this is hard, you should make a plot of the eigenvalues as a function of \\(\\Delta\\), first for the case  \\(\\Omega = 0\\), and then make additional plots for \\(\\Omega \\ne 0\\) and comment on what happens. What is happening in our system, and why? (1)</p> <ol> <li>Hint: Think hard about what eigenvalues and eigenvectors mean physically</li> </ol> <p>As per the hint, we are going to produce a plot of the eigenvalues as a function of \\(\\Delta\\) for different values of \\(\\Omega\\) and code to produce this can be found below</p> <p> </p><pre><code># Parameters\nDelta = np.linspace(-6, 6, 500)  # Detuning\nOmega_values = np.linspace(0.6,6,16) # Rabi frequency\n\n# Function to calculate eigenvalues (energies)\ndef eigenvalues(Delta, Omega):\n   Omega_prime = np.sqrt(Delta**2 + Omega**2)\n   lambda_plus = -Delta / 2 + Omega_prime / 2\n   lambda_minus = -Delta / 2 - Omega_prime / 2\n   return lambda_plus, lambda_minus\n\n# Set the colormap\ncolormap = plt.get_cmap('plasma')\n\n# Plotting\nplt.figure(figsize=(12,6))\n\n# Make a plot of the bare states (no coupling)\nno_coupling_plus, no_coupling_minus = eigenvalues(Delta, 0)\nplt.plot(Delta, no_coupling_plus, c = 'k', linewidth=2.5, linestyle='--')\nplt.plot(Delta, no_coupling_minus, c= 'k', linewidth=2.5, linestyle='--')\n\n# Plot the eigenstates with non-zero coupling\nfor i, o in enumerate(Omega_values):\n   lambda_plus, lambda_minus = eigenvalues(Delta, o)\n   plotcolour = colormap(i / len(Omega_values))\n   l = f'$\\Omega$ = {o:.1f}'\n   plt.plot(Delta, lambda_plus, color=plotcolour, label=l)\n   plt.plot(Delta, lambda_minus, color=plotcolour)\n\n# Add legend outside the plot on the right-hand side\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)\n\n# Adjust the subplot parameters to give some space for the legend\nplt.subplots_adjust(right=0.75)\n\n# Adding text - nothing fancy, just brute force\nplt.text(-5, 0.2, f'$E_1 = 0$', fontsize=16, color='k')\nplt.text(-4.6,2.6, f'$E_2 = -\\hbar\\Delta$', fontsize=16, color='k')\ntextcolour = colormap(0.2) # Need something better than 'plotcolour' as this is unreadable for text\nplt.text(-2.5, -2.5, f'$E_-$', fontsize=16, color=textcolour)\nplt.text(2.5, 2.3, f'$E_+$', fontsize=16, color=textcolour)\n\n# tweak plot\nplt.title('Energy eigenstates for a coupled system')\nplt.xlabel('Detuning $\\Delta$')\nplt.ylabel('Energy [a.u]')\nplt.ylim(-3, 3)\nplt.xlim(-5.5, 5.25)\n\nif savefigflag: # Change to True to save the figures\n   plt.savefig('Images/A3_coupled-eigenstates.pdf', bbox_inches= 'tight')\n\nplt.show()\n</code></pre> <p>Now what does this all mean? The solutions above have already poked at what is going on: our beloved states \\(\\ket{1}\\) and \\(\\ket{2}\\) are no longer eigenstates of our Hamiltonian: when a coupling field is present, we need to solve the Schr\\\"{o}dinger equation and look for the new energy eigenstates: this is exactly what we have just done. The states \\(\\ket{\\pm}\\) are these states, and they have associated energies \\(E_\\pm\\) which are given by the associated eigenvalues. These states are referred to as dressed states, and introduces an important feature we have not encountered, namely, states crossing in energy, which in this case occurs when \\(\\Delta=0\\) when there is no coupling (see the plot). But to talk about a detuning \\(\\Delta\\), we must have a non-zero coupling - in the same way we can't meaningfully talk about the frequency of a laser with zero intensity - and the consequence of any coupling is to lift the degeneracy of the states at \\(\\Delta = 0\\) (again, see the plot) in such a way this is usually referred to as an avoided crossing. Notable features of the coupled and uncoupled states are that for large detunings, they are approximately equal and therefore we can identify these states, but near resonance, the coupled energies are shifted, departing from the uncoupled states, and importantly, the eigenstates are formed from a mixture of the ground and excited states - which can lead to interesting behaviour. The energy shift which arises here is usually called the AC Stark shift by analogy with the DC Stark shift - it is just our AC frequency is emph high.</p>"},{"location":"s-a-3/#35","title":"3.5","text":"<p>Perhaps the most surprising result of this work is how this system will respond to a frequency chirp (i.e. changing \\(\\Delta\\)). Let us imagine we start in the ground state, and we sweep our coupling field frequency, beginning such that \\(\\Delta \\ll \\Omega\\) and ending with \\(\\Delta \\gg \\Omega\\), and we do this sufficiently slowly such that no funny business will occur (effects that we have not considered). Explain physically what will have transpired, and why it happens(1).</p> <ol> <li>Note: This is an advanced problem, insomuch as it knocks on the door of the kind of quantum effects which start to crop up everywhere once you start looking, but are not routinely covered in undergraduate courses because it is hard to interpret classically. As such, this question is only worth one point, but a golden point, meaning there is an additional bonus (which will not alter your mark) for providing a correct answer.  </li> </ol> <p>The answer to this is effectively given in the discussion above, but in short: with a large detuning, our dressed states are our uncoupled eiegenstates \\(\\ket{1}\\) and \\(\\ket{2}\\). If we look at the plot shown above, let us imagine that we chirp from \\(-\\Delta\\) to \\(\\Delta\\) and we were initially in the state \\(\\ket{1}\\) with energy \\(E_1\\), which for negative detuning would be state \\(\\ket{-}\\) with energy \\(E_-\\). If the chirp happens nicely (and when we say nicely in physics, we usually mean adiabatically) we will remain in the same eigenstate, namely \\(\\ket{-}\\). If we follow this to the end of our chirp, say some positive value of \\(\\Delta\\), we now find ourselves in a state that looks much like \\(\\ket{2}\\) with an energy \\(E_2\\). In essence, the existence of an avoided crossing exchanges the identities of the uncoupled eigenstates. Cool huh!</p> <p>I want to pause and reiterate what has happened: we stated in \\(\\ket{-}\\approx\\ket{1}\\), we slowly increased the laser frequency such that we followed the \\(\\ket{-}\\) state (which ultimately means that the angle \\(\\theta\\) in our eigenstates goes from \\(0\\) to \\(\\pi/2\\)) and then we have ended up in \\(\\ket{-}\\approx\\ket{2}\\)! This is super crazy! We have seen that we can send an atom from the ground to the excited state by applying a \\(\\pi\\) pulse, but sometimes this is not feasible - lasers drift in both frequency and amplitude, resulting in imperfect \\(\\pi\\) pulses - and using so-called adiabatic passage is a better way to achieve high-fidelity population transfer. You might ask \"how good is the transfer?\", to which I respond: you should see if you can calculate this - although it is pretty hard to actually get through this - but you can research the Landau-Zener limit if you want to know. Basically, ensure there are lots of photons and a very adiabatic process (slow frequency slew) and you will be sitting pretty.</p>"},{"location":"s-a-3/#question-4","title":"Question 4","text":"<p>Ramsey interferometry</p> <p>In this question, we are going to explore Ramsey interferometry in a two-level atom.</p> <ol> <li>Explain how Ramsey interferometry allows for a precise determination of the resonance frequency in a two-level system</li> </ol> <p>As seen in class, the excited-state population for a general detuning \\(\\Delta\\) for Ramsey interferometry can be calculated via</p> \\[     \\rho_{22} = 4\\left(\\frac{\\Omega}{\\Omega'}\\right)^2\\sin^2\\left(\\frac{\\Omega'\\tau}{2}\\right)\\left[\\cos\\left(\\frac{\\Delta T}{2}\\right)\\cos\\left(\\frac{\\Omega'\\tau}{2}\\right)-\\frac{\\Delta}{     \\Omega'}\\sin\\left(\\frac{\\Delta T}{2}\\right)\\sin\\left(\\frac{\\Omega'\\tau}{2}\\right)\\right]^2. \\] <ol> <li>The NIST-7 (also discussed in class) has experimental parameters \\(\\left(v,l,L,\\tau,\\frac{\\Omega}{2\\pi}\\right)=(230 \\unit{m/s}, 23\\unit{mm},1.53\\unit{m},100\\unit{\\mu s},2.5\\unit{kHz})\\) for the beam velocity, interaction region width, propagation distance, interaction time, and Rabi frequency. Explain why these values are chosen to obtain an accurate measurement of the transition, with specific reference the the effect(s) of changing the parameters.</li> </ol> <p><code>\\begin{Computational content}</code></p> <p>Our goal is to understand the experimental signal that is measured when the clock is operational.</p> <ol> <li>Using the experimental parameters given above, plot the expected signal - that is, excited state population as a function of the detuning from atomic resonance, \\(\\Delta\\)</li> <li>Shown below is a (simulated) plot of the measured signal from the apparatus</li> </ol> <p> The energy levels of muonic hydrogen </p> <p>This should look distinctly different to the plot you produced in the previous question. Your goal is to reproduce the above plot, which will require you to identify why the signal does not look like the signal predicted by the equation above, and then you will have to simulate what is actually measured, and hopefully it will look like the above plot (which is indeed what is actually measured)(1)</p> <ol> <li>Hint: The Maxwell-Boltzmann distribution may or may not be useful, along with the information that the temperature of the caesium beam oven was about 55 degrees C.</li> </ol>"},{"location":"s-a-3/#41","title":"4.1","text":"<p>Explain how Ramsey interferometry allows for a precise determination of the resonance frequency in a two-level system</p> <p>Let us consider the \"simple\" way of measuring a resonance frequency, namely simply sweeping our frequency across the atomic resonance. If we monitor the population in the excited state from a single interaction region, we know that there will be Rabi oscillations and</p> \\[    \\rho_{22}=\\frac{1}{1+\\left(\\frac{\\Delta}{\\Omega}\\right)^2}\\sin^2\\left(\\frac{\\sqrt{\\Omega^2+\\Delta^2}\\tau}{2}\\right)=\\frac{1}{1+\\left(\\frac{\\Delta}{\\Omega}\\right)^2}\\sin^2\\left(\\frac{L}{2v}\\sqrt{\\Omega^2+\\Delta^2}\\right) \\] <p>which will have a width dominated by the Lorentzian \\(\\frac{\\Delta}{\\Omega}\\). As we want \\(\\Omega\\tau=\\pi\\) for maximum \\(\\rho_{22}\\), the width of this peak will scale with \\(\\Omega\\) and hence \\(\\frac{\\pi}{\\tau}=\\frac{\\pi v}{L}\\). As we want the most precise value, that is, we want to reduce the width, we can either increase \\(L\\) and/or decrease \\(v\\) and \\(\\Omega\\) - although there are practical considerations to doing this.</p> <p>In the case of Ramsey interferometry, in class we saw that the excited state population goes as \\(\\rho_{22}=\\cos^2\\left(\\frac{\\Delta T}{2}\\right)=\\cos^2\\left(\\frac{\\Delta L}{2v}\\right)\\). Firstly, we note that there is no dependence of \\(\\Omega\\), and in addition, there is no longer a Lorentzian term. Once again, we can improve the precision of this measurement by increasing \\(L\\) or decreasing \\(v\\). So seemingly, by magic, we now have a more sensitive measurement. But what is going on?</p> <p>Fundamentally, we are probing the phase evolution of the states - which accumulate phase at different rates - by interfering them, and the rate at which the phase difference accumulates is related to the difference in energy of the states. This is analogous to the Mach-Zehnder interferometer, where our \\(\\frac{\\pi}{2}\\) pulses from the light-atom interaction are the atomic equivalent to optical beamsplitters.</p> <p>The principle which undergirds the precise measurement of frequency is one that exists for all waves, namely the relationship between the time and frequency domains. Explicitly, the more precisely I desire knowledge of a frequency, the longer time I must sample the wave. This concept is often bundled up in uncertainty relations; however, these themselves are manifestations of this fundamental property of waves (which are elegantly elucidated through Fourier analysis). If one is more comfortable in the language of uncertainty principles, one can simply say that achieving a small energy uncertainty (a precise measurement) requires a large time uncertainty.</p>"},{"location":"s-a-3/#42","title":"4.2","text":"<p>The NIST-7 (also discussed in class) has experimental parameters \\(\\left(v,l,L,\\tau,\\frac{\\Omega}{2\\pi}\\right)=(230 \\unit{m/s}, 23\\unit{mm},1.53\\unit{m},100\\unit{\\mu s},2.5\\unit{kHz})\\) for the beam velocity, interaction region width, propagation distance, interaction time, and Rabi frequency. Explain why these values are chosen to obtain an accurate measurement of the transition, with specific reference the the effect(s) of changing the parameters.</p> <p>The answer to this is effectively covered in the previous question, as the difference between the expression provided here for \\(\\rho_{22}\\) when there is detuning and that discussed in class (no detuning) is a modulation of the on-resonance signal by something that looks much like an off-resonant Rabi oscillation. Therefore, the considerations which result in precise measurements as discussed above for both the Rabi and Ramsey interferometery cases apply. These include</p> <ul> <li>Increase \\(L\\) (further to travel \\(\\Rightarrow\\) more phase)</li> <li>Decrease \\(v\\) (longer propagation time \\(\\Rightarrow\\) more phase)</li> <li>Increase \\(\\tau = L/v\\), but this is set by \\(L\\) and \\(v\\)</li> <li>Decrease \\(l\\) (Try to minimise the effect of velocity spread, coupling field inhomogeneities)</li> <li>Decrease \\(\\Omega\\) (no saturation broadening)</li> </ul> <p>It is also worth mentioning practical considerations: For example, I can keep increasing \\(L\\), but at some point, I have a really long stainless steel vacuum chamber that I need to store somewhere! Similarly, I would love to have a low \\(v\\), but if I require a beam of atoms, I need them to vapourise, which means heating up a solid and then one gets an increased velocity spread. Ultimately \\(l\\) will be limited by the optical system (e.g. diffraction limit for a lens) but one also wants to have a uniform, well-defined intensity of the coupling field, and thus aberrations play a role. Similarly, \\(\\Omega\\) should be small, but if it is too small, there will be no interaction with the atoms! The values used here represent the maximum bang for buck.</p>"},{"location":"s-a-3/#43","title":"4.3","text":"<p>Using the experimental parameters given above, plot the expected signal - that is, excited state population as a function of the detuning from atomic resonance, \\(\\Delta\\)</p> <pre><code># Parameters\nL = 1.53\nl = 0.023\nT_new = np.pi / 2 * (L / l)\ntau_new = np.pi / 2\n\n# Define the probability for excitation function for a given velocity\ndef P(Delta, T_new, tau_new):\n   term1 = (4 / (1 + Delta**2)) * (np.sin(np.sqrt(1 + Delta**2) * tau_new / 2))**2\n   cos_term = np.cos((Delta * T_new) / 2) * np.cos(np.sqrt(1 + Delta**2) * tau_new / 2)\n   sin_term = (Delta / np.sqrt(1 + Delta**2)) * np.sin((Delta * T_new) / 2) * np.sin(np.sqrt(1 + Delta**2) * tau_new / 2)\n   term2 = (cos_term - sin_term)**2\n   return term1 * term2\n\n# Define the detuning range\nDelta_range = np.linspace(-6, 6, 10000)\n\n# Calculate the excitation probability\nP_values = P(Delta_range, T_new, tau_new)\n\n# Plotting\nplt.figure(figsize=(14, 8))\nplt.plot(Delta_range, P_values, lw = 1)\nplt.xlabel(r'Detuning [$\\Delta/\\Omega$]')\nplt.ylabel('Excited state population')\nplt.title('Ramsey Fringes for a Single Atom')\nplt.grid(True)\n\nif savefigflag: # Change to True to save the figures\n   plt.savefig('Images/A3_NIST-7-ramsey.pdf', bbox_inches= 'tight')\n\nplt.show()\n</code></pre> <p>The code above should produce the following plot:</p> <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nL = 1.53\nl = 0.023\nT_new = np.pi / 2 * (L / l)\ntau_new = np.pi / 2\n\n# Define the probability for excitation function for a given velocity\ndef P(Delta, T_new, tau_new):\n   term1 = (4 / (1 + Delta**2)) * (np.sin(np.sqrt(1 + Delta**2) * tau_new / 2))**2\n   cos_term = np.cos((Delta * T_new) / 2) * np.cos(np.sqrt(1 + Delta**2) * tau_new / 2)\n   sin_term = (Delta / np.sqrt(1 + Delta**2)) * np.sin((Delta * T_new) / 2) * np.sin(np.sqrt(1 + Delta**2) * tau_new / 2)\n   term2 = (cos_term - sin_term)**2\n   return term1 * term2\n\n# Define the detuning range\nDelta_range = np.linspace(-6, 6, 10000)\n\n# Calculate the excitation probability\nP_values = P(Delta_range, T_new, tau_new)\n\n# Plotting\nplt.figure(figsize=(14, 8))\nplt.plot(Delta_range, P_values, lw = 1)\nplt.xlabel(r'Detuning [$\\Delta/\\Omega$]')\nplt.ylabel('Excited state population')\nplt.title('Ramsey Fringes for a Single Atom')\nplt.grid(True)\n\nplt.show()</pre> <p></p>"},{"location":"s-a-3/#44","title":"4.4","text":"<p>Shown below is a (simulated) plot of the measured signal from the apparatus</p> <p> The energy levels of muonic hydrogen </p> <p>This should look distinctly different to the plot you produced in the previous question. Your goal is to reproduce the above plot, which will require you to identify why the signal does not look like the signal predicted by the equation above, and then you will have to simulate what is actually measured, and hopefully it will look like the above plot (which is indeed what is actually measured)(1)</p> <ol> <li>Hint: The Maxwell-Boltzmann distribution may or may not be useful, along with the information that the temperature of the caesium beam oven was about 55 degrees C.</li> </ol> <p>This question requires that you understand both what Ramsey interferometry is, but also how it is performed in practice. As we discussed in class, initial experiments typically had a source of atoms which passed through two light fields separated by some distance. Fundamental to our description of this experiment is that an atom will have a velocity \\(v\\), which determines the time spent in the laser field and also the evolution time. If \\(v\\) changes, the Rabi cycle will be slightly different and hence we will no longer have a \\(\\frac{\\pi}{2}-\\)pulse, so our population after a given evolution time will change - and hence our fringes will look different.\\</p> <p>Returning to the experimental setup, typically a beam of atoms is created by heating a reservoir of atoms to about \\(50^{\\circ}\\)C, meaning that there will be a spread in velocities as determined by the Maxwell-Boltzmann distribution. Each atom will have a different Ramsey fringe pattern and we measure the collective behaviour, which means that these fringes will effectively interfere and act to reduce the visibility of the our detected fringes - and in the extreme only leave one or two fringes.\\</p> <p>In an attempt to reproduce the effects of moving from a single atom to an ensemble of atoms with different speeds, below I show the Ramsey interference pattern individual atoms with different velocities as selected from the Maxwell-Boltzmann distribution.</p> <p></p> <p>We can see that the fringe patterns look wildly different, with different frequencies of oscillation, but also different modulation envelopes. Hopefully it is clear that when these are averaged, that is, incoherently summed together as they would be in an experiment - the different frequency components will result in an averaged signal (i.e. no fringes), expect a small amount of remnant fringe structure about \\(\\Delta=0\\). This is a bit similar to observing interference in a Michelson interferometer using white light. Likewise, due to the a common envelope structure, there is a \"background\" to this fringe pattern, which is manifest as a much broader blob of signal around \\(\\Delta=0\\). For small values of the detuning, this platform is essentially constant, and the primary fringes are clearly observable - see the insert - and this is the kind of data you would be measuring if you were doing such a measurement. For reference, the figure in the question was produced by summing the excited state populations for \\(100,000\\) atoms.</p> <pre><code># Constants and Parameters\nmCs = 132.90545 * amu  # Caesium mass in kg (from atomic mass units)\nbeamtemp = 330  # Temperature in Kelvin\n\n# Mean velocity calculation from Maxwell-Boltzmann distribution\ndef vmean(T):\n   return np.sqrt((8 * kb * T) / (np.pi * mCs))\n\n# Generate velocities using Maxwell-Boltzmann distribution\nn_atoms = 100000\nMDvelocities = maxwell.rvs(scale=np.sqrt(kb * beamtemp / mCs), size=n_atoms)\n\n# Plot the velocity distribution\nplt.figure(figsize=(10, 6))\nplt.hist(MDvelocities, bins=50, density=True, edgecolor='w', linewidth=.5, alpha=0.6, label=\"Histogram of velocities\")\nv_values = np.linspace(0, 1000, 1000)\nplt.plot(v_values, maxwell.pdf(v_values, scale=np.sqrt(kb * beamtemp / mCs)), 'C1', lw=2, label=\"Maxwell-Boltzmann PDF\")\nplt.xlabel('Speed [m/s]')\nplt.ylabel('Probability Density')\nplt.title('Velocity Distribution of Cs Atoms at 100\u00c2\u00b0C')\nplt.legend()\n\nif savefigflag: # Change to True to save the figures\n   plt.savefig('Images/A3_NIST-7-velocity-dist.pdf', bbox_inches= 'tight')\nplt.show()\n\n#### Calculate Ramsey signal for individual atoms with different velocities\n\n# Sample velocities and calculate probabilities\nsamplev = MDvelocities[:6]\nsampleprobs = [P(np.linspace(-6, 6, 300), np.pi / 2 * v / vmean(beamtemp) * L / l, np.pi / 2 * v / vmean(beamtemp)) for v in samplev]\n\n# Plotting the Ramsey fringes for the sample velocities\nfig, axes = plt.subplots(3, 2, figsize=(15, 10))\naxes = axes.flatten()\nfor i, prob in enumerate(sampleprobs):\n   axes[i].plot(np.linspace(-6, 6, 300), prob)\n   axes[i].set_ylim(0, 1)\n   axes[i].set_xlabel(r'Detuning [$\\Delta/\\Omega$]')\n   axes[i].set_ylabel('Excited state population')\n   axes[i].set_title(f'Sample Atom {i+1}')\nplt.tight_layout()\n\nif savefigflag: # Change to True to save the figures\n   plt.savefig('Images/A3_NIST-7-NIST-7-ramsey-individual.pdf', bbox_inches= 'tight')\n\nplt.show()\n\n#### Calculate experimental Ramsey signal\n\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter\n\ndelta_range = 6 # Range of detuning, -delta_range - 0 - delta_range\ndelta = np.linspace(-delta_range, delta_range, 10000)\n\n# Calculate the ensemble signal by summing over all atoms\nprobs = np.array([P(delta, np.pi / 2 * v / vmean(beamtemp) * L / l, np.pi / 2 * v / vmean(beamtemp)) for v in MDvelocities])\nsignal = np.mean(probs, axis=0)\ndelta_lim = 0.15 # Subplot range, around zero detuning\nselector = (delta &gt; -delta_lim) &amp; (delta &lt; delta_lim) # Select elements to plot for insert\n\n# Create the main plot\nfig, ax = plt.subplots(figsize=(12, 8))\nax.plot(delta, signal)\nplt.xlabel(r'Detuning [$\\Delta/\\Omega$]')\nplt.ylabel('Excited state population')\nplt.title('Ramsey Fringes for NIST-7')\nplt.grid(True)\n\n# Add an inset with zoom around the origin\ninset_ax = fig.add_axes([0.675, 0.45, 0.4, 0.4])  # [left, bottom, width, height]\ninset_ax.plot(delta[selector], signal[selector])\n\n# Customize tick spacing\n# Specify the number of ticks\ninset_ax.xaxis.set_major_locator(MaxNLocator(nbins=5))  # Set number of x-axis ticks to 5\ninset_ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n\ninset_ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\ninset_ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n\nif savefigflag: # Change to True to save the figures\n   plt.savefig('Images/A3_NIST-7-NIST-7-ramsey-ensemble.pdf', bbox_inches= 'tight')\nplt.show()\n</code></pre>"},{"location":"s-t-1/","title":"S t 1","text":"<pre>\n\nfrom matplotlib import pyplot\nimport numpy as np\n\nfrom common import draw_classic_axes, configure_plotting\n\nconfigure_plotting()\n\n  </pre>"},{"location":"s-t-1/#tutorial-1","title":"Tutorial 1","text":"<p>Eventually, this content will be fleshed out on the site. In the short term, you can find a .pdf of the tutorial problems and solutions below</p> <p> Tutorial 1 problems and solutions     </p>"}]}