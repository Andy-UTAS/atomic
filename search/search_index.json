{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the home of solid-state physics \u00b6 This site exists to enhance the distribution of course information and content for the solid-state physics component of KYA322: Statistical and solid-state physics . All official communication will be though MyLO 1 . A Crocoite sample found near Dundas in western Tasmania. As part of this course, we will investigate the what happens when atoms are no longer considered in isolation, and see that systems of interacting particles can have some pretty incredible outcomes Course expectations: my expectations of you Solid-state physics is where the rubber meets the road: abstract concepts will be applied in an effort to model realistic and complex systems. There are delights are to be had, but like most truly rewarding endeavours, said delights do not come for free. In undertaking this course, it is expected that: You will view the video content before any scheduled face-to-face sessions Prescribed problems and reading will be undertaken before any face-to-face sessions You will attend all face-to-face sessions, and actively contribute to discussions and problem solving If you are experiencing difficulties, that you will get in contact as soon as is reasonably possible Course objectives The purpose of this course is to provide an introduction to the world of solid-state physics. At the conclusion of your journey, you should: Be familiar with the main models of solid-state physics, and their application to real-world systems Have proficiency parsing and extracting information from applied solid-state systems, with an eye to identification the relevant theoretical framework(s) and the concise formulation of physics to be investigated Have experience identifying and applying appropriate approximations and physical insight to make difficult problems more tractable Be familiar with experimental and analytical apparatus relevant to probing solid-state systems, in addition to some of the main systems and devices one might encounter in the wild Have had some fun! Course expectations: my promises to you Rightly, you should have expectations of me. It is my intention that: I will work to communicate my understanding and insight in the course material I will actively seek input to steer and shape the content discussed, and develop relevant resources I will be widely available for consultation and discussion I will do my best to cultivate a safe and open forum for discussion If you think that I am not fulfilling my commitments, or if you have other comments, I would hope that you are comfortable conveying your concerns and/or comments to me, either directly or anonymously. It is my genuine desire to help you navigate learning new concepts and ideas, and I want to do this to the best of my ability, and feedback is the best way to ensure this endeavour proceeds as smoothly as possible. For the curious traveller, MyLO is The University of Tasmania's learning management system, based on Brightspace as developed D2L. \u21a9","title":"Home"},{"location":"#welcome-to-the-home-of-solid-state-physics","text":"This site exists to enhance the distribution of course information and content for the solid-state physics component of KYA322: Statistical and solid-state physics . All official communication will be though MyLO 1 . A Crocoite sample found near Dundas in western Tasmania. As part of this course, we will investigate the what happens when atoms are no longer considered in isolation, and see that systems of interacting particles can have some pretty incredible outcomes Course expectations: my expectations of you Solid-state physics is where the rubber meets the road: abstract concepts will be applied in an effort to model realistic and complex systems. There are delights are to be had, but like most truly rewarding endeavours, said delights do not come for free. In undertaking this course, it is expected that: You will view the video content before any scheduled face-to-face sessions Prescribed problems and reading will be undertaken before any face-to-face sessions You will attend all face-to-face sessions, and actively contribute to discussions and problem solving If you are experiencing difficulties, that you will get in contact as soon as is reasonably possible Course objectives The purpose of this course is to provide an introduction to the world of solid-state physics. At the conclusion of your journey, you should: Be familiar with the main models of solid-state physics, and their application to real-world systems Have proficiency parsing and extracting information from applied solid-state systems, with an eye to identification the relevant theoretical framework(s) and the concise formulation of physics to be investigated Have experience identifying and applying appropriate approximations and physical insight to make difficult problems more tractable Be familiar with experimental and analytical apparatus relevant to probing solid-state systems, in addition to some of the main systems and devices one might encounter in the wild Have had some fun! Course expectations: my promises to you Rightly, you should have expectations of me. It is my intention that: I will work to communicate my understanding and insight in the course material I will actively seek input to steer and shape the content discussed, and develop relevant resources I will be widely available for consultation and discussion I will do my best to cultivate a safe and open forum for discussion If you think that I am not fulfilling my commitments, or if you have other comments, I would hope that you are comfortable conveying your concerns and/or comments to me, either directly or anonymously. It is my genuine desire to help you navigate learning new concepts and ideas, and I want to do this to the best of my ability, and feedback is the best way to ensure this endeavour proceeds as smoothly as possible. For the curious traveller, MyLO is The University of Tasmania's learning management system, based on Brightspace as developed D2L. \u21a9","title":"Welcome to the home of solid-state physics"},{"location":"Lost/","text":"Placeholder \u00b6 Empty page This page is a placeholder: the treasure you seek is in another castle Unbaked The content here is still very much under development. Please come back soon! Introduction \u00b6 Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Text reference The material covered here is discussed in section(s) \\S of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Thermal expansion \u00b6 While the quadratic approximation of the interatomic potential is certainly the most important, the anharmonic term \\kappa_3(r-a)^3/6 also has physical significance. Let us examine its role visually by comparing the harmonic and the third order approximations in the plot below. Notice that the second-order approximation is symmetric around the minimum while the third-order term is not. r = np . linspace ( - 2 , 2.5 , 750 ) a = 0 b = 0 c = 1 d = - 0.2 U_quadratic = a + b * r + c * r ** 2 U_cubic = a + b * r + c * r ** 2 + d * r ** 3 r_min = - 2 r_max = 2.5 U_min = - 0.2 U_max = 4 E_t_min = 0.4 E_t_max = 3.5 N_values = 20 l_width = 1.5 N_active = 0 # Create figure fig = go . Figure () def U_c ( r ): return a + b * r + c * r ** 2 + d * r ** 3 def line ( E_t ): right = min ( np . roots ([ c , b , a - E_t ])) roots = np . roots ([ d , c , b , a - E_t ]) roots = np . real ( roots [ np . isreal ( roots )]) roots . sort () left = roots [ 1 ] return [ left , right ] def avg_pos_cubic ( E_t ): Z = integrate . simps ( np . exp ( - U_cubic / E_t ), r ) r_avg = integrate . simps ( r * np . exp ( - U_cubic / E_t ), r ) x = r_avg / Z return x # Add traces, one for each slider step for E_t in np . linspace ( E_t_min , E_t_max , N_values ): avg = avg_pos_cubic ( E_t ) fig . add_trace ( go . Scatter ( visible = False , x = r , y = U_quadratic , mode = 'lines' , line_color = 'blue' , name = \"Quadratic potential\" , )) fig . add_trace ( go . Scatter ( visible = False , x = r , y = U_cubic , mode = 'lines' , line_color = 'red' , name = \"Cubic potential\" , )) fig . add_trace ( go . Scatter ( visible = False , x = line ( E_t ), y = [ E_t , E_t ], mode = 'lines' , line_color = 'black' , name = r 'Thermal energy level' )) fig . add_trace ( go . Scatter ( visible = False , x = [ 0 , 0 ], y = [ 0 , E_t ], mode = 'lines' , line_color = 'blue' , line_dash = 'dot' , name = r '$\\langle r \\rangle$ for the quadratic potential' )) fig . add_trace ( go . Scatter ( visible = False , x = [ avg , avg ], y = [ U_c ( avg ), E_t ], mode = 'lines' , line_color = 'red' , line_dash = 'dot' , name = r '$\\langle r \\rangle$ for the cubic potential' )) # Initial starting image N_trace = int ( len ( fig . data ) / N_values ) # Number of traces added per step for j in range ( N_trace ): fig . data [ N_active * N_trace + j ] . visible = True # Creation of the aditional images steps = [] for i in range ( int ( len ( fig . data ) / N_trace )): step = dict ( method = \"restyle\" , args = [{ \"visible\" : [ False ] * len ( fig . data )}], value = str ( 0.1 * ( i + 1 )) ) for j in range ( N_trace ): step [ \"args\" ][ 0 ][ \"visible\" ][ N_trace * i + j ] = True # Toggle i'th trace to \"visible\" steps . append ( step ) # Creating the slider sliders = [ dict ( tickcolor = 'White' , font_color = 'White' , currentvalue_font_color = 'Black' , active = N_active , name = r 'Thermal Energy' , font_size = 16 , currentvalue = { \"prefix\" : r 'Thermal Energy k_B T: ' }, pad = { \"t\" : 50 }, steps = steps , )] # Updating the images for each step fig . update_layout ( sliders = sliders , showlegend = True , plot_bgcolor = 'rgb(254, 254, 254)' , width = 700 , height = 580 , xaxis = dict ( range = [ r_min , r_max ], visible = True , showticklabels = True , showline = True , linewidth = l_width , linecolor = 'black' , gridcolor = 'white' , tickfont = dict ( size = 16 )), yaxis = dict ( range = [ U_min , U_max ], visible = True , showticklabels = True , showline = True , linewidth = l_width , linecolor = 'black' , gridcolor = 'white' , tickfont = dict ( size = 16 )), title = { 'text' : r 'Thermal expansion of cubic potential' , 'y' : 0.9 , 'x' : 0.45 , 'xanchor' : 'center' , 'yanchor' : 'top' }, xaxis_title = r '$r$' , yaxis_title = r '$U [k_b T]$' , ) # Edit slider labels and adding text next to the horizontal bar indicating T_E for i in range ( N_values ): fig [ 'layout' ][ 'sliders' ][ 0 ][ 'steps' ][ i ][ 'label' ] = ' %.1f ' % (( E_t_max - E_t_min ) * i / ( N_values - 1 ) + E_t_min ) # Showing the figure plt . plot ( fig ) fig . show () The asymmetry due to nonzero \\kappa_3 slows the growth of the potential when the interatomic distance increases. On the other hand, when the interatomic distance decreases, the asymmetry accelerates the growth of the potential. Therefore, stretching the material is more energetically favorable than contracting it. As a result, thermal excitations increase the interatomic distance. This gives us a simple model of thermal expansion . Van der Waals bond \u00b6 While we focus on the mechanisms of covalent bonding, let us also review another bond type. A Van der Waals bond originates from an attraction between the dipole moments of two atoms. Suppose we have two atoms separated by an interatomic distance r . If one atom has a dipole moment \\mathbf{p_1} , it creates an electric field \\mathbf{E} = \\frac{\\mathbf{p_1}}{4\\pi \\varepsilon_0 r^3} at the position of the other atom. The other atom then develops a dipole moment \\mathbf{p_2} = \\chi \\mathbf{E} with \\chi the polarizability of the atom. The potential energy between between the two dipoles is \\begin{align} U(r) &= \\frac{-|\\mathbf{p_1}||\\mathbf{p_2}|}{4\\pi\\varepsilon_0 r^3}\\\\ &= \\frac{-|\\mathbf{p_1}| \\chi \\mathbf{E}}{4\\pi\\varepsilon_0 r^3}\\\\ &= \\frac{-|\\mathbf{p_1}|^2 \\chi}{(4\\pi\\varepsilon_0 r^3)^2}\\\\ &\\propto \\frac{1}{r^6}. \\end{align} The dipole attraction is much weaker than the covalent bonds but drops slower with increasing distance. How does the strength of a covalent bond scale with distance? The strength of the bond is determined by the interatomic hopping integral -t = \\langle 1 | H | 2 \\rangle . Since the wavefunction of a bound electron typically decays exponentially, so does the overlap integral. Although the Van der Waals force is weak, it is the only force when there are no chemically active electrons or when the atoms are too far apart to form covalent bonds. Therefore, there are materials where Van der Waals interactions are the dominant interactions. An example of such a material is graphite. The Van der Waals bonds in graphite hold layers of covalently bonded carbon atoms together: (image source: Wikipedia ) Looking ahead: multiple atoms \u00b6 So far we have only considered the interatomic interactions between diatomic systems. However, our aim is to understand electrons and phonons in solids containing N\\to\\infty atoms. Let us see what happens when we consider more than two atoms. Phonons \u00b6 In order to understand phonons better, we need to understand how a vibrational motion in a solid arises. To that end, we model an array of atoms that are connected by springs with a spring constant \\kappa . Our plan: Consider only a harmonic potential acting between the atoms ( \\kappa is constant) Write down equations of motion Compute normal modes For simplicity we consider 1D motion, and let us start with a chain of 3 atoms: # Defining constants y_max = 6 max_m = 3 # Off set for the new masses deviation_arr = [ + 2 , - 2 , - 3.5 ] def plot_spring ( x1 , x2 , y , annotation = r '$\\kappa$' ): L = ( x2 - x1 ) width , nturns = 1 , 10 N = 1000 pad = 200 # Make the spring, unit scale (0 to 1) x_spring = np . linspace ( 0 , L , N ) # distance along the spring y_spring = np . zeros ( N ) y_spring [ pad : - pad ] = width * np . sin ( 2 * np . pi * nturns * x_spring [ pad : - pad ] / L ) x_plot , y_plot = np . vstack (( x_spring , y_spring )) # And offset it x_plot += x1 y_plot += y ax . plot ( x_plot , y_plot , c = 'k' ) ax . annotate ( annotation , (( x2 + x1 ) / 2 , y + 2 ), fontsize = 40 ) def plot_mass ( x , y , annotation = r '$m$' ): mass = Circle (( x , y ), . 5 , fc = 'k' ) ax . add_patch ( mass ) ax . annotate ( annotation , ( x -. 5 , y + 2 ), fontsize = 30 ) def make_plot ( u , deviation_arr , max_masses = 3 ): # plotting initial masses plot_mass ( 0 , 0 ) plot_mass ( 0 + deviation_arr [ 0 ], u , annotation = '' ) # Plot initial dotted lines pyplot . plot (( 0 , 0 ), ( 0 , u - 2.5 ), 'k--' ) pyplot . plot (( deviation_arr [ 0 ], deviation_arr [ 0 ]), ( u , u - 2.5 ), 'k--' ) # pyplot.plot((0, deviation_arr[0]), (u-2.5, u-2.5), 'k--') pyplot . arrow ( 0 , u - 2.5 , deviation_arr [ 0 ] -. 5 , 0 , fc = \"k\" , ec = \"k\" , head_width =. 25 , head_length =. 5 ) # Annotate the deviations annot_arr = [ r '$u_1$' , r '$u_2$' , r '$u_3$' ] off_set = - 1 ax . annotate ( annot_arr [ 0 ], ( deviation_arr [ 0 ] / 2 + off_set +. 4 , u - 3 + off_set ), fontsize = 30 ) # Annotate large arrow containing the interatomic distance pyplot . arrow ( 0 , u / 3 , 9 , 0 , fc = \"k\" , ec = \"k\" , head_width =. 5 , head_length = 1 ) ax . annotate ( r '$a$' , ( 5 , u / 3 + off_set ), fontsize = 30 ) # Plotting other masses and springs for j in range ( max_masses - 1 ): # Equilibrium plot plot_spring ( 10 * j , 10 * ( j + 1 ), 0 ) plot_mass ( 10 * ( j + 1 ), 0 ) # Plot containing deviations from it's equilibrium plot_spring ( 10 * j + deviation_arr [ j ], 10 * ( j + 1 ) + deviation_arr [ j + 1 ], u , annotation = '' ) plot_mass ( 10 * ( j + 1 ) + deviation_arr [ j + 1 ], u , annotation = '' ) # Plot dotted lines pyplot . plot (( 10 * ( j + 1 ), 10 * ( j + 1 )), ( 0 , u - 2.5 ), 'k--' ) pyplot . plot (( 10 * ( j + 1 ) + deviation_arr [ j + 1 ], 10 * ( j + 1 ) + deviation_arr [ j + 1 ]), ( u , u - 2.5 ), 'k--' ) # pyplot.plot((10*(j+1), 10*(j+1)+deviation_arr[j+1]), (u-2.5, u-2.5), 'k--') pyplot . arrow ( 10 * ( j + 1 ), u - 2.5 , ( deviation_arr [ j + 1 ] +. 5 ), 0 , fc = \"k\" , ec = \"k\" , head_width =. 25 , head_length =. 5 ) # Annotations ax . annotate ( annot_arr [ j + 1 ], ( 10 * ( j + 1 ) + deviation_arr [ j + 1 ] / 2 + off_set +. 4 , u - 3 + off_set ), fontsize = 30 ) # Initializing figure fig , ax = pyplot . subplots ( figsize = ( 10 , 7 )) pyplot . axis ( 'off' ) ax . set_xlim (( - 1 , 10 * ( max_m - 1 ) + 1 )) ax . set_ylim (( - y_max - 4.5 , y_max - 2.5 )) # Plottig system make_plot ( - y_max , deviation_arr , max_m ); fig . show () Let us denote the deviation of atom i from its equilibrium position by u_i . Newton's equations of motion for this system are then given by \\begin{aligned} m \\ddot{u}_1 &= - \\kappa (u_1 - u_2) \\\\ m \\ddot{u}_2 &= - \\kappa (u_2 - u_1) - \\kappa (u_2 - u_3) \\\\ m \\ddot{u}_3 &= - \\kappa (u_3 - u_2). \\end{aligned}, We write this system of equations in matrix form m \\ddot{\\mathbf{u}} = -\\kappa \\begin{pmatrix} 1 & -1 & 0\\\\ -1 & 2 & -1\\\\ 0 & -1 & 1 \\end{pmatrix}\\mathbf{u} We are interested in phonons, patterns of motion that are periodic and have a fixed frequency \\omega . Hence we guess that the motion of the atoms is \\mathbf{u}(t) = \\mathbf{u}_0 e^{i\\omega t}. We substitute our guess into the equations of motion to yield an eigenvalue problem: \\omega^2 \\mathbf{u}_0 = \\frac{\\kappa}{m} \\begin{pmatrix} 1 & -1 & 0\\\\ -1 & 2 & -1\\\\ 0 & -1 & 1 \\end{pmatrix}\\mathbf{u}_0. The solutions to the eigenvalue problem are phonon modes. Electrons \u00b6 We just looked at how a chain of atoms moves. Let us now look at how the electrons of those atoms behave. To that end, we consider a 3 atom chain without any motion. In order to understand how electrons behave, we use the LCAO model. The LCAO model generalizes in a very simple way. Let us consider the wavefunction: \\vert \\psi\\rangle = \\varphi_1 |1\\rangle + \\varphi_2 |2\\rangle + \\varphi_3 |3\\rangle. Because the three atoms are identical, the onsite energy is the same on all atoms \\langle 1|H|1 \\rangle = \\langle2|H|2 \\rangle = \\langle3|H|3 \\rangle = E_0 . Furthermore, we assume hopping only between the nearest neighbors and assume that it is real valued: \\langle1|H|2\\rangle = \\langle2|H|3\\rangle = -t . We also assume that the orbitals are orthogonal to eachother. Just as we did in the previous lecture, we use the Schr\u00f6dinger equation H |\\psi\\rangle = E |\\psi\\rangle to set up a system of equations: \\begin{align} E \\varphi_1 &= E_0 \\varphi_1 - t \\varphi_2\\\\ E \\varphi_2 &= E_0 \\varphi_2 - t \\varphi_1 - t \\varphi_3\\\\ E \\varphi_3 &= E_0 \\varphi_3 -t \\varphi_2. \\end{align} Again, we write this in a matrix form: E \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3 \\end{pmatrix} = \\begin{pmatrix} E_0 & -t & 0 \\\\ -t & E_0 & -t \\\\ 0 & -t & E_0 \\end{pmatrix} \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3. \\end{pmatrix} Numerical test \u00b6 Diagonalizing large matrices is unwieldy, but let's try and check it numerically to see if we notice a trend. Let us first model 3 atoms on a chain. The eigenfrequencies of the 3 atoms are: [0.0 1.0 1.732050] def DOS_finite_phonon_chain ( n ): rhs = 2 * np . eye ( n ) - np . eye ( n , k = 1 ) - np . eye ( n , k =- 1 ) rhs [ 0 , 0 ] -= 1 rhs [ - 1 , - 1 ] -= 1 pyplot . figure () pyplot . hist ( np . sqrt ( np . abs ( np . linalg . eigvalsh ( rhs ))), bins = 30 ) pyplot . xlabel ( \"$\\omega$\" ) pyplot . ylabel ( \"Number of eigenfrequencies\" ) DOS_finite_phonon_chain ( 3 ) The eigenenergies of the 3 orbitals are: [-1.41421356 0.0 1.41421356] def DOS_finite_electron_chain ( n ): rhs = 2 * np . eye ( n , k = 0 ) - np . eye ( n , k = 1 ) - np . eye ( n , k = - 1 ) pyplot . figure () pyplot . hist ( np . linalg . eigvalsh ( rhs ), bins = 30 ) pyplot . xlabel ( \"$E$\" ) pyplot . ylabel ( \"Number of eigenenergies\" ) DOS_finite_electron_chain ( 3 ) However, 3 atoms are far too few to model an actual solid. Hence, we need 'many more' atoms. From 3 atoms to 300 \u00b6 Phonon modes of the many atom chain are shown below. DOS_finite_phonon_chain ( 300 ) We observe that when \\omega is small, we have a constant DOS. This is in line with what we saw in the Debye model. There the DOS of a 1D system was constant! However, when the frequencies are higher, the DOS is not constant anymore. A plot of electron energies in the many atom chain is shown below. DOS_finite_electron_chain ( 300 ) The numerical results once again agree with the models we developed earlier. In the Sommerfeld free electron model, the DOS in 1D is proportional to \\frac{1}{\\sqrt{E}} . The above histogram also reflects this proportionality for small energies E . However, when E is higher, we observe a significant deviation from the \\frac{1}{\\sqrt{E}} behavior. In both cases, we find that our models agree with the numerical results whenever frequencies/energies are small. When the frequencies/energies are high, we find that there is a significant deviation from the Debye/Sommerfeld models. The nature of this deviation is the subject of the next lecture! Conclusions \u00b6 The DOS of phonons used in the Debye model is justified by modeling the atoms as particles on a chain connected by a spring in the small \\omega limit. The DOS of electrons in the Sommerfeld model is justified by modeling electrons as particles that can hop between atoms in the small E limit. Exercises \u00b6 Preliminary provocations \u00b6 What does the LCAO matrix in the lecture notes look like if we also consider a next nearest-neighbour hopping -\\tilde{t} ? What does the LCAO matrix look like if we consider six atoms instead of three? You may assume that each atom has a single orbital, onsite energy E_0 and the hopping between neighbouring atoms -t . How do you determine which part of the interatomic potential is attractive and which is repulsive? You may assume that the interatomic potential is only a function of the interatomic distance r . Exercise 1: Linear triatomic molecule \u00b6 Consider carbon dioxide (C0 _2 ) which is a linear triatomic molecule shown below ??? info \"source\" By Jasek FH. - Own work, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/3.0 \"Creative Commons Attribution-Share Alike 3.0\"), [Link](https://commons.wikimedia.org/w/index.php?curid=2875238) How many normal modes does this molecule have assuming motion in only 1D? How many normal modes does it have if the atoms can move in all three dimensions? For simplicity, we only consider 1D motion of the atoms. Write down Newton's equations of motion for the atoms, you may assume that the spring constant is the same for both bonds. Consider a symmetric mode, for which the displacements of the oxygen atoms are equal in magnitude and have an opposite direction. Find the eigenfrequency of this mode. Now consider the antisymmetric mode when both of the oxygen atoms move in phase and have the same displacement. Find the ratio between the displacements of the carbon and oxygen atoms. Make sure that the center of mass of the molecule is at rest. Compute the eigenfrequency of the antisymmetric mode. Hint Compare your answers with Wikipedia . From Diatomic solids Exercise 2: the Peierls transition \u00b6 In the previous lecture, we have derived the electronic band structure of an 1D, equally spaced atomic chain. Such chains, however, are in fact not stable and the equal spacing will be distorted. This is also known as the Peierls transition . The spacing of the distorted chain alternates between two different distances and this also causes the hopping energy to alternate between t_1 and t_2 . We further set the onsite energies of the atoms to \\epsilon . The situation is depicted in the figure below. Due to the alternating hopping energies, we must treat two consecutive atoms as two different orbitals ( |n,1\u27e9 and |n,2 \u27e9 in the figure) from the same unit cell. The corresponding LCAO of this chain is given by \\left|\\Psi \\right\\rangle = \\sum_n \\left(\\phi_n \\left| n,1 \\right\\rangle + \\psi_n \\left| n,2 \\right\\rangle\\right) As usual, we assume that all these atomic orbitals are orthogonal to each other. Indicate the length of the unit cell a in the figure. Using the Schr\u00f6dinger equation, write the equations of motion of the electrons. Hint To this end, find expressions for E \\left< n,1 \\vert \\Psi \\right> = \\left< n,1 \\right| H \\left|\\Psi \\right> and E \\left< n,2 \\vert \\Psi \\right> = \\left< n,2 \\right| H \\left|\\Psi \\right> . Using the trial solutions \\phi_n = \\phi_0 e^{ikna} and \\psi_n = \\psi_0 e^{ikna} , show that the Sch\u00f6dinger equation can be written in matrix form: \\begin{pmatrix} \\epsilon & t_1 + t_2 e^{-i k a} \\\\ t_1 + t_2 e^{i k a} & \\epsilon \\end{pmatrix} \\begin{pmatrix} \\phi_0 \\\\ \\psi_0 \\end{pmatrix} = E \\begin{pmatrix} \\phi_0 \\\\ \\psi_0 \\end{pmatrix}. Derive the dispersion relation of this Hamiltonian. Does it look like the figure of the band structure shown on the Wikipedia page ? Does it reduce to the 1D, equally spaced atomic chain if t_1 = t_2 ? Find an expression of the group velocity v(k) and effective mass m^*(k) of both bands. Derive an expression for the density of states g(E) of the entire band structure and make a plot of it. Does your result makes sense when considering the band structure? Tight binding \u00b6 Group velocity, effective mass, density of states \u00b6 (here we only discuss electrons; for phonons everything is the same except for replacing E = \\hbar \\omega ) Let us think what happens if we apply an external electric field to the crystal: x = np . linspace ( 0.001 , 3 , 200 ) fig , ax = pyplot . subplots ( 1 , 1 ) ax . plot ( x , 1.2 - 1 / np . abs ( np . sin ( np . pi * x )) ** ( 1 / 2 ) + . 2 * ( x - 1.5 )) ax . plot ( x , . 2 * ( x - 0.25 ), '--' ) ax . set_ylim ( -. 7 , . 5 ) ax . set_xlabel ( \"$x$\" ) ax . set_ylabel ( \"$U(x)$\" ) ax . set_xticks ([ -. 05 , 1 , 2 ]) ax . set_xticklabels ([ \"$0$\" , \"$a$\" , \"$2a$\" ]) draw_classic_axes ( ax ) The full Hamiltonian of the system is H = \\frac{p^2}{2m} + U_\\textrm{atomic}(x) + e \\mathcal{E} x, where U_\\textrm{atomic} is the potential created by the nuclei, and \\mathcal{E} the electric field. A typical electric field is much smaller than the interatomic potential, and therefore we can start by obtaining the dispersion relation E(k) without electric field (by applying the LCAO method), and then solve H = E(k) + e\\mathcal{E}x. To derive how particles with an arbitrary dispersion relation move, we recall the Hamilton's equations for particle velocity v and force F : \\begin{aligned} v \\equiv \\frac{dr}{dt} &= \\frac{\\partial H(p, r)}{\\partial p}\\\\ F \\equiv \\frac{dp}{dt} &= -\\frac{\\partial H(p, r)}{\\partial r}. \\end{aligned} Substituting p = \\hbar k into the first equation we arrive to the expression for the electron group velocity v \\equiv \\hbar^{-1}\\partial E/\\partial k . From the second equation we obtain that the force acting on electron in a band stays -e\\mathcal{E} , which in turn gives results in the acceleration \\frac{dv}{dt} = \\frac{\u2202v}{\u2202p}\\frac{dp}{dt} = F/m. Comparing this expression with dv/dt = F/m , we arrive to the effective mass : m^* \\equiv \\left(\\frac{\u2202v}{\u2202p}\\right)^{-1} = \\left(\\frac{\u2202\u00b2E}{\u2202p\u00b2}\\right)^{-1} = \u0127\u00b2\\left(\\frac{\u2202\u00b2E}{\u2202k\u00b2}\\right)^{-1}. The group velocity describes how quickly electrons with a certain k -vector move, while the effective mass describes how hard they are to accelerate by applying external force. By using the dispersion relation we derived earlier, we obtain the effective mass like this: pyplot . figure ( figsize = ( 8 , 5 )) k = np . linspace ( - pi , pi , 300 ) meff = 1 / np . cos ( k ) color = list ( matplotlib . rcParams [ 'axes.prop_cycle' ])[ 0 ][ 'color' ] pyplot . plot ( k [ meff > 0 ], meff [ meff > 0 ], c = color ) pyplot . plot ( k [ meff < 0 ], meff [ meff < 0 ], c = color ) pyplot . ylim ( - 5 , 5 ) pyplot . xlabel ( '$ka$' ); pyplot . ylabel ( '$m^*$' ) pyplot . xticks ([ - pi , 0 , pi ], [ r '$-\\pi$' , 0 , r '$\\pi$' ]); Notice that the effective mass can be negative, which implies the electrons accelerate in the direction opposite to the applied force. Density of states \u00b6 The DOS is the number of states per unit energy. In 1D we have g(E) = \\frac{L}{2\\pi}\\sum |dk/dE| = \\frac{L}{2\\pi}\\sum |v|^{-1} The sum goes over all possible values of k and spin which have the same energy E . If we are working in two or more dimensions, we must integrate over the values of k with the same energy. Also take note that for energies below E_0 - 2t or above E_0 + 2t , there are no values of k with that energy, so there is nothing to sum over. Once again, starting from E = E_0 - 2t \\cos(ka), we get ka = \\pm\\arccos[(E - E_0) / 2t], and |v| ^{-1} = \\left|\\frac{dk}{dE} \\right| = \\frac{1}{a}\\frac{1}{\\sqrt{4t^2 - (E - E_0)^2}}. You can get to this result immediately if you remember the derivative of arccosine. Otherwise you need to go the long way: compute dE/dk as a function of k , express k through E as we did above, and take the inverse. We now add together the contributions of the positive and the negative momenta as well both spin orientations, and arrive to the density of states g(E) = \\frac{L}{2\\pi}\\frac{4}{a}\\frac{1}{\\sqrt{4t^2 - (E - E_0)^2}}. A quick check: when the energy is close to the bottom of the band, E = E_0 - 2t + \\delta E , we get g(E) \\propto \\delta E^{-1/2} , as we expect in 1D. The process of calculating the DOS at a given energy E of a spin-independent Hamiltonian is done systematically with the following steps: At a given energy E , determine all of the values of k which correspond to that E using the dispersion relation. Compute \\rvert dk / dE \\rvert . Do this either by writing k as a (multi-valued) function of E and differentiating, or by computing (dE / dk)^{-1} . Sum or integrate dk / dE over the allowed values of k found in 1 and multiply by any degeneracies (spin/polarization). Multiply by spin degeneracy. If the Hamiltonian depends on spin, then there is no spin degeneracy and the spin number s must be treated in the same way as k .","title":"Placeholder"},{"location":"Lost/#placeholder","text":"Empty page This page is a placeholder: the treasure you seek is in another castle Unbaked The content here is still very much under development. Please come back soon!","title":"Placeholder"},{"location":"Lost/#introduction","text":"Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Text reference The material covered here is discussed in section(s) \\S of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"Lost/#thermal-expansion","text":"While the quadratic approximation of the interatomic potential is certainly the most important, the anharmonic term \\kappa_3(r-a)^3/6 also has physical significance. Let us examine its role visually by comparing the harmonic and the third order approximations in the plot below. Notice that the second-order approximation is symmetric around the minimum while the third-order term is not. r = np . linspace ( - 2 , 2.5 , 750 ) a = 0 b = 0 c = 1 d = - 0.2 U_quadratic = a + b * r + c * r ** 2 U_cubic = a + b * r + c * r ** 2 + d * r ** 3 r_min = - 2 r_max = 2.5 U_min = - 0.2 U_max = 4 E_t_min = 0.4 E_t_max = 3.5 N_values = 20 l_width = 1.5 N_active = 0 # Create figure fig = go . Figure () def U_c ( r ): return a + b * r + c * r ** 2 + d * r ** 3 def line ( E_t ): right = min ( np . roots ([ c , b , a - E_t ])) roots = np . roots ([ d , c , b , a - E_t ]) roots = np . real ( roots [ np . isreal ( roots )]) roots . sort () left = roots [ 1 ] return [ left , right ] def avg_pos_cubic ( E_t ): Z = integrate . simps ( np . exp ( - U_cubic / E_t ), r ) r_avg = integrate . simps ( r * np . exp ( - U_cubic / E_t ), r ) x = r_avg / Z return x # Add traces, one for each slider step for E_t in np . linspace ( E_t_min , E_t_max , N_values ): avg = avg_pos_cubic ( E_t ) fig . add_trace ( go . Scatter ( visible = False , x = r , y = U_quadratic , mode = 'lines' , line_color = 'blue' , name = \"Quadratic potential\" , )) fig . add_trace ( go . Scatter ( visible = False , x = r , y = U_cubic , mode = 'lines' , line_color = 'red' , name = \"Cubic potential\" , )) fig . add_trace ( go . Scatter ( visible = False , x = line ( E_t ), y = [ E_t , E_t ], mode = 'lines' , line_color = 'black' , name = r 'Thermal energy level' )) fig . add_trace ( go . Scatter ( visible = False , x = [ 0 , 0 ], y = [ 0 , E_t ], mode = 'lines' , line_color = 'blue' , line_dash = 'dot' , name = r '$\\langle r \\rangle$ for the quadratic potential' )) fig . add_trace ( go . Scatter ( visible = False , x = [ avg , avg ], y = [ U_c ( avg ), E_t ], mode = 'lines' , line_color = 'red' , line_dash = 'dot' , name = r '$\\langle r \\rangle$ for the cubic potential' )) # Initial starting image N_trace = int ( len ( fig . data ) / N_values ) # Number of traces added per step for j in range ( N_trace ): fig . data [ N_active * N_trace + j ] . visible = True # Creation of the aditional images steps = [] for i in range ( int ( len ( fig . data ) / N_trace )): step = dict ( method = \"restyle\" , args = [{ \"visible\" : [ False ] * len ( fig . data )}], value = str ( 0.1 * ( i + 1 )) ) for j in range ( N_trace ): step [ \"args\" ][ 0 ][ \"visible\" ][ N_trace * i + j ] = True # Toggle i'th trace to \"visible\" steps . append ( step ) # Creating the slider sliders = [ dict ( tickcolor = 'White' , font_color = 'White' , currentvalue_font_color = 'Black' , active = N_active , name = r 'Thermal Energy' , font_size = 16 , currentvalue = { \"prefix\" : r 'Thermal Energy k_B T: ' }, pad = { \"t\" : 50 }, steps = steps , )] # Updating the images for each step fig . update_layout ( sliders = sliders , showlegend = True , plot_bgcolor = 'rgb(254, 254, 254)' , width = 700 , height = 580 , xaxis = dict ( range = [ r_min , r_max ], visible = True , showticklabels = True , showline = True , linewidth = l_width , linecolor = 'black' , gridcolor = 'white' , tickfont = dict ( size = 16 )), yaxis = dict ( range = [ U_min , U_max ], visible = True , showticklabels = True , showline = True , linewidth = l_width , linecolor = 'black' , gridcolor = 'white' , tickfont = dict ( size = 16 )), title = { 'text' : r 'Thermal expansion of cubic potential' , 'y' : 0.9 , 'x' : 0.45 , 'xanchor' : 'center' , 'yanchor' : 'top' }, xaxis_title = r '$r$' , yaxis_title = r '$U [k_b T]$' , ) # Edit slider labels and adding text next to the horizontal bar indicating T_E for i in range ( N_values ): fig [ 'layout' ][ 'sliders' ][ 0 ][ 'steps' ][ i ][ 'label' ] = ' %.1f ' % (( E_t_max - E_t_min ) * i / ( N_values - 1 ) + E_t_min ) # Showing the figure plt . plot ( fig ) fig . show () The asymmetry due to nonzero \\kappa_3 slows the growth of the potential when the interatomic distance increases. On the other hand, when the interatomic distance decreases, the asymmetry accelerates the growth of the potential. Therefore, stretching the material is more energetically favorable than contracting it. As a result, thermal excitations increase the interatomic distance. This gives us a simple model of thermal expansion .","title":"Thermal expansion"},{"location":"Lost/#van-der-waals-bond","text":"While we focus on the mechanisms of covalent bonding, let us also review another bond type. A Van der Waals bond originates from an attraction between the dipole moments of two atoms. Suppose we have two atoms separated by an interatomic distance r . If one atom has a dipole moment \\mathbf{p_1} , it creates an electric field \\mathbf{E} = \\frac{\\mathbf{p_1}}{4\\pi \\varepsilon_0 r^3} at the position of the other atom. The other atom then develops a dipole moment \\mathbf{p_2} = \\chi \\mathbf{E} with \\chi the polarizability of the atom. The potential energy between between the two dipoles is \\begin{align} U(r) &= \\frac{-|\\mathbf{p_1}||\\mathbf{p_2}|}{4\\pi\\varepsilon_0 r^3}\\\\ &= \\frac{-|\\mathbf{p_1}| \\chi \\mathbf{E}}{4\\pi\\varepsilon_0 r^3}\\\\ &= \\frac{-|\\mathbf{p_1}|^2 \\chi}{(4\\pi\\varepsilon_0 r^3)^2}\\\\ &\\propto \\frac{1}{r^6}. \\end{align} The dipole attraction is much weaker than the covalent bonds but drops slower with increasing distance. How does the strength of a covalent bond scale with distance? The strength of the bond is determined by the interatomic hopping integral -t = \\langle 1 | H | 2 \\rangle . Since the wavefunction of a bound electron typically decays exponentially, so does the overlap integral. Although the Van der Waals force is weak, it is the only force when there are no chemically active electrons or when the atoms are too far apart to form covalent bonds. Therefore, there are materials where Van der Waals interactions are the dominant interactions. An example of such a material is graphite. The Van der Waals bonds in graphite hold layers of covalently bonded carbon atoms together: (image source: Wikipedia )","title":"Van der Waals bond"},{"location":"Lost/#looking-ahead-multiple-atoms","text":"So far we have only considered the interatomic interactions between diatomic systems. However, our aim is to understand electrons and phonons in solids containing N\\to\\infty atoms. Let us see what happens when we consider more than two atoms.","title":"Looking ahead: multiple atoms"},{"location":"Lost/#phonons","text":"In order to understand phonons better, we need to understand how a vibrational motion in a solid arises. To that end, we model an array of atoms that are connected by springs with a spring constant \\kappa . Our plan: Consider only a harmonic potential acting between the atoms ( \\kappa is constant) Write down equations of motion Compute normal modes For simplicity we consider 1D motion, and let us start with a chain of 3 atoms: # Defining constants y_max = 6 max_m = 3 # Off set for the new masses deviation_arr = [ + 2 , - 2 , - 3.5 ] def plot_spring ( x1 , x2 , y , annotation = r '$\\kappa$' ): L = ( x2 - x1 ) width , nturns = 1 , 10 N = 1000 pad = 200 # Make the spring, unit scale (0 to 1) x_spring = np . linspace ( 0 , L , N ) # distance along the spring y_spring = np . zeros ( N ) y_spring [ pad : - pad ] = width * np . sin ( 2 * np . pi * nturns * x_spring [ pad : - pad ] / L ) x_plot , y_plot = np . vstack (( x_spring , y_spring )) # And offset it x_plot += x1 y_plot += y ax . plot ( x_plot , y_plot , c = 'k' ) ax . annotate ( annotation , (( x2 + x1 ) / 2 , y + 2 ), fontsize = 40 ) def plot_mass ( x , y , annotation = r '$m$' ): mass = Circle (( x , y ), . 5 , fc = 'k' ) ax . add_patch ( mass ) ax . annotate ( annotation , ( x -. 5 , y + 2 ), fontsize = 30 ) def make_plot ( u , deviation_arr , max_masses = 3 ): # plotting initial masses plot_mass ( 0 , 0 ) plot_mass ( 0 + deviation_arr [ 0 ], u , annotation = '' ) # Plot initial dotted lines pyplot . plot (( 0 , 0 ), ( 0 , u - 2.5 ), 'k--' ) pyplot . plot (( deviation_arr [ 0 ], deviation_arr [ 0 ]), ( u , u - 2.5 ), 'k--' ) # pyplot.plot((0, deviation_arr[0]), (u-2.5, u-2.5), 'k--') pyplot . arrow ( 0 , u - 2.5 , deviation_arr [ 0 ] -. 5 , 0 , fc = \"k\" , ec = \"k\" , head_width =. 25 , head_length =. 5 ) # Annotate the deviations annot_arr = [ r '$u_1$' , r '$u_2$' , r '$u_3$' ] off_set = - 1 ax . annotate ( annot_arr [ 0 ], ( deviation_arr [ 0 ] / 2 + off_set +. 4 , u - 3 + off_set ), fontsize = 30 ) # Annotate large arrow containing the interatomic distance pyplot . arrow ( 0 , u / 3 , 9 , 0 , fc = \"k\" , ec = \"k\" , head_width =. 5 , head_length = 1 ) ax . annotate ( r '$a$' , ( 5 , u / 3 + off_set ), fontsize = 30 ) # Plotting other masses and springs for j in range ( max_masses - 1 ): # Equilibrium plot plot_spring ( 10 * j , 10 * ( j + 1 ), 0 ) plot_mass ( 10 * ( j + 1 ), 0 ) # Plot containing deviations from it's equilibrium plot_spring ( 10 * j + deviation_arr [ j ], 10 * ( j + 1 ) + deviation_arr [ j + 1 ], u , annotation = '' ) plot_mass ( 10 * ( j + 1 ) + deviation_arr [ j + 1 ], u , annotation = '' ) # Plot dotted lines pyplot . plot (( 10 * ( j + 1 ), 10 * ( j + 1 )), ( 0 , u - 2.5 ), 'k--' ) pyplot . plot (( 10 * ( j + 1 ) + deviation_arr [ j + 1 ], 10 * ( j + 1 ) + deviation_arr [ j + 1 ]), ( u , u - 2.5 ), 'k--' ) # pyplot.plot((10*(j+1), 10*(j+1)+deviation_arr[j+1]), (u-2.5, u-2.5), 'k--') pyplot . arrow ( 10 * ( j + 1 ), u - 2.5 , ( deviation_arr [ j + 1 ] +. 5 ), 0 , fc = \"k\" , ec = \"k\" , head_width =. 25 , head_length =. 5 ) # Annotations ax . annotate ( annot_arr [ j + 1 ], ( 10 * ( j + 1 ) + deviation_arr [ j + 1 ] / 2 + off_set +. 4 , u - 3 + off_set ), fontsize = 30 ) # Initializing figure fig , ax = pyplot . subplots ( figsize = ( 10 , 7 )) pyplot . axis ( 'off' ) ax . set_xlim (( - 1 , 10 * ( max_m - 1 ) + 1 )) ax . set_ylim (( - y_max - 4.5 , y_max - 2.5 )) # Plottig system make_plot ( - y_max , deviation_arr , max_m ); fig . show () Let us denote the deviation of atom i from its equilibrium position by u_i . Newton's equations of motion for this system are then given by \\begin{aligned} m \\ddot{u}_1 &= - \\kappa (u_1 - u_2) \\\\ m \\ddot{u}_2 &= - \\kappa (u_2 - u_1) - \\kappa (u_2 - u_3) \\\\ m \\ddot{u}_3 &= - \\kappa (u_3 - u_2). \\end{aligned}, We write this system of equations in matrix form m \\ddot{\\mathbf{u}} = -\\kappa \\begin{pmatrix} 1 & -1 & 0\\\\ -1 & 2 & -1\\\\ 0 & -1 & 1 \\end{pmatrix}\\mathbf{u} We are interested in phonons, patterns of motion that are periodic and have a fixed frequency \\omega . Hence we guess that the motion of the atoms is \\mathbf{u}(t) = \\mathbf{u}_0 e^{i\\omega t}. We substitute our guess into the equations of motion to yield an eigenvalue problem: \\omega^2 \\mathbf{u}_0 = \\frac{\\kappa}{m} \\begin{pmatrix} 1 & -1 & 0\\\\ -1 & 2 & -1\\\\ 0 & -1 & 1 \\end{pmatrix}\\mathbf{u}_0. The solutions to the eigenvalue problem are phonon modes.","title":"Phonons"},{"location":"Lost/#electrons","text":"We just looked at how a chain of atoms moves. Let us now look at how the electrons of those atoms behave. To that end, we consider a 3 atom chain without any motion. In order to understand how electrons behave, we use the LCAO model. The LCAO model generalizes in a very simple way. Let us consider the wavefunction: \\vert \\psi\\rangle = \\varphi_1 |1\\rangle + \\varphi_2 |2\\rangle + \\varphi_3 |3\\rangle. Because the three atoms are identical, the onsite energy is the same on all atoms \\langle 1|H|1 \\rangle = \\langle2|H|2 \\rangle = \\langle3|H|3 \\rangle = E_0 . Furthermore, we assume hopping only between the nearest neighbors and assume that it is real valued: \\langle1|H|2\\rangle = \\langle2|H|3\\rangle = -t . We also assume that the orbitals are orthogonal to eachother. Just as we did in the previous lecture, we use the Schr\u00f6dinger equation H |\\psi\\rangle = E |\\psi\\rangle to set up a system of equations: \\begin{align} E \\varphi_1 &= E_0 \\varphi_1 - t \\varphi_2\\\\ E \\varphi_2 &= E_0 \\varphi_2 - t \\varphi_1 - t \\varphi_3\\\\ E \\varphi_3 &= E_0 \\varphi_3 -t \\varphi_2. \\end{align} Again, we write this in a matrix form: E \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3 \\end{pmatrix} = \\begin{pmatrix} E_0 & -t & 0 \\\\ -t & E_0 & -t \\\\ 0 & -t & E_0 \\end{pmatrix} \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3. \\end{pmatrix}","title":"Electrons"},{"location":"Lost/#numerical-test","text":"Diagonalizing large matrices is unwieldy, but let's try and check it numerically to see if we notice a trend. Let us first model 3 atoms on a chain. The eigenfrequencies of the 3 atoms are: [0.0 1.0 1.732050] def DOS_finite_phonon_chain ( n ): rhs = 2 * np . eye ( n ) - np . eye ( n , k = 1 ) - np . eye ( n , k =- 1 ) rhs [ 0 , 0 ] -= 1 rhs [ - 1 , - 1 ] -= 1 pyplot . figure () pyplot . hist ( np . sqrt ( np . abs ( np . linalg . eigvalsh ( rhs ))), bins = 30 ) pyplot . xlabel ( \"$\\omega$\" ) pyplot . ylabel ( \"Number of eigenfrequencies\" ) DOS_finite_phonon_chain ( 3 ) The eigenenergies of the 3 orbitals are: [-1.41421356 0.0 1.41421356] def DOS_finite_electron_chain ( n ): rhs = 2 * np . eye ( n , k = 0 ) - np . eye ( n , k = 1 ) - np . eye ( n , k = - 1 ) pyplot . figure () pyplot . hist ( np . linalg . eigvalsh ( rhs ), bins = 30 ) pyplot . xlabel ( \"$E$\" ) pyplot . ylabel ( \"Number of eigenenergies\" ) DOS_finite_electron_chain ( 3 ) However, 3 atoms are far too few to model an actual solid. Hence, we need 'many more' atoms.","title":"Numerical test"},{"location":"Lost/#from-3-atoms-to-300","text":"Phonon modes of the many atom chain are shown below. DOS_finite_phonon_chain ( 300 ) We observe that when \\omega is small, we have a constant DOS. This is in line with what we saw in the Debye model. There the DOS of a 1D system was constant! However, when the frequencies are higher, the DOS is not constant anymore. A plot of electron energies in the many atom chain is shown below. DOS_finite_electron_chain ( 300 ) The numerical results once again agree with the models we developed earlier. In the Sommerfeld free electron model, the DOS in 1D is proportional to \\frac{1}{\\sqrt{E}} . The above histogram also reflects this proportionality for small energies E . However, when E is higher, we observe a significant deviation from the \\frac{1}{\\sqrt{E}} behavior. In both cases, we find that our models agree with the numerical results whenever frequencies/energies are small. When the frequencies/energies are high, we find that there is a significant deviation from the Debye/Sommerfeld models. The nature of this deviation is the subject of the next lecture!","title":"From 3 atoms to 300"},{"location":"Lost/#conclusions","text":"The DOS of phonons used in the Debye model is justified by modeling the atoms as particles on a chain connected by a spring in the small \\omega limit. The DOS of electrons in the Sommerfeld model is justified by modeling electrons as particles that can hop between atoms in the small E limit.","title":"Conclusions"},{"location":"Lost/#exercises","text":"","title":"Exercises"},{"location":"Lost/#preliminary-provocations","text":"What does the LCAO matrix in the lecture notes look like if we also consider a next nearest-neighbour hopping -\\tilde{t} ? What does the LCAO matrix look like if we consider six atoms instead of three? You may assume that each atom has a single orbital, onsite energy E_0 and the hopping between neighbouring atoms -t . How do you determine which part of the interatomic potential is attractive and which is repulsive? You may assume that the interatomic potential is only a function of the interatomic distance r .","title":"Preliminary provocations"},{"location":"Lost/#exercise-1-linear-triatomic-molecule","text":"Consider carbon dioxide (C0 _2 ) which is a linear triatomic molecule shown below ??? info \"source\" By Jasek FH. - Own work, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/3.0 \"Creative Commons Attribution-Share Alike 3.0\"), [Link](https://commons.wikimedia.org/w/index.php?curid=2875238) How many normal modes does this molecule have assuming motion in only 1D? How many normal modes does it have if the atoms can move in all three dimensions? For simplicity, we only consider 1D motion of the atoms. Write down Newton's equations of motion for the atoms, you may assume that the spring constant is the same for both bonds. Consider a symmetric mode, for which the displacements of the oxygen atoms are equal in magnitude and have an opposite direction. Find the eigenfrequency of this mode. Now consider the antisymmetric mode when both of the oxygen atoms move in phase and have the same displacement. Find the ratio between the displacements of the carbon and oxygen atoms. Make sure that the center of mass of the molecule is at rest. Compute the eigenfrequency of the antisymmetric mode. Hint Compare your answers with Wikipedia . From Diatomic solids","title":"Exercise 1: Linear triatomic molecule"},{"location":"Lost/#exercise-2-the-peierls-transition","text":"In the previous lecture, we have derived the electronic band structure of an 1D, equally spaced atomic chain. Such chains, however, are in fact not stable and the equal spacing will be distorted. This is also known as the Peierls transition . The spacing of the distorted chain alternates between two different distances and this also causes the hopping energy to alternate between t_1 and t_2 . We further set the onsite energies of the atoms to \\epsilon . The situation is depicted in the figure below. Due to the alternating hopping energies, we must treat two consecutive atoms as two different orbitals ( |n,1\u27e9 and |n,2 \u27e9 in the figure) from the same unit cell. The corresponding LCAO of this chain is given by \\left|\\Psi \\right\\rangle = \\sum_n \\left(\\phi_n \\left| n,1 \\right\\rangle + \\psi_n \\left| n,2 \\right\\rangle\\right) As usual, we assume that all these atomic orbitals are orthogonal to each other. Indicate the length of the unit cell a in the figure. Using the Schr\u00f6dinger equation, write the equations of motion of the electrons. Hint To this end, find expressions for E \\left< n,1 \\vert \\Psi \\right> = \\left< n,1 \\right| H \\left|\\Psi \\right> and E \\left< n,2 \\vert \\Psi \\right> = \\left< n,2 \\right| H \\left|\\Psi \\right> . Using the trial solutions \\phi_n = \\phi_0 e^{ikna} and \\psi_n = \\psi_0 e^{ikna} , show that the Sch\u00f6dinger equation can be written in matrix form: \\begin{pmatrix} \\epsilon & t_1 + t_2 e^{-i k a} \\\\ t_1 + t_2 e^{i k a} & \\epsilon \\end{pmatrix} \\begin{pmatrix} \\phi_0 \\\\ \\psi_0 \\end{pmatrix} = E \\begin{pmatrix} \\phi_0 \\\\ \\psi_0 \\end{pmatrix}. Derive the dispersion relation of this Hamiltonian. Does it look like the figure of the band structure shown on the Wikipedia page ? Does it reduce to the 1D, equally spaced atomic chain if t_1 = t_2 ? Find an expression of the group velocity v(k) and effective mass m^*(k) of both bands. Derive an expression for the density of states g(E) of the entire band structure and make a plot of it. Does your result makes sense when considering the band structure?","title":"Exercise 2: the Peierls transition"},{"location":"Lost/#tight-binding","text":"","title":"Tight binding"},{"location":"Lost/#group-velocity-effective-mass-density-of-states","text":"(here we only discuss electrons; for phonons everything is the same except for replacing E = \\hbar \\omega ) Let us think what happens if we apply an external electric field to the crystal: x = np . linspace ( 0.001 , 3 , 200 ) fig , ax = pyplot . subplots ( 1 , 1 ) ax . plot ( x , 1.2 - 1 / np . abs ( np . sin ( np . pi * x )) ** ( 1 / 2 ) + . 2 * ( x - 1.5 )) ax . plot ( x , . 2 * ( x - 0.25 ), '--' ) ax . set_ylim ( -. 7 , . 5 ) ax . set_xlabel ( \"$x$\" ) ax . set_ylabel ( \"$U(x)$\" ) ax . set_xticks ([ -. 05 , 1 , 2 ]) ax . set_xticklabels ([ \"$0$\" , \"$a$\" , \"$2a$\" ]) draw_classic_axes ( ax ) The full Hamiltonian of the system is H = \\frac{p^2}{2m} + U_\\textrm{atomic}(x) + e \\mathcal{E} x, where U_\\textrm{atomic} is the potential created by the nuclei, and \\mathcal{E} the electric field. A typical electric field is much smaller than the interatomic potential, and therefore we can start by obtaining the dispersion relation E(k) without electric field (by applying the LCAO method), and then solve H = E(k) + e\\mathcal{E}x. To derive how particles with an arbitrary dispersion relation move, we recall the Hamilton's equations for particle velocity v and force F : \\begin{aligned} v \\equiv \\frac{dr}{dt} &= \\frac{\\partial H(p, r)}{\\partial p}\\\\ F \\equiv \\frac{dp}{dt} &= -\\frac{\\partial H(p, r)}{\\partial r}. \\end{aligned} Substituting p = \\hbar k into the first equation we arrive to the expression for the electron group velocity v \\equiv \\hbar^{-1}\\partial E/\\partial k . From the second equation we obtain that the force acting on electron in a band stays -e\\mathcal{E} , which in turn gives results in the acceleration \\frac{dv}{dt} = \\frac{\u2202v}{\u2202p}\\frac{dp}{dt} = F/m. Comparing this expression with dv/dt = F/m , we arrive to the effective mass : m^* \\equiv \\left(\\frac{\u2202v}{\u2202p}\\right)^{-1} = \\left(\\frac{\u2202\u00b2E}{\u2202p\u00b2}\\right)^{-1} = \u0127\u00b2\\left(\\frac{\u2202\u00b2E}{\u2202k\u00b2}\\right)^{-1}. The group velocity describes how quickly electrons with a certain k -vector move, while the effective mass describes how hard they are to accelerate by applying external force. By using the dispersion relation we derived earlier, we obtain the effective mass like this: pyplot . figure ( figsize = ( 8 , 5 )) k = np . linspace ( - pi , pi , 300 ) meff = 1 / np . cos ( k ) color = list ( matplotlib . rcParams [ 'axes.prop_cycle' ])[ 0 ][ 'color' ] pyplot . plot ( k [ meff > 0 ], meff [ meff > 0 ], c = color ) pyplot . plot ( k [ meff < 0 ], meff [ meff < 0 ], c = color ) pyplot . ylim ( - 5 , 5 ) pyplot . xlabel ( '$ka$' ); pyplot . ylabel ( '$m^*$' ) pyplot . xticks ([ - pi , 0 , pi ], [ r '$-\\pi$' , 0 , r '$\\pi$' ]); Notice that the effective mass can be negative, which implies the electrons accelerate in the direction opposite to the applied force.","title":"Group velocity, effective mass, density of states"},{"location":"Lost/#density-of-states","text":"The DOS is the number of states per unit energy. In 1D we have g(E) = \\frac{L}{2\\pi}\\sum |dk/dE| = \\frac{L}{2\\pi}\\sum |v|^{-1} The sum goes over all possible values of k and spin which have the same energy E . If we are working in two or more dimensions, we must integrate over the values of k with the same energy. Also take note that for energies below E_0 - 2t or above E_0 + 2t , there are no values of k with that energy, so there is nothing to sum over. Once again, starting from E = E_0 - 2t \\cos(ka), we get ka = \\pm\\arccos[(E - E_0) / 2t], and |v| ^{-1} = \\left|\\frac{dk}{dE} \\right| = \\frac{1}{a}\\frac{1}{\\sqrt{4t^2 - (E - E_0)^2}}. You can get to this result immediately if you remember the derivative of arccosine. Otherwise you need to go the long way: compute dE/dk as a function of k , express k through E as we did above, and take the inverse. We now add together the contributions of the positive and the negative momenta as well both spin orientations, and arrive to the density of states g(E) = \\frac{L}{2\\pi}\\frac{4}{a}\\frac{1}{\\sqrt{4t^2 - (E - E_0)^2}}. A quick check: when the energy is close to the bottom of the band, E = E_0 - 2t + \\delta E , we get g(E) \\propto \\delta E^{-1/2} , as we expect in 1D. The process of calculating the DOS at a given energy E of a spin-independent Hamiltonian is done systematically with the following steps: At a given energy E , determine all of the values of k which correspond to that E using the dispersion relation. Compute \\rvert dk / dE \\rvert . Do this either by writing k as a (multi-valued) function of E and differentiating, or by computing (dE / dk)^{-1} . Sum or integrate dk / dE over the allowed values of k found in 1 and multiply by any degeneracies (spin/polarization). Multiply by spin degeneracy. If the Hamiltonian depends on spin, then there is no spin degeneracy and the spin number s must be treated in the same way as k .","title":"Density of states"},{"location":"additional/","text":"Additional resources \u00b6 This page is the space for additional resources which may prove to be of some use and/or interest for curious individuals. Websites Open Solid State Notes from Delft University of Technology : a site after my own heart, given the usage of the same reference text and the same static site generator . Moreover, the open-source nature of their project ( CC BY-SA 4.0 ) is the reason much of the content here exists. 2021 is the first year that this course will run, and the dutiful preparation of content of a high calibre has allowed for reproduction of content, with edits running the gamut from a light to touch to utter destruction. Britney Spears' Guide to Semiconductor Physics : a relic of the time. A viral website before the phrase existed, the site does not hold up to modern standards - especially in light of the recent details of the singer's probate conservatorship - but sports surprisingly high-quality quality content relating to semiconductors, especially semiconductor lasers. Texts Quantum Mechanics and Quantum and Atom Optics by Daniel A. Steck from the University of Oregon : marvellous books that have been diligently curated and provide excellent reading for both revision and learning new content. Note that the Quantum and Atom Optics text contains relevant content and includes some of the most interesting material one is likely to encounter in the realm of quantum mechanics 1 , but it pitched at the graduate level and assumed knowledge of material not taught in the UTAS undergraduate program. I still think it worthwhile as an additional resource, but don't fret if it becomes a bit hard to follow once it gets into the weeds. Previous course notes Solid-state physics at UTAS is taught every odd-numbered year, with the previous course outing having been taught by Andrew Cole and Ross Turner for solid-state and semiconductor physics respectively. The notes from this course are posted here as a reference, but it should be emphasised that the course structure is vastly different, and consequently one's mileage may vary. Solid-state physics Week 1 : The lectures from week 1 of the Solid State Physics sequence, covering the introduction to solid state physics; the definition of a crystal; close-packed crystal structures; the Wigner-Seitz primitive cell; calculation of cohesive energy for an ionic solid; introduction to covalent and metallic solids. Week 2 - Lectures from week 2 of solid state: diffraction and scattering of x-rays, neutrons, and electrons by crystals. Bragg's Law, reciprocal lattices, Brillouin zones. Mechanical properties of solids: compressibility, stress, strain, dilation. Bulk modulus, shearing, and Poisson's ratio. Mechanical wave propagation. Week 3 - Crystal vibrations in the quantum limit; phonons. Monatomic and diatomic linear chains, and the optical vs. acoustic wave modes; phonon momentum. Thermal properties of solids. The law of Dulong & Petit, the Einstein model for state density, and the Debye model for specific heat. Week 4 - Introduction to Electrons in solids: the classical free electron picture, drift velocity and relaxation time. The Drude model for conductivity. The Fermi energy. Heat capacity of electrons, resistivity, conductivity, Ohm's Law, electron-phonon scattering. Magnetism: the Hall effect; the Weidemann-Franz law. Week 5 - Updates to electrons in solids: Umklapp scattering and resistivity; the Lorenz number of a metal. Magnetism: the Hall effect, magnetic susceptibility, diamagnetism, paramagnetism, Brillouin functions, and the Curie law for paramagnetism. Spontaneous magnetization, the Curie temperature, and the Weiss field (Exchange torque). Semiconductor physics Section 1 : Introduction to band theory looking at the Bloch Theorem, Kronig-Penney Model and the Empty Lattice Approximation. These are used to explain the electrical properties of solids, including conductors, insulator and simple intrinsic semiconductors. Section 2: Conduction properties of Semiconductors . This module looks at how the Fermi-Dirac distribution describes the changing conductivity of intrinsic semiconductors with temperature and explore how doping the crystal lattice with other atoms affects the conductivity. Section 3: PN Junctions and diodes . A brief look at some of the practical applications of semiconductors including light-emitting diodes and photo detectors. This includes a detailed description of the electrical behaviour of pn-junctions, crystals with one half doped with one type of atom and the other half with atoms from different group. and I am not just saying this because my research has been in this area, it is objectively true! \u21a9","title":"Additional resources"},{"location":"additional/#additional-resources","text":"This page is the space for additional resources which may prove to be of some use and/or interest for curious individuals. Websites Open Solid State Notes from Delft University of Technology : a site after my own heart, given the usage of the same reference text and the same static site generator . Moreover, the open-source nature of their project ( CC BY-SA 4.0 ) is the reason much of the content here exists. 2021 is the first year that this course will run, and the dutiful preparation of content of a high calibre has allowed for reproduction of content, with edits running the gamut from a light to touch to utter destruction. Britney Spears' Guide to Semiconductor Physics : a relic of the time. A viral website before the phrase existed, the site does not hold up to modern standards - especially in light of the recent details of the singer's probate conservatorship - but sports surprisingly high-quality quality content relating to semiconductors, especially semiconductor lasers. Texts Quantum Mechanics and Quantum and Atom Optics by Daniel A. Steck from the University of Oregon : marvellous books that have been diligently curated and provide excellent reading for both revision and learning new content. Note that the Quantum and Atom Optics text contains relevant content and includes some of the most interesting material one is likely to encounter in the realm of quantum mechanics 1 , but it pitched at the graduate level and assumed knowledge of material not taught in the UTAS undergraduate program. I still think it worthwhile as an additional resource, but don't fret if it becomes a bit hard to follow once it gets into the weeds. Previous course notes Solid-state physics at UTAS is taught every odd-numbered year, with the previous course outing having been taught by Andrew Cole and Ross Turner for solid-state and semiconductor physics respectively. The notes from this course are posted here as a reference, but it should be emphasised that the course structure is vastly different, and consequently one's mileage may vary. Solid-state physics Week 1 : The lectures from week 1 of the Solid State Physics sequence, covering the introduction to solid state physics; the definition of a crystal; close-packed crystal structures; the Wigner-Seitz primitive cell; calculation of cohesive energy for an ionic solid; introduction to covalent and metallic solids. Week 2 - Lectures from week 2 of solid state: diffraction and scattering of x-rays, neutrons, and electrons by crystals. Bragg's Law, reciprocal lattices, Brillouin zones. Mechanical properties of solids: compressibility, stress, strain, dilation. Bulk modulus, shearing, and Poisson's ratio. Mechanical wave propagation. Week 3 - Crystal vibrations in the quantum limit; phonons. Monatomic and diatomic linear chains, and the optical vs. acoustic wave modes; phonon momentum. Thermal properties of solids. The law of Dulong & Petit, the Einstein model for state density, and the Debye model for specific heat. Week 4 - Introduction to Electrons in solids: the classical free electron picture, drift velocity and relaxation time. The Drude model for conductivity. The Fermi energy. Heat capacity of electrons, resistivity, conductivity, Ohm's Law, electron-phonon scattering. Magnetism: the Hall effect; the Weidemann-Franz law. Week 5 - Updates to electrons in solids: Umklapp scattering and resistivity; the Lorenz number of a metal. Magnetism: the Hall effect, magnetic susceptibility, diamagnetism, paramagnetism, Brillouin functions, and the Curie law for paramagnetism. Spontaneous magnetization, the Curie temperature, and the Weiss field (Exchange torque). Semiconductor physics Section 1 : Introduction to band theory looking at the Bloch Theorem, Kronig-Penney Model and the Empty Lattice Approximation. These are used to explain the electrical properties of solids, including conductors, insulator and simple intrinsic semiconductors. Section 2: Conduction properties of Semiconductors . This module looks at how the Fermi-Dirac distribution describes the changing conductivity of intrinsic semiconductors with temperature and explore how doping the crystal lattice with other atoms affects the conductivity. Section 3: PN Junctions and diodes . A brief look at some of the practical applications of semiconductors including light-emitting diodes and photo detectors. This includes a detailed description of the electrical behaviour of pn-junctions, crystals with one half doped with one type of atom and the other half with atoms from different group. and I am not just saying this because my research has been in this area, it is objectively true! \u21a9","title":"Additional resources"},{"location":"particulars/","text":"Course information \u00b6 Administration \u00b6 The solid-state component of the course will run for seven weeks, beginning in week 7 and concluding at the end of semester. In previous years, the solid-state physics and semiconductor physics components of the course have been explicitly differentiated; however, in this iteration, the two will be more closely intertwined, with semiconductors being considered a flourish to the foundations that we shall construct during our adventures in describing matter. Prerequisite knowledge The content covered in this course is complicated, and without the frim bedrock of requisite knowledge and associated competencies, attempts to construct additional structures may be compromised. It is critical that one is comfortable with the following: The principles and machinery of quantum mechanics. Explicitly, an understanding of how physical systems and their evolution are modelled using the Schr\u00f6dinger equation, along with a fluency in common examples (e.g. particle in a box, harmonic oscillator, the hydrogen atom), and a vague familiarity with Dirac notation is assumed. Thermodynamic quantities and concepts abound, with statistical mechanics looming large in the background. Conveniently, you have just completed a course in statistical mechanics, but it will be assumed that you are comfortable with the content It is my intention that you will be required to call upon many of the other tools from the toolbox that you have been developing during your studies, with the explicit aim of further honing these tools, and maybe adding a few to the kit. Delivery of content \u00b6 The course will operate in a flipped-mode configuration, whereby the undergirding principle is that your out-of-class time is used to consume prepared content (e.g. lectures ) and scheduled times are used for discussions in a problem-based learning framework. I prefer to refer to the lecture-style material as the content download , and interactive, active-learning sessions as the content unpacking component. Subject matter \u00b6 The content for this course draws heavily from off the excellent text The Oxford Solid State Basics by Steven H. Simon , and the book is a prescribed text for the course, that is, it is assumed that you will access to this book. This particular text was chosen because of its concise discussion of the content, its accessibility, and the wry whit which permeates the content, in concert with the availability of freely distributed pre-print of the book . I will be working from the printed text, and I encourage you to do the same. Unsupported material Steven himself has said that the preprint is roughly 85% of the book; however, if you elect to work from the preprint, you do so at your own risk. Course outline \u00b6 Course summary This subject is designed to serve as an introduction into the field of solid-state physics. Solid-state physics is the largest field of condensed matter physics, which itself is the largest branch of physics, and so there is only so material we will cover. The trajectory we shall take begins with bulk descriptors of solids, into considering the fundamental nature of solids, collective behaviour within solids, and the place of these systems in the real world. A rough outline of the course is as follows: An introduction to solid-state physics The structure of materials Solids in one dimension The geometry of solids Electrons in solids Magnetism with approximately one week devoted to each topic, but with the natural ebb and flow ultimately dictating the timeline. The notes \u00b6 The notes are designed to be consumed in concert to the video content, with certain aspects highlighted differently in the different media, and in extreme cases with different paths used to arrive at the same destination. One of my aims is to expose you to different ways of thinking about problems, not just have the same method and then just \"press play\". Expected competencies Content has been constructed with the assumption that the material is consumed sequentially, with sections often explicitly relying on results from previous work. I have also to placed expected competency markers at the beginning of each section, outlining knowledge that I expect you to have seen in other areas of study, and importantly, these are cumulative from section to section. Text reference You will also encounter text references at the beginning of each section, relating to the relevant content in the course text The Oxford Solid State Basics . Computational content Computational content, which is normally just the jupyter notebook associated with the section, also appears at the top of the page, but may also contain links to other software or pertinent computational material. The videos \u00b6 The video content is hosted on Echo360 and is available only to those enrolled at the University of Tasmania, and is best accessed through the course MyLO page . Content download : table of contents A brief summary of the topics discussed in the content download sessions is shown below: Video Topic (click for details and timestamps) w0v01 An introduction to solid-state physics w1v01 The Einstein model of solids w1v02 The Debye model of solids w1v03 The Drude model of metals w2v01 The Sommerfeld free-electron model w2v02 Chemistry 101 w3v01 The 1D harmonic chain w3v02 The quantum chain w3v03 The diatomic chain w4v01 The tight binding model w4v02 Crystals w4v03 Crystals in three dimensions w5v01 The reciprocal lattice (part I) w5v02 The reciprocal lattice (part II) w5v03 Scattering (part I) w5v04 Scattering (part II) w6v01 Scattering (part III) Timestamps Timestamps detailing the topics discussed for each video are available in the video descriptions. Support \u00b6 You are not on this journey alone: there are many avenues available to you to help you on the journey (depending on your inclination to play antiquated video games, you may recognise this as a scene from The Legend of Zelda ). \" We are all in this together \" \u00b6 The course materials as consumed through the content download components are necessarily an individual effort, but in all other facets I strongly encourage collaboration. The structure of content unpacking sessions is deliberately geared towards discussion, the exchange of ideas, and collective problem solving moreso than the execution of a solution finding program. Computational resources \u00b6 As part of the course, it will be expected that you perform calculation and computations. You are welcome to do this in which ever language you prefer, but it is strongly recommended that you use Python , and indeed, this is the only language that will be supported. In order to ensure equitable and easy access to Python computing resources, a Jupyter Notebook server has been established, which allows for one to write and execute code via a web browser. The server is named Jove 1 , and access is through the JupyterHub portal . You will need to create an account to start using the server, but beyond this is should be click and go. Should you have a machine upon which you already have, or you wish to deploy, your own instance of Python , this is perfectly acceptable, but note that you will have to manually import the distributed materials into Jupyter. This can be effectively accomplished using the clone functionality of GitHub as this is where the course content is hosted. Bug catcher \u00b6 Finally, this is more of a request than anything else, but should you find any errors in the content - this site, the distributed notebooks, the content download sessions, whatever - please let me know so I can correct the content, which is a major boon for everyone involved. Thanks! For those wondering, Jove is an alternate name for the Roman god Jupiter. \u21a9","title":"Course particulars"},{"location":"particulars/#course-information","text":"","title":"Course information"},{"location":"particulars/#administration","text":"The solid-state component of the course will run for seven weeks, beginning in week 7 and concluding at the end of semester. In previous years, the solid-state physics and semiconductor physics components of the course have been explicitly differentiated; however, in this iteration, the two will be more closely intertwined, with semiconductors being considered a flourish to the foundations that we shall construct during our adventures in describing matter. Prerequisite knowledge The content covered in this course is complicated, and without the frim bedrock of requisite knowledge and associated competencies, attempts to construct additional structures may be compromised. It is critical that one is comfortable with the following: The principles and machinery of quantum mechanics. Explicitly, an understanding of how physical systems and their evolution are modelled using the Schr\u00f6dinger equation, along with a fluency in common examples (e.g. particle in a box, harmonic oscillator, the hydrogen atom), and a vague familiarity with Dirac notation is assumed. Thermodynamic quantities and concepts abound, with statistical mechanics looming large in the background. Conveniently, you have just completed a course in statistical mechanics, but it will be assumed that you are comfortable with the content It is my intention that you will be required to call upon many of the other tools from the toolbox that you have been developing during your studies, with the explicit aim of further honing these tools, and maybe adding a few to the kit.","title":"Administration"},{"location":"particulars/#delivery-of-content","text":"The course will operate in a flipped-mode configuration, whereby the undergirding principle is that your out-of-class time is used to consume prepared content (e.g. lectures ) and scheduled times are used for discussions in a problem-based learning framework. I prefer to refer to the lecture-style material as the content download , and interactive, active-learning sessions as the content unpacking component.","title":"Delivery of content"},{"location":"particulars/#subject-matter","text":"The content for this course draws heavily from off the excellent text The Oxford Solid State Basics by Steven H. Simon , and the book is a prescribed text for the course, that is, it is assumed that you will access to this book. This particular text was chosen because of its concise discussion of the content, its accessibility, and the wry whit which permeates the content, in concert with the availability of freely distributed pre-print of the book . I will be working from the printed text, and I encourage you to do the same. Unsupported material Steven himself has said that the preprint is roughly 85% of the book; however, if you elect to work from the preprint, you do so at your own risk.","title":"Subject matter"},{"location":"particulars/#course-outline","text":"Course summary This subject is designed to serve as an introduction into the field of solid-state physics. Solid-state physics is the largest field of condensed matter physics, which itself is the largest branch of physics, and so there is only so material we will cover. The trajectory we shall take begins with bulk descriptors of solids, into considering the fundamental nature of solids, collective behaviour within solids, and the place of these systems in the real world. A rough outline of the course is as follows: An introduction to solid-state physics The structure of materials Solids in one dimension The geometry of solids Electrons in solids Magnetism with approximately one week devoted to each topic, but with the natural ebb and flow ultimately dictating the timeline.","title":"Course outline"},{"location":"particulars/#the-notes","text":"The notes are designed to be consumed in concert to the video content, with certain aspects highlighted differently in the different media, and in extreme cases with different paths used to arrive at the same destination. One of my aims is to expose you to different ways of thinking about problems, not just have the same method and then just \"press play\". Expected competencies Content has been constructed with the assumption that the material is consumed sequentially, with sections often explicitly relying on results from previous work. I have also to placed expected competency markers at the beginning of each section, outlining knowledge that I expect you to have seen in other areas of study, and importantly, these are cumulative from section to section. Text reference You will also encounter text references at the beginning of each section, relating to the relevant content in the course text The Oxford Solid State Basics . Computational content Computational content, which is normally just the jupyter notebook associated with the section, also appears at the top of the page, but may also contain links to other software or pertinent computational material.","title":"The notes"},{"location":"particulars/#the-videos","text":"The video content is hosted on Echo360 and is available only to those enrolled at the University of Tasmania, and is best accessed through the course MyLO page . Content download : table of contents A brief summary of the topics discussed in the content download sessions is shown below: Video Topic (click for details and timestamps) w0v01 An introduction to solid-state physics w1v01 The Einstein model of solids w1v02 The Debye model of solids w1v03 The Drude model of metals w2v01 The Sommerfeld free-electron model w2v02 Chemistry 101 w3v01 The 1D harmonic chain w3v02 The quantum chain w3v03 The diatomic chain w4v01 The tight binding model w4v02 Crystals w4v03 Crystals in three dimensions w5v01 The reciprocal lattice (part I) w5v02 The reciprocal lattice (part II) w5v03 Scattering (part I) w5v04 Scattering (part II) w6v01 Scattering (part III) Timestamps Timestamps detailing the topics discussed for each video are available in the video descriptions.","title":"The videos"},{"location":"particulars/#support","text":"You are not on this journey alone: there are many avenues available to you to help you on the journey (depending on your inclination to play antiquated video games, you may recognise this as a scene from The Legend of Zelda ).","title":"Support"},{"location":"particulars/#we-are-all-in-this-together","text":"The course materials as consumed through the content download components are necessarily an individual effort, but in all other facets I strongly encourage collaboration. The structure of content unpacking sessions is deliberately geared towards discussion, the exchange of ideas, and collective problem solving moreso than the execution of a solution finding program.","title":"\"We are all in this together\""},{"location":"particulars/#computational-resources","text":"As part of the course, it will be expected that you perform calculation and computations. You are welcome to do this in which ever language you prefer, but it is strongly recommended that you use Python , and indeed, this is the only language that will be supported. In order to ensure equitable and easy access to Python computing resources, a Jupyter Notebook server has been established, which allows for one to write and execute code via a web browser. The server is named Jove 1 , and access is through the JupyterHub portal . You will need to create an account to start using the server, but beyond this is should be click and go. Should you have a machine upon which you already have, or you wish to deploy, your own instance of Python , this is perfectly acceptable, but note that you will have to manually import the distributed materials into Jupyter. This can be effectively accomplished using the clone functionality of GitHub as this is where the course content is hosted.","title":"Computational resources"},{"location":"particulars/#bug-catcher","text":"Finally, this is more of a request than anything else, but should you find any errors in the content - this site, the distributed notebooks, the content download sessions, whatever - please let me know so I can correct the content, which is a major boon for everyone involved. Thanks! For those wondering, Jove is an alternate name for the Roman god Jupiter. \u21a9","title":"Bug catcher"},{"location":"placeholder/","text":"Placeholder \u00b6 Empty page This page is a placeholder: the treasure you seek is in another castle Unbaked The content here is still very much under development. Please come back soon! Introduction \u00b6 Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Text reference The material covered here is discussed in section(s) \\S of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Content Conclusions \u00b6 Exercises \u00b6 Preliminary provocations \u00b6","title":"Quantum simulators"},{"location":"placeholder/#placeholder","text":"Empty page This page is a placeholder: the treasure you seek is in another castle Unbaked The content here is still very much under development. Please come back soon!","title":"Placeholder"},{"location":"placeholder/#introduction","text":"Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Text reference The material covered here is discussed in section(s) \\S of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Content","title":"Introduction"},{"location":"placeholder/#conclusions","text":"","title":"Conclusions"},{"location":"placeholder/#exercises","text":"","title":"Exercises"},{"location":"placeholder/#preliminary-provocations","text":"","title":"Preliminary provocations"},{"location":"1-intoduction/1-1-specificheatI/","text":"The specific heat of solids I \u00b6 Introduction \u00b6 We embark on our journey by starting at the nexus of the known and unknown, namely around the turn of the nineteenth and twentieth centuries, where the development of \"modern\" physics was being applied to systems which had hitherto be poorly understood. Now this is not to say that nothing was known about the systems, on the contrary: empirical laws had been used to great effect to describe the observable world, but with the increasing sophistication of experimental technique and apparatus, the cracks in certain rules started to appear. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Thermal physics: heat capacity Quantum mechanics: energy spectrum of the harmonic oscillator Statistical physics: the partition function, equipartition theorem Text reference The material covered here is discussed in section(s) \\S 2.1 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: The Dulong\u2013Petit law \u00b6 Consider the heat capacity of a solid. 1.1 Explain the concept of heat capacity in a manner understandable to someone without a science background. The heat capacity is the a measure of how much heat, or how much energy transfer, is required to change the temperature of a material. By measuring the heat capacity per weight, that is the mass-specific heat capacity , of a range of different elements, the two chemists Pierre Dulong and Alexis Petit observed the value was approximately constant when multiplied by the atomic weight of the element, stating that: c \\times M = constant where c is the specific heat capacity and M is the molar mass of the material. More commonly, one will see the law expressed in terms of the heat capacity C and number of moles n : C/n=\\frac{\\partial Q}{\\partial T} = 3R where R \\approx 8.314~\\mathrm{J K^{-1} mol^{-1}} is the ideal gas constant 1 . In the physics context, it is much more common do talk about the number of atoms N , which transforms the above equation into the Dulong-Peteit Law : C/N= 3 k_\\mathrm{B} But is this an accurate description? Let's have a look. Shown below is a plot of the heat capacity C 2 (in units of R ) as a function of atomic number: Heat capacity of the elements at room temperature as sourced from the CRC handbook of chemistry of physics which is pretty incredible. But a natural question arises: why is this the case? The Boltzmann model of a solid \u00b6 It was exactly the question of \"why does the Dulong-Petit law seem to work?\" that motivated Ludwig Boltzmann to use his novel - and at the time completely unaccepted - ideas, notably the existence of atoms and molecules and the mechanics that arises from statistically significant numbers of these atoms and molecules, to model unexplained systems. The insight of Boltzmann was to consider a solid as a collection of constituent particles, but unlike gasses, these particles would be strongly interacting. Explicitly, the idea of atoms interacting with their nearest neighbours through an elastic spring-like potential - an harmonic potential - would allow the system to be modelled with statistical mechanics. Like the case of a gas, energy can be stored in the system in the form of atomic motion, but unlike a gas, the motion of the atoms is constrained. A schematic of the atomic-scale model constructed by Boltzmann in an attempt to explain the Dulong-Petit law Whilst this may seem like a major leap forward, it is worth considering what the explaination for this behaviour had been prior to this proposal: nothing . Then, using the recently minted ideas such as the equipartition theorem , it was clear why the C/N= 3 k_\\mathrm{B} . To see this, recall that for a gas in thermal equilibrium, we have C_V/N = f/2 k_\\mathrm{B} where f is the number of thermodynamic degrees of freedom, or stated another way: each degree of freedom contributes k_\\mathrm{B}/2 to the heat capacity. Immediately, we can see that the Dulong\u2013Petit law is of this form, but suggests that the number of DoF is six, i.e. twice that of an ideal monatomic gas. 1.2 What is the difference between the heat capacities C_V and C_P ? What is the relationship between the two quantities, and what is the implication for the heat capacity of solids? As the heat capacity of an object is defined through \\frac{\\partial Q}{\\partial T} , one must consider the different thermodynamic processes (e.g. isochoric versus isobraic) as the heat supplied to the system will be different (e.g. dQ = dU versus dQ = dU + PdV ). It then follows that we define the heat capacity at constant volume C_V and the heat capacity at constant pressure C_P . Linking the two quantities is Mayer's relation, which states that for an ideal gas: C_P - C_V = nR and more generally C_P - C_V = \\frac{VT\\alpha^2}{\\beta} where \\alpha is the thermal expansion coefficient and \\beta is the isothermal compressibility. This should immediately point to the implication for solids: solids tend to be rather incompressible, which manifests in small values of \\alpha and \\beta , and especially small values of \\alpha^2 , and a negligible difference between C_P and C_V . Hence the usage of C ! The thermal expansion coefficients of various elements, noting that \\beta is of order 10^{-6} \\mathrm{m}^2 \\mathrm{N}^{-1} for squishy things (e.g. soft clay) down to 10^{-10} \\mathrm{m}^2 \\mathrm{N}^{-1} for things that a not squishy (e.g. rocks) Hopefully it not a mystery why there are additional degrees of freedom in a solid as compared to a monatomic gas: remember that Boltzmann's model of a solid is effectively a collection of harmonic oscillators, so not only could energy be stored in the motion of the motion of the constituent atoms, but also their position, storing energy in the \"bonds\" between atoms. Monatomic gas The degrees of freedom for a monatomic gas are listed below: Property Degrees of freedom Momentum p_x , p_y , and p_z The 4 degrees of freedom each contribute k_\\mathrm{B}/2 to the specific heat such that C=3k_\\mathrm{B}/2 . Diatomic gas The degrees of freedom for a diatomic gas (at room temperature) are listed below: Property Degrees of freedom Momentum p_x , p_y , and p_z Rotation Axial and end-over-end The 5 degrees of freedom each contribute k_\\mathrm{B}/2 to the specific heat such that C=5k_\\mathrm{B}/2 . Solid The degrees of freedom for a solid are listed below: Property Degrees of freedom Momentum p_x , p_y , and p_z Position x , y , and z The 6 degrees of freedom each contribute k_\\mathrm{B}/2 to the specific heat such that C=3k_\\mathrm{B} . This result stood as one of the great achievements of statistical mechanics at the time, as up to (and indeed past this point) it was still considered a fringe theory. So, what happened? Diamond is the worst \u00b6 but also, diamond is remarkable Whilst most people will be familiar with the cool properties of diamond, some things are best seen. For example, diamond is often quoted as having the greatest thermal conductivity of any material, but what does that actually look like? Well, take a look : If one inspects the plot of heat capacities for the different elements, one can see that there are a few outliers, but it should be made clear that these measurements were taken at room temperature. Above room temperature, there is widespread agreement - even better than room temperature - but if one makes the same measurements at low temperatures, the Dulong-Petit law completely falls apart, with C \\rightarrow 0 as T \\rightarrow 0 . And even at room temperature, some materials do not behave as expected, and in particular: diamond. At room temperature, diamond has a value of C/R \\approx 0.74 , which is much less than 3! The remarkable properties of diamond had long been known, and consequently any theory its salt had to explain why diamond was special. Shown below is a plot of the heat capacity of diamond versus temperature: The heat capacity of diamond as a function of temperature. Data has been sourced from Einstein's original paper of $T>230~\\mathrm{K}$ and from J. E. Desnoyehs & J. A. Morrison for $T < 230~\\mathrm{K}$. Immediately one can notice: C is not a constant Things are good at high temperature Things are bad at low temperature and Boltzmann's theory does not do anything to explain any of this. The Einstein model of a solid \u00b6 It is perhaps unsurprising that a both difficult and well-known problem became the focus of attention for Einstein, someone who even at the very beginning of his career showed remarkable insight into physical systems, often reasoning from observations what must be going on, and constructing a theory to make it all work. Following his work on the photoelectric effect and Brownian motion, he was well placed to tackle the problem of the unexpected behaviour of heat capacity at low-temperatures. The model \u00b6 Like Boltzmann's model, Einstein's model was based around atoms in an harmonic potential, but they key - and highly consequential - difference being that each atom is an identical potential, and that oscillation in said potential occurs at a frequency \\omega , later dubbed the Einstein frequency . Basically, he took Boltzmann's model, injected quantum mechanics and asked: what will be the result. It is worth pausing to point out that this was done prior to quantum mechanics having been developed: Einstein's explanation of the photoelectric effect is widely heralded as the starting point of quantum, but it was not until roughly 20 years later that the Schr\u00f6dinger equation was published! So this was a pretty wild assertion. To see the implications, we can make use of our knowledge of statistical physics and the quantum harmonic oscillator: using the energy eigenstates of the system E_n , we can calculate the partition function Z , then the expectation value for the energy \\langle E \\rangle , and ultimately the heat cavity C . 1.3 Beginning with the energy eigenstates of a single one-dimensional harmonic oscillator, show that the heat capacity for a single oscillator is C = k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} In one dimension, the energy eigenstates E_n of a single harmonic oscillator are given by: E_n = \\hbar\\omega(n+1/2) where \\omega is the frequency of the harmonic oscillator. The partition function is then given by: \\begin{aligned} Z = & \\sum_{n\\ge0} \\exp\\left[-\\beta\\hbar\\omega(n+1/2)\\right] \\\\ = & \\frac{\\exp(-\\beta\\hbar\\omega/2)}{1-\\exp(-\\beta\\hbar\\omega)} = \\frac{1}{2\\sinh(\\beta\\hbar\\omega/2)} \\end{aligned} We can then compute the expectation value of the energy \\langle E \\rangle via \\begin{aligned} \\langle E \\rangle = -\\frac{1}{Z}\\frac{\\partial Z}{\\partial \\beta} & = \\frac{\\hbar \\omega}{2}\\coth\\left(\\beta\\hbar\\omega/2\\right) \\\\ & = \\hbar \\omega \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + 1/2\\right) \\end{aligned} where n_\\mathrm{B} is the Bose occupation factor, defined as n_\\mathrm{B}(x) = \\frac{1}{\\exp(x)-1} It then straightforward to extract the heat capacity for a single oscillator through C = \\frac{\\partial \\langle E \\rangle}{\\partial T} = k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} The above result is stated for a one-dimensional harmonic oscillator, but to expend the system three dimensions we need to multiply this result by three 3 which gives the final result C = 3k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} 1.4 Produce a plot the specific heat C versus temperature for realistic values of \\omega , providing your code. Code the produce the plot as requested in shown below, along with the output of said code. Note that # Import all the goodies required for running code in this unit from ssp import * # Define a function to calculate the heat capacity def c_einstein ( T , w ): \"\"\" Calculate the specific heat capacity according to the Einstein model of a solid Input: --- T: Temperature [K] w: Einstein frequency \\omega [rad.s^-1] Returns: --- The heat capacity in units of k_B \"\"\" x = ( hbar * w ) / ( T * kb ) # scale the variable return 3 * x ** 2 * np . exp ( x ) / ( np . exp ( x ) - 1 ) ** 2 # compute the heat capacity # The range of temperatures over which the heat capity will be calculated # Note: overflow errors will occur is the x_min value is too small temp = np . linspace ( 10 , 1000 , 200 ) # The range of Einstein freqeuncies to be computed. For reference, diamond has \\omega \\approx 170 w = np . linspace ( 20 , 200 , 5 ) # Create the plot instance fig , ax = plt . subplots () # Plot and label each frequency for f in w : ax . plot ( temp , c_einstein ( temp , f * 1e12 ), label = f '$\\omega= { f : .0f } $ THz' ) # Make the plot readable ax . set_xlabel ( '$T [K]$' ) ax . set_ylabel ( r '$C/k_B$' ); ax . set_title ( r 'The Einstein model of heat capacity' ) ax . legend () # Save the figure plt . savefig ( '01_Einstein_c.svg' , facecolor = 'white' , transparent = False ) plt . show () # Show the plot A plot of the specific heat as computed from the Einstein model. Note that diamond has a an Einstein frequency of approximately \\omega = 170~\\mathrm{Hz} Looking at the from of C , it is clear: In the high-temperature limit ( k_{\\mathrm{B}} T/\\hbar\\omega \\gg 1 ) we recover the Dulong-Petit law In the low-temperature limit, the heat capacity is exponentially small But what is the physical interpretation of this behaviour? For higher temperatures ( k_{\\mathrm{B}} T/\\hbar\\omega \\gg 1 ), the ability to store energy in harmonic motion is unencumbered, with a decrease in temperature, this ceases to be the case of these degrees of freedom are \"frozen out\". Once the temperature is sufficiently low ( k_{\\mathrm{B}} T/\\hbar\\omega < 1 ), atoms are necessarily in the ground states of the harmonic oscillator; only with sufficient energy ( E = \\hbar\\omega ) can an atom be excited, and with a temperature much less than the energy level spacing, atoms are stuck and thus cannot absorb any energy. It is incredible that Einstein reasoned that this process must be occurring, which prompted him to describe the theory, essentially leading to him inventing the quantisation of energy levels. Coming up diamonds \u00b6 Attempting to explain the heat capacity of diamond had proven the death knell of all theories up to this point, and so it is unsurprising that in Einstein's original paper on the topic cantered around measurements of the heat capacity of diamond, which is shown below: Plot A plot of the molar heat capacity of diamond as a function of temperature. The plot is somewhat diabolical in its omission of labels and units, which should read k_\\mathrm{B}T/\\hbar\\omega and C~[\\mathrm{cal}~\\mathrm{K}^{-1}~\\mathrm{mol}^{-1}] for the x and y axes respectively Data The raw data used to produce the figure of the heat capacity of diamond There are obviously a few discrepancies, but on the whole it looks much better than the Boltzmann model and rightly was seen as a major triumph. But again, the question is why does this happen physically? What is it about diamond that makes is act so strangely? An energy-level diagram for the quantum harmonic oscillator, showing the wavefunctions for the three lowest-energy eigenstates Well if we consider the energy spacing of the harmonic oscillator, \\hbar\\omega , it is related to both the mass ( m ) and the spring constant ( \\kappa ) of the oscillator. For most materials, the Einstein frequency is such that C/N \\approx 3 k_\\mathrm{B} , but diamond has an especially low value of \\omega = \\sqrt{\\kappa/m} , which perhaps is unsurprising given that carbon is light (low m ) and diamond is incredibly hard (large \\kappa ). Conclusions \u00b6 The law of Dulong\u2013Petit is an observation that all materials have C \\approx 3k_B per atom. The Einstein model describes each atom in a solid as an independent quantum harmonic oscillator with the same eigenfrequency \\omega_0 . At sufficiently low T , the thermal excitations freeze out, resulting in \\langle E \\rangle = \\hbar \\omega_0/2 . The Einstein model correctly predicts that the heat capacity drops to 0 as T\\rightarrow 0 . Exercises \u00b6 Preliminary provocations \u00b6 What is the high-temperature heat capacity of an atom in a solid with two momentum and two spatial coordinate degrees of freedom? Sketch the Bose Einstein distribution as a function of \\omega for two different values of T Exercise 1: Total heat capacity of a diatomic material \u00b6 One of the assumptions of the Einstein model states that every atom in a solid oscillates with the same frequency \\omega_0 . However, if the solid contains different types of atoms, it is unreasonable to assume that the atoms oscillate with the same frequency. One example of such a solid is a lithium crystal, which consists of the two stable isotopes ^6 Li (7.5%) and ^7 Li (92.5%) in their natural abundance. Let us extend the Einstein model to take into account the different masses of these different isotopes. Assume that the solid is 1D (1D quantum harmonic oscillator). Assume that the strength of the returning force k experienced by each atom is the same. What is the difference in the oscillation frequencies of the two different isotopes in the lithium crystal? Write down the total energy stored in the vibrations of each atom of the lithium crystal, assuming that all ^6 Li atoms are in n=2 vibrational mode and all ^7 Li atoms are in n=4 vibrational mode. In the case where the oscilators can occupy any vibrational mode, write down the total energy stored in the vibrations of each atom in the lithium crystal at a temperature T by modifying the Einstein model. Compute the heat capacity of the lithium crystal as a function of T . The exact value can be found on the NIST database \u21a9 Data is collated in the Heat Capacity of the Elements at 25 ^{\\circ} C as published in the CRC handbook of chemistry of physics , but was sourced from wikipedia \u21a9 Verify this explicitly if it is not obvious: noting the result Z_{3D} = Z_{1D}^3 is a key observation \u21a9","title":"1.1: The specific heat of solids I"},{"location":"1-intoduction/1-1-specificheatI/#the-specific-heat-of-solids-i","text":"","title":"The specific heat of solids I"},{"location":"1-intoduction/1-1-specificheatI/#introduction","text":"We embark on our journey by starting at the nexus of the known and unknown, namely around the turn of the nineteenth and twentieth centuries, where the development of \"modern\" physics was being applied to systems which had hitherto be poorly understood. Now this is not to say that nothing was known about the systems, on the contrary: empirical laws had been used to great effect to describe the observable world, but with the increasing sophistication of experimental technique and apparatus, the cracks in certain rules started to appear. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Thermal physics: heat capacity Quantum mechanics: energy spectrum of the harmonic oscillator Statistical physics: the partition function, equipartition theorem Text reference The material covered here is discussed in section(s) \\S 2.1 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"1-intoduction/1-1-specificheatI/#the-dulongpetit-law","text":"Consider the heat capacity of a solid. 1.1 Explain the concept of heat capacity in a manner understandable to someone without a science background. The heat capacity is the a measure of how much heat, or how much energy transfer, is required to change the temperature of a material. By measuring the heat capacity per weight, that is the mass-specific heat capacity , of a range of different elements, the two chemists Pierre Dulong and Alexis Petit observed the value was approximately constant when multiplied by the atomic weight of the element, stating that: c \\times M = constant where c is the specific heat capacity and M is the molar mass of the material. More commonly, one will see the law expressed in terms of the heat capacity C and number of moles n : C/n=\\frac{\\partial Q}{\\partial T} = 3R where R \\approx 8.314~\\mathrm{J K^{-1} mol^{-1}} is the ideal gas constant 1 . In the physics context, it is much more common do talk about the number of atoms N , which transforms the above equation into the Dulong-Peteit Law : C/N= 3 k_\\mathrm{B} But is this an accurate description? Let's have a look. Shown below is a plot of the heat capacity C 2 (in units of R ) as a function of atomic number: Heat capacity of the elements at room temperature as sourced from the CRC handbook of chemistry of physics which is pretty incredible. But a natural question arises: why is this the case?","title":"The Dulong\u2013Petit law"},{"location":"1-intoduction/1-1-specificheatI/#the-boltzmann-model-of-a-solid","text":"It was exactly the question of \"why does the Dulong-Petit law seem to work?\" that motivated Ludwig Boltzmann to use his novel - and at the time completely unaccepted - ideas, notably the existence of atoms and molecules and the mechanics that arises from statistically significant numbers of these atoms and molecules, to model unexplained systems. The insight of Boltzmann was to consider a solid as a collection of constituent particles, but unlike gasses, these particles would be strongly interacting. Explicitly, the idea of atoms interacting with their nearest neighbours through an elastic spring-like potential - an harmonic potential - would allow the system to be modelled with statistical mechanics. Like the case of a gas, energy can be stored in the system in the form of atomic motion, but unlike a gas, the motion of the atoms is constrained. A schematic of the atomic-scale model constructed by Boltzmann in an attempt to explain the Dulong-Petit law Whilst this may seem like a major leap forward, it is worth considering what the explaination for this behaviour had been prior to this proposal: nothing . Then, using the recently minted ideas such as the equipartition theorem , it was clear why the C/N= 3 k_\\mathrm{B} . To see this, recall that for a gas in thermal equilibrium, we have C_V/N = f/2 k_\\mathrm{B} where f is the number of thermodynamic degrees of freedom, or stated another way: each degree of freedom contributes k_\\mathrm{B}/2 to the heat capacity. Immediately, we can see that the Dulong\u2013Petit law is of this form, but suggests that the number of DoF is six, i.e. twice that of an ideal monatomic gas. 1.2 What is the difference between the heat capacities C_V and C_P ? What is the relationship between the two quantities, and what is the implication for the heat capacity of solids? As the heat capacity of an object is defined through \\frac{\\partial Q}{\\partial T} , one must consider the different thermodynamic processes (e.g. isochoric versus isobraic) as the heat supplied to the system will be different (e.g. dQ = dU versus dQ = dU + PdV ). It then follows that we define the heat capacity at constant volume C_V and the heat capacity at constant pressure C_P . Linking the two quantities is Mayer's relation, which states that for an ideal gas: C_P - C_V = nR and more generally C_P - C_V = \\frac{VT\\alpha^2}{\\beta} where \\alpha is the thermal expansion coefficient and \\beta is the isothermal compressibility. This should immediately point to the implication for solids: solids tend to be rather incompressible, which manifests in small values of \\alpha and \\beta , and especially small values of \\alpha^2 , and a negligible difference between C_P and C_V . Hence the usage of C ! The thermal expansion coefficients of various elements, noting that \\beta is of order 10^{-6} \\mathrm{m}^2 \\mathrm{N}^{-1} for squishy things (e.g. soft clay) down to 10^{-10} \\mathrm{m}^2 \\mathrm{N}^{-1} for things that a not squishy (e.g. rocks) Hopefully it not a mystery why there are additional degrees of freedom in a solid as compared to a monatomic gas: remember that Boltzmann's model of a solid is effectively a collection of harmonic oscillators, so not only could energy be stored in the motion of the motion of the constituent atoms, but also their position, storing energy in the \"bonds\" between atoms. Monatomic gas The degrees of freedom for a monatomic gas are listed below: Property Degrees of freedom Momentum p_x , p_y , and p_z The 4 degrees of freedom each contribute k_\\mathrm{B}/2 to the specific heat such that C=3k_\\mathrm{B}/2 . Diatomic gas The degrees of freedom for a diatomic gas (at room temperature) are listed below: Property Degrees of freedom Momentum p_x , p_y , and p_z Rotation Axial and end-over-end The 5 degrees of freedom each contribute k_\\mathrm{B}/2 to the specific heat such that C=5k_\\mathrm{B}/2 . Solid The degrees of freedom for a solid are listed below: Property Degrees of freedom Momentum p_x , p_y , and p_z Position x , y , and z The 6 degrees of freedom each contribute k_\\mathrm{B}/2 to the specific heat such that C=3k_\\mathrm{B} . This result stood as one of the great achievements of statistical mechanics at the time, as up to (and indeed past this point) it was still considered a fringe theory. So, what happened?","title":"The Boltzmann model of a solid"},{"location":"1-intoduction/1-1-specificheatI/#diamond-is-the-worst","text":"but also, diamond is remarkable Whilst most people will be familiar with the cool properties of diamond, some things are best seen. For example, diamond is often quoted as having the greatest thermal conductivity of any material, but what does that actually look like? Well, take a look : If one inspects the plot of heat capacities for the different elements, one can see that there are a few outliers, but it should be made clear that these measurements were taken at room temperature. Above room temperature, there is widespread agreement - even better than room temperature - but if one makes the same measurements at low temperatures, the Dulong-Petit law completely falls apart, with C \\rightarrow 0 as T \\rightarrow 0 . And even at room temperature, some materials do not behave as expected, and in particular: diamond. At room temperature, diamond has a value of C/R \\approx 0.74 , which is much less than 3! The remarkable properties of diamond had long been known, and consequently any theory its salt had to explain why diamond was special. Shown below is a plot of the heat capacity of diamond versus temperature: The heat capacity of diamond as a function of temperature. Data has been sourced from Einstein's original paper of $T>230~\\mathrm{K}$ and from J. E. Desnoyehs & J. A. Morrison for $T < 230~\\mathrm{K}$. Immediately one can notice: C is not a constant Things are good at high temperature Things are bad at low temperature and Boltzmann's theory does not do anything to explain any of this.","title":"Diamond is the worst"},{"location":"1-intoduction/1-1-specificheatI/#the-einstein-model-of-a-solid","text":"It is perhaps unsurprising that a both difficult and well-known problem became the focus of attention for Einstein, someone who even at the very beginning of his career showed remarkable insight into physical systems, often reasoning from observations what must be going on, and constructing a theory to make it all work. Following his work on the photoelectric effect and Brownian motion, he was well placed to tackle the problem of the unexpected behaviour of heat capacity at low-temperatures.","title":"The Einstein model of a solid"},{"location":"1-intoduction/1-1-specificheatI/#the-model","text":"Like Boltzmann's model, Einstein's model was based around atoms in an harmonic potential, but they key - and highly consequential - difference being that each atom is an identical potential, and that oscillation in said potential occurs at a frequency \\omega , later dubbed the Einstein frequency . Basically, he took Boltzmann's model, injected quantum mechanics and asked: what will be the result. It is worth pausing to point out that this was done prior to quantum mechanics having been developed: Einstein's explanation of the photoelectric effect is widely heralded as the starting point of quantum, but it was not until roughly 20 years later that the Schr\u00f6dinger equation was published! So this was a pretty wild assertion. To see the implications, we can make use of our knowledge of statistical physics and the quantum harmonic oscillator: using the energy eigenstates of the system E_n , we can calculate the partition function Z , then the expectation value for the energy \\langle E \\rangle , and ultimately the heat cavity C . 1.3 Beginning with the energy eigenstates of a single one-dimensional harmonic oscillator, show that the heat capacity for a single oscillator is C = k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} In one dimension, the energy eigenstates E_n of a single harmonic oscillator are given by: E_n = \\hbar\\omega(n+1/2) where \\omega is the frequency of the harmonic oscillator. The partition function is then given by: \\begin{aligned} Z = & \\sum_{n\\ge0} \\exp\\left[-\\beta\\hbar\\omega(n+1/2)\\right] \\\\ = & \\frac{\\exp(-\\beta\\hbar\\omega/2)}{1-\\exp(-\\beta\\hbar\\omega)} = \\frac{1}{2\\sinh(\\beta\\hbar\\omega/2)} \\end{aligned} We can then compute the expectation value of the energy \\langle E \\rangle via \\begin{aligned} \\langle E \\rangle = -\\frac{1}{Z}\\frac{\\partial Z}{\\partial \\beta} & = \\frac{\\hbar \\omega}{2}\\coth\\left(\\beta\\hbar\\omega/2\\right) \\\\ & = \\hbar \\omega \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + 1/2\\right) \\end{aligned} where n_\\mathrm{B} is the Bose occupation factor, defined as n_\\mathrm{B}(x) = \\frac{1}{\\exp(x)-1} It then straightforward to extract the heat capacity for a single oscillator through C = \\frac{\\partial \\langle E \\rangle}{\\partial T} = k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} The above result is stated for a one-dimensional harmonic oscillator, but to expend the system three dimensions we need to multiply this result by three 3 which gives the final result C = 3k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} 1.4 Produce a plot the specific heat C versus temperature for realistic values of \\omega , providing your code. Code the produce the plot as requested in shown below, along with the output of said code. Note that # Import all the goodies required for running code in this unit from ssp import * # Define a function to calculate the heat capacity def c_einstein ( T , w ): \"\"\" Calculate the specific heat capacity according to the Einstein model of a solid Input: --- T: Temperature [K] w: Einstein frequency \\omega [rad.s^-1] Returns: --- The heat capacity in units of k_B \"\"\" x = ( hbar * w ) / ( T * kb ) # scale the variable return 3 * x ** 2 * np . exp ( x ) / ( np . exp ( x ) - 1 ) ** 2 # compute the heat capacity # The range of temperatures over which the heat capity will be calculated # Note: overflow errors will occur is the x_min value is too small temp = np . linspace ( 10 , 1000 , 200 ) # The range of Einstein freqeuncies to be computed. For reference, diamond has \\omega \\approx 170 w = np . linspace ( 20 , 200 , 5 ) # Create the plot instance fig , ax = plt . subplots () # Plot and label each frequency for f in w : ax . plot ( temp , c_einstein ( temp , f * 1e12 ), label = f '$\\omega= { f : .0f } $ THz' ) # Make the plot readable ax . set_xlabel ( '$T [K]$' ) ax . set_ylabel ( r '$C/k_B$' ); ax . set_title ( r 'The Einstein model of heat capacity' ) ax . legend () # Save the figure plt . savefig ( '01_Einstein_c.svg' , facecolor = 'white' , transparent = False ) plt . show () # Show the plot A plot of the specific heat as computed from the Einstein model. Note that diamond has a an Einstein frequency of approximately \\omega = 170~\\mathrm{Hz} Looking at the from of C , it is clear: In the high-temperature limit ( k_{\\mathrm{B}} T/\\hbar\\omega \\gg 1 ) we recover the Dulong-Petit law In the low-temperature limit, the heat capacity is exponentially small But what is the physical interpretation of this behaviour? For higher temperatures ( k_{\\mathrm{B}} T/\\hbar\\omega \\gg 1 ), the ability to store energy in harmonic motion is unencumbered, with a decrease in temperature, this ceases to be the case of these degrees of freedom are \"frozen out\". Once the temperature is sufficiently low ( k_{\\mathrm{B}} T/\\hbar\\omega < 1 ), atoms are necessarily in the ground states of the harmonic oscillator; only with sufficient energy ( E = \\hbar\\omega ) can an atom be excited, and with a temperature much less than the energy level spacing, atoms are stuck and thus cannot absorb any energy. It is incredible that Einstein reasoned that this process must be occurring, which prompted him to describe the theory, essentially leading to him inventing the quantisation of energy levels.","title":"The model"},{"location":"1-intoduction/1-1-specificheatI/#coming-up-diamonds","text":"Attempting to explain the heat capacity of diamond had proven the death knell of all theories up to this point, and so it is unsurprising that in Einstein's original paper on the topic cantered around measurements of the heat capacity of diamond, which is shown below: Plot A plot of the molar heat capacity of diamond as a function of temperature. The plot is somewhat diabolical in its omission of labels and units, which should read k_\\mathrm{B}T/\\hbar\\omega and C~[\\mathrm{cal}~\\mathrm{K}^{-1}~\\mathrm{mol}^{-1}] for the x and y axes respectively Data The raw data used to produce the figure of the heat capacity of diamond There are obviously a few discrepancies, but on the whole it looks much better than the Boltzmann model and rightly was seen as a major triumph. But again, the question is why does this happen physically? What is it about diamond that makes is act so strangely? An energy-level diagram for the quantum harmonic oscillator, showing the wavefunctions for the three lowest-energy eigenstates Well if we consider the energy spacing of the harmonic oscillator, \\hbar\\omega , it is related to both the mass ( m ) and the spring constant ( \\kappa ) of the oscillator. For most materials, the Einstein frequency is such that C/N \\approx 3 k_\\mathrm{B} , but diamond has an especially low value of \\omega = \\sqrt{\\kappa/m} , which perhaps is unsurprising given that carbon is light (low m ) and diamond is incredibly hard (large \\kappa ).","title":"Coming up diamonds"},{"location":"1-intoduction/1-1-specificheatI/#conclusions","text":"The law of Dulong\u2013Petit is an observation that all materials have C \\approx 3k_B per atom. The Einstein model describes each atom in a solid as an independent quantum harmonic oscillator with the same eigenfrequency \\omega_0 . At sufficiently low T , the thermal excitations freeze out, resulting in \\langle E \\rangle = \\hbar \\omega_0/2 . The Einstein model correctly predicts that the heat capacity drops to 0 as T\\rightarrow 0 .","title":"Conclusions"},{"location":"1-intoduction/1-1-specificheatI/#exercises","text":"","title":"Exercises"},{"location":"1-intoduction/1-1-specificheatI/#preliminary-provocations","text":"What is the high-temperature heat capacity of an atom in a solid with two momentum and two spatial coordinate degrees of freedom? Sketch the Bose Einstein distribution as a function of \\omega for two different values of T","title":"Preliminary provocations"},{"location":"1-intoduction/1-1-specificheatI/#exercise-1-total-heat-capacity-of-a-diatomic-material","text":"One of the assumptions of the Einstein model states that every atom in a solid oscillates with the same frequency \\omega_0 . However, if the solid contains different types of atoms, it is unreasonable to assume that the atoms oscillate with the same frequency. One example of such a solid is a lithium crystal, which consists of the two stable isotopes ^6 Li (7.5%) and ^7 Li (92.5%) in their natural abundance. Let us extend the Einstein model to take into account the different masses of these different isotopes. Assume that the solid is 1D (1D quantum harmonic oscillator). Assume that the strength of the returning force k experienced by each atom is the same. What is the difference in the oscillation frequencies of the two different isotopes in the lithium crystal? Write down the total energy stored in the vibrations of each atom of the lithium crystal, assuming that all ^6 Li atoms are in n=2 vibrational mode and all ^7 Li atoms are in n=4 vibrational mode. In the case where the oscilators can occupy any vibrational mode, write down the total energy stored in the vibrations of each atom in the lithium crystal at a temperature T by modifying the Einstein model. Compute the heat capacity of the lithium crystal as a function of T . The exact value can be found on the NIST database \u21a9 Data is collated in the Heat Capacity of the Elements at 25 ^{\\circ} C as published in the CRC handbook of chemistry of physics , but was sourced from wikipedia \u21a9 Verify this explicitly if it is not obvious: noting the result Z_{3D} = Z_{1D}^3 is a key observation \u21a9","title":"Exercise 1: Total heat capacity of a diatomic material"},{"location":"1-intoduction/1-2-specificheatII/","text":"The specific heat of solids II \u00b6 Introduction \u00b6 Previously, we saw how an empirical observation describing the behaviour of the specific heat of solids motivated the development of atomic-scale models of solids in order to understand and predict their behaviour. This marked the beginning of using theories which involved the quantisation of certain observables to accurately predict previously unexplained behaviour, but as we shall see, one must continue down the rabbit hole of quantisation in order to better model physical systems. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Wave mechanics: acoustic waves in solids (sound) Mathematics: periodic boundary conditions, spherical coordinates Text reference The material covered here is discussed in section(s) \\S 2.2 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Shortcomings of the Einstein model \u00b6 The Einstein model did much better than the Boltzmann model at explaining the behaviour of solids outside the regime of k_{\\mathrm{B}} T/\\hbar\\omega \\gg 1 ; however, it would turn out that the model routinely underpredicts the heat capacity as T \\rightarrow 0 . This can be seen in Einstein's plot of diamond , but also in other, better behaved materials. For example shown below is a plot of the heat capacity of silver (and diamond), along with a fit of the data using the Einstein model: The heat capacity of silver and diamond as a function of temperature, with the data fitted to the Einstein model with fitting parameter $\\omega$. Indeed, it was known that at low temperatures, the heat capacity displayed cubic behaviour, that is C \\propto T^3 . 2.1 Using the provided data for the heat capacity of silver, verify the cubic behaviour of the heat capacity at low temperatures. Ensure to include you code. # Import the data from the supplied .csv file data = pd . read_csv ( 'Heat_capacity_Ag.csv' ) data = data [ data [ 'T' ] < 25 ] # take only the low-termperature data # Define the function to fit (a cubic) def cubic ( x , a ): return a * x ** 3 # The range of temperatures over which the fit capity will be calculated temp = np . linspace ( 0 , 25 , 100 ) fit = curve_fit ( cubic , data [ 'T' ], data [ 'C' ]) # perform the fit a_ag = fit [ 0 ][ 0 ] # extract the fit parameter # Make the plot fig , ax = plt . subplots () ax . scatter ( data [ 'T' ], data [ 'C' ], label = 'Silver' , color = 'C1' ) ax . plot ( temp , cubic ( temp , a_ag ), label = f 'Cubic fit' , color = 'C1' ) ax . set_xlabel ( '$T$ [K]' ) ax . set_ylabel ( '$C/k_\\mathrm {B} $' ) ax . set_ylim (( 0 , . 3 )) ax . set_xlim (( 0 , 25 )) ax . set_title ( 'Heat capacity of silver at low temperature' ); ax . legend () plt . savefig ( '02_heat_capacity_cubic.svg' , facecolor = 'white' , transparent = False ) plt . show () The heat capacity of silver at low temperature, T<25~\\mathrm{K} , follows well a cubic relationship in temperature. 2.2 How does C predicted by the Einstein model behave at low T ? From the Einstein model, we have C = 3k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} which means that as T \\rightarrow 0 , \\beta \\rightarrow \\infty and thus C \\propto \\beta^2/\\exp(\\beta\\hbar\\omega) , which is exponentially small - and definitely not cubic! The Debye model \u00b6 In the years following the development of Einstein's model, with more people delving into the world of quantised oscillators, people were grappling with the links to other areas of physics. A key insight of Peter Debye was that oscillators in a crystal cannot be thought of as isolated identical systems, but rather as a coupled network of oscillators: recognising that oscillations in solids gives rise to sound waves, these waves should be quantised in the same way that Max Plank had done previously for light. Sound wave refresher A sound wave is a collective motion of atoms through a solid. The displacement \\mathbf{\\delta r} of an atom at position \\mathbf{r} and time t is described by \\mathbf{\\delta r} = \\mathbf{\\delta r}_0 e^{i(\\mathbf{k} \\cdot \\mathbf{r}-\\omega t)}, where \\mathbf{\\delta r}_0 is the amplitude of the wave and \\mathbf{k} = (k_x, k_y, k_z) the wave vector . The wavelength \\lambda is related to the wavevector \\mathbf{k} though \\lambda = 2\\pi/|\\mathbf{k}| . The wave depends on time only through the factor e^{-i\\omega t} . Therefore these waves are normal modes : oscillations of a system in which all parts of the system oscillate with the same frequency and fixed phase relation. In addition to direction of the wave k , each sound wave has another degree of freedom: the direction in which the atoms themselves move or the wave polarization . Per wavevector \\mathbf{k} there are three modes in a 3D solid: two transverse (perpendicular to \\mathbf{k} ) and one longitudinal mode (parallel to \\mathbf{k} ). The space containing all possible values of \\mathbf{k} is called the k -space (also named the reciprocal space ). Debye modelled the oscillation modes of a solid as waves with frequency \\omega , which through the dispersion relation is related to the wavevector k , explicitly \\omega(\\mathbf{k}) = v|\\mathbf{k}| where v is the speed of sound in the material. If we then construct a partition function and from this, compute the expectation value of the energy, we arrive at \\langle E \\rangle = 3 \\sum_\\mathbf{k} \\hbar\\omega(\\mathbf{k}) \\left( n_\\mathrm{B}(\\beta\\hbar\\omega(\\mathbf{k}))+\\frac{1}{2} \\right) which is very similar to the equivalent expression from Einstein's treatment, other than the sum over wavevectors. It is also interesting to note that the oscillation modes also obey Bose statistics - we shall discuss this more later. 2.3 From Where does the factor of 3 in the above expression originate? The factor 3 comes from the three possible normal modes of polarization for each wavevector \\mathbf{k} . To reiterate where we are and where we are going: instead of having 3N oscillators with the same frequency \\omega_0 , we now have 3N possible vibrational modes with frequencies depending on \\textbf{k} through the dispersion relation \\omega(\\mathbf{k}) = v_s|\\mathbf{k}| . But the question is now, how do we evaluate the above expression? And there are a few natural questions that arise from what we have done thus far: Don't normal modes depend on the material's shape. What impact does this have on the heat capacity? Which values of \\mathbf{k} are possible? and if all \\mathbf{k} are possible, won't E be infinite? Detour: periodic boundary conditions \u00b6 It is expected that you will have seen periodic boundary conditions in various contexts, mostly likely in studies of differential equations of electromagnetism, but during this course we shall use them regularly. Importantly, we must first establish why we would impose periodic boundary conditions on a system which is not periodic: solids have boundaries! An intuition can be cultivated by considering that C is a macroscopic property : it should not depend on the material's shape and should only be proportional to its volume. Therefore, we can consider making measurements of quantities of interest far from any boundary, and with this, we are free to choose the geometry of material and of course, we pick things that make our life easier! Consider a box of dimension L \\times L \\times L , which then has volume V = L^3 with periodic boundary conditions. These conditions enforce that the atomic displacement \\mathbf{\\delta r} is periodic inside the material. If we consider a translation by L in the x -direction \\mathbf{\\delta r}(\\mathbf{r} + L\\mathbf{\\hat{x}}) = \\mathbf{\\delta r}(\\mathbf{r}) and a wave in this sample \\delta \\mathbf{r_0} e^{i(\\mathbf{k}\\cdot\\mathbf{r}-\\omega t)} must satisfy the above equation, implying \\delta\\mathbf{r_0} e^{i(\\mathbf{k} \\cdot \\mathbf{r}+k_xL-\\omega t)} = \\mathbf{\\delta r}_0 e^{i(\\mathbf{k} \\cdot \\mathbf{r}-\\omega t)} and ultimately e^{i k_x L} = e^{i 0} = 1. This then restricts the possible values of k_x = n_x \\frac{2 \\pi}{L} , for n_x \\in \\mathbb{Z} . Given the same condition holds for the y - and z -direction, the allowed values for \\mathbf{k} are given by \\mathbf{k} = \\frac{2\\pi}{L}(n_x, n_y, n_z), \\quad \\{n_x, n_y, n_z\\} \\in \\mathbb{Z}. A key observation here is that the imposition of periodic boundary conditions results in a discretisation of k -space, where the allowed values of \\mathbf{k} form a regular grid in k -space, and moreover, per volume \\left(\\frac{2\\pi}{L}\\right)^3 in k -space there is exactly one allowed \\mathbf{k} . In the standard way, if we are required to sum over all possible values of \\mathbf{k} and the volume of L is sufficiently large - that is, the volume per allowed mode becomes smaller - we can replace the sum over \\mathbf{k} with an integral \\sum_\\mathbf{k} \\approx \\frac{L^3}{(2\\pi)^3}\\int \\textrm{d} \\textbf{k} Integral over k -space We shall use this result very regularly. The conversion from a sum over the discrete grid of k -space states to a volume integral provides an effective way to count all the possible waves. The density of states \u00b6 Armed with a shiny new tool (wrapping our sample into a hypertorus to make the maths nicer), we can return to evaluating the expectation value \\langle E \\rangle : \\begin{aligned} \\langle E \\rangle & = 3 \\sum_\\mathbf{k} \\hbar\\omega(\\mathbf{k}) \\left( n_\\mathrm{B}(\\beta\\hbar\\omega(\\mathbf{k}))+\\frac{1}{2} \\right) \\\\ & = 3 \\frac{L^3}{(2\\pi)^3}\\int \\mathrm{d} \\mathbf{k} \\hbar\\omega(\\mathbf{k}) \\left( n_\\mathrm{B}(\\beta\\hbar\\omega(\\mathbf{k}))+\\frac{1}{2} \\right) \\end{aligned} Note that in the above expression, the integrand depends only on \\omega(\\mathbf{k}) \\propto |\\mathbf{k}| , and it is therefore natural to move to spherical coordinates, where the 3-dimensional integral can be collapsed to one dimension since: \\int \\mathrm{d} \\mathbf{k} \\rightarrow 4\\pi\\int_0^\\infty k^2 \\mathrm{d} \\mathbf{k} Spherical coordinate transformation As a refresher, using the coordinate system defined by x = r \\sin(\\theta)\\cos(\\varphi) , y = r \\sin(\\theta)\\sin(\\varphi) , and z = r\\cos(\\theta) , the transformation of the integral can be performed via \\int f(\\mathbf{r}) \\textrm{d} \\textbf{r} \\to \\int\\limits_0^{2\\pi}\\int\\limits_0^{\\pi} \\int\\limits_0^\\infty f(r, \\theta, \\varphi) ~ r^2 \\sin(\\theta) \\textrm{d}r \\textrm{d}\\theta \\textrm{d}\\varphi Performing the change of variables, we obtain the expression for the total energy in spherical coordinates: \\langle E \\rangle = \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ g(\\omega) (\\hbar\\omega) \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + \\frac{1}{2} \\right) where we have introduced g(\\omega) , the density of states g(\\omega) = L^3\\left[\\frac{12\\pi\\omega^2}{(2\\pi)^3 v_s^3} \\right] = n\\left[\\frac{12\\pi\\omega^2}{(2\\pi)^3 n v_s^3} \\right] = N \\frac{9\\omega^2}{\\omega_d^3} and \\omega_d^3 is the Debye frequency \\omega_d^3 = 6\\pi^2 n v_s^3 The (\\hbar\\omega) \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + \\frac{1}{2} \\right) term describes the average energy of an oscillation with frequency \\omega , and g(\\omega) describes the number of modes at the frequency \\omega The density of states Technically, the density of states total is related to number of oscillation modes between frequencies \\omega and \\omega + \\mathrm{d} \\omega via g(\\omega)\\mathrm{d} \\omega It can be convenient to express the density of states in the form g(\\omega) = 3 \\left(\\frac{L}{2\\pi}\\right)^3 \\frac{4\\pi \\omega^2}{v_s^3} which allows for us to see g(\\omega) in terms of its constituent components: 3 comes from the number of possible polarizations in 3D (two transversal, one longitudinal). (\\frac{L}{2\\pi})^3 is the density of \\textbf{k} points in k -space. 4\\pi is the area of a unit sphere. \\omega^2 is due to the area of a sphere in k -space being proportional to its squared radius k^2 and by having a linear dispersion relation \\omega = v_sk . v_s^{-3} is from the linear dispersion relation \\omega = v_sk . So in our case, due to the spherical symmetry, g(\\omega)\\textrm{d} \\omega can be obtained by calculating the density of states of a volume element dV = 4\\pi k^2 dk in k -space and substituting the dispersion relation \\omega(k) . In terms of the calculation of \\langle E \\rangle , we multiply the number of modes g(\\omega) by the average energy of a single mode at a given frequency \\omega and integrate over all frequencies. Low-temperature behaviour \u00b6 In order to calculate a value of C , we must compute C = \\partial \\langle E \\rangle/\\partial T , which means actually computing \\langle E \\rangle . From above we have \\langle E \\rangle = \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ g(\\omega) (\\hbar\\omega) \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + \\frac{1}{2} \\right) which we are going to separate into two components: \\begin{aligned} \\langle E \\rangle & = \\frac{9N\\hbar}{\\omega_d^3} \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ \\frac{\\omega^3}{\\exp(\\beta\\hbar\\omega)-1} + \\frac{9N\\hbar}{\\omega_d^3} \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ \\frac{\\omega^3}{2} \\\\ & = \\frac{9N\\hbar}{\\omega_d^3} \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ \\frac{\\omega^3}{\\exp(\\beta\\hbar\\omega)-1} + \\textrm{ something independent of } T \\end{aligned} The independent component is the zero-point energy E_{ZP} of the vibrational modes, which despite diverging towards infinity, does not contribute to C . One can then make the substitution x =\\beta\\hbar\\omega to transform the above equation into something a little more palatable: \\langle E \\rangle = \\frac{9N\\hbar}{\\omega_d^3 (\\beta\\hbar)^4} \\int\\limits_0^{\\infty}\\mathrm{d} x ~ \\frac{x^3}{\\exp(x)-1} + E_{ZP} which evaluates to \\langle E \\rangle = 9N\\frac{\\left(k_\\mathrm{B} T\\right)^4}{(\\hbar \\omega_d)^3}\\frac{\\pi^4}{15} + E_{ZP} It should not be conspicuous that this result looks similar to Plank's result that E \\propto T^4 for photons. The above result also yields the result that C = \\frac{\\partial \\langle E\\rangle}{\\partial T} = N k_\\mathrm{B} \\frac{(k_\\mathrm{B} T)}{(\\hbar\\omega_d)^3}\\frac{12\\pi^4}{5} \\propto T^3 which produces the desired T^3 dependence, but critically, and unlike the result of Einstein, does not have any free parameters rather only requiring knowledge material density and the sound velocity. 2.4 What is the physical reason for the difference in the low-temperature behaviour of C for the Einstein and Debye models? In the Einstein model, once the temperature was below the energy associated with the oscillator, there was no capacity for the heat to absorb heat. In contrast, in the Debye model, there exist vibrational modes of the solid do have the capacity to absorb heat. Debye's interpolation (the duct-tape solution) \u00b6 Debye successfully constructed a model which quantised the vibrational modes of solids to accurately predict the low-temperature behaviour of the heat capacity. Unfortunately, the model produces a T^3 dependence for all T , not just low temperature and thus does not recover the Dulong-Petit law at high temperature. Debye understood the problem arose from allowing an infinite number of modes, with an implication that there are more modes of oscillation than atoms in the system. The \"quick fix\" for this - which will be revisited with rigour later in the course - to ensure that there are only as many oscillation modes as there are degrees of freedom is to introduce a cutoff frequency \\omega_{\\textrm{cutoff}} such that the total number of oscillation modes equates to the 3N , the number of normal modes in an 3-dimension material. This manifests in the following way: 3N = \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega) which leads to an altered expression for \\langle E \\rangle : \\langle E \\rangle = \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega)~\\hbar\\omega ~ n_{\\textrm{B}}(\\beta\\hbar\\omega) + E_{ZP} 2.5 Verify that the expected high-temperature behaviour for C is recovered when using Debye's interpolation For high temperature: n_{\\textrm{B}}(\\beta\\hbar\\omega) = \\frac{1}{\\exp(\\beta\\hbar\\omega)-1} \\to \\frac{k_{\\textrm{B}}T}{\\hbar\\omega} which when put into the expression above (ignoring the zero-point energy) \\langle E \\rangle = k_{\\textrm{B}}T \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega) which by design returns 3 k_{\\textrm{B}} T N , which is exactly the Dulong-Petit law. 2.6 Explicitly evaluate the cutoff frequency, and express the solution in terms of the Debye frequency An educated guess would probably land you in the correct spot: given the arbitrary definition of the Debye frequency, it is likely that \\omega_{\\textrm{cutoff}} = \\omega_d . Let's have a look: 3N = \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega) = 9N \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ \\frac{\\omega^2}{\\omega_d^3} = 3N \\frac{\\omega_{\\textrm{cutoff}}^3}{\\omega_d^3} That is, \\omega_{\\textrm{cutoff}} = \\omega_d . The question that one must ask: was it worth it? Well, let's look at the Debye model applied to the silver data as shown at the beginning of this section: The heat capacity of silver with fits to both the Einstein and Debye models of solids. Conclusions \u00b6 The Debye model assumes that atoms in materials move in a collective fashion, described by quantized normal modes with a dispersion relation \\omega = v_s|\\mathbf{k}| . The oscillation modes have a constant density of (L/2\\pi)^3 in the reciprocal / k -space. The total energy and heat capacity are obtained by integrating the contribution of the individual modes over k -space. The density of states g(\\omega) is the number of states per frequency. With a dispersion relation \u03c9 = v_s|\\mathbf{k}| , g(\\omega) is proportional to \\omega^2 for a 3D bosonic system. At low temperatures the heat capacity due to oscillations is proportional to T^3 . Modes of oscillation exist only up until the Debye frequency \\omega_D , after which there are no modes in the system. Exercises \u00b6 Preliminary provocations \u00b6 Why are there only 3 polarizations when there are 6 degrees of freedom in three-dimensions for an oscillator? Express the two-dimensional integral \\int\\mathrm{d}k_x\\mathrm{d}k_y in terms of polar coordinates. You can assume rotational symmetry. The Einstein model has a material-dependent frequency \\omega_0 = k_\\mathrm{B} T_E/\\hbar of the quantum harmonic oscillators as a free fitting parameter. What is the material-dependent parameter that plays a similar role in the Debye model? Derive an expression for the shortest possible wavelength in the Debye model it in terms of the interatomic distance a . Hint: assume that the number of atoms is given by N=V/a^3 . Discuss if the answer is reasonable. Exercise 1: Debye model - concepts \u00b6 Consider the probability to find an atom of a 1D solid that originally had a position x at a displacement \\delta x shown below: Describe which k -states are occupied. Explain your answer. Describe the concept of k -space. What momenta are allowed in a 2D system with dimensions L\\times L ? Explain the concept of density of states. Calculate the density of states g(\\omega) for oscillations of a 3D, 2D and 1D solid with linear dispersion \\omega=v_s|\\mathbf{k}| . Exercise 2: Debye model in 2D \u00b6 State the assumptions of the Debye model. Determine the energy of a two-dimensional solid as a function of T using the Debye approximation. You do not have to solve the integral. Calculate the heat capacity in the high T limit. At low T , show that C_V=KT^{n} . Find n . Express K as an indefinite integral (similarly to what done during the lecture). Exercise 3: Different oscillation modes \u00b6 (adapted from exercise 2.6a of The Oxford Solid State Basics ) During the lecture we derived the low-temperature heat capacity assuming that all the modes of oscillation have the same sound velocity v . In reality, the longitudinal and transverse modes have different sound velocities (see Wikipedia for an illustration of different sound wave types). Assume that there are two types of excitations: One longitudinal mode with \\omega = v_\\parallel |k| Two transverse modes with \\omega = v_\\bot |k| Write down the total energy of oscillations in this material. Verify that at high T you reproduce the Dulong-Petit law. Compute the behaviour of heat capacity at low T . Exercise 4: Anisotropic sound velocities \u00b6 (adapted from exercise 2.6b of The Oxford Solid State Basics ) Suppose now that the velocity is anisotropic ( v_x \\neq v_y \\neq v_z ) and \\omega = \\sqrt{v_x^2 k_x^2 + v_y^2 k_y^2 + v_z^2 k_z^2} . How does this change the Debye result for the heat capacity? Hint Write down the total energy as an integral over k , then change the integration variables so that the spherical symmetry of the integrand is restored.","title":"1.2: The specific heat of solids II"},{"location":"1-intoduction/1-2-specificheatII/#the-specific-heat-of-solids-ii","text":"","title":"The specific heat of solids II"},{"location":"1-intoduction/1-2-specificheatII/#introduction","text":"Previously, we saw how an empirical observation describing the behaviour of the specific heat of solids motivated the development of atomic-scale models of solids in order to understand and predict their behaviour. This marked the beginning of using theories which involved the quantisation of certain observables to accurately predict previously unexplained behaviour, but as we shall see, one must continue down the rabbit hole of quantisation in order to better model physical systems. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Wave mechanics: acoustic waves in solids (sound) Mathematics: periodic boundary conditions, spherical coordinates Text reference The material covered here is discussed in section(s) \\S 2.2 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"1-intoduction/1-2-specificheatII/#shortcomings-of-the-einstein-model","text":"The Einstein model did much better than the Boltzmann model at explaining the behaviour of solids outside the regime of k_{\\mathrm{B}} T/\\hbar\\omega \\gg 1 ; however, it would turn out that the model routinely underpredicts the heat capacity as T \\rightarrow 0 . This can be seen in Einstein's plot of diamond , but also in other, better behaved materials. For example shown below is a plot of the heat capacity of silver (and diamond), along with a fit of the data using the Einstein model: The heat capacity of silver and diamond as a function of temperature, with the data fitted to the Einstein model with fitting parameter $\\omega$. Indeed, it was known that at low temperatures, the heat capacity displayed cubic behaviour, that is C \\propto T^3 . 2.1 Using the provided data for the heat capacity of silver, verify the cubic behaviour of the heat capacity at low temperatures. Ensure to include you code. # Import the data from the supplied .csv file data = pd . read_csv ( 'Heat_capacity_Ag.csv' ) data = data [ data [ 'T' ] < 25 ] # take only the low-termperature data # Define the function to fit (a cubic) def cubic ( x , a ): return a * x ** 3 # The range of temperatures over which the fit capity will be calculated temp = np . linspace ( 0 , 25 , 100 ) fit = curve_fit ( cubic , data [ 'T' ], data [ 'C' ]) # perform the fit a_ag = fit [ 0 ][ 0 ] # extract the fit parameter # Make the plot fig , ax = plt . subplots () ax . scatter ( data [ 'T' ], data [ 'C' ], label = 'Silver' , color = 'C1' ) ax . plot ( temp , cubic ( temp , a_ag ), label = f 'Cubic fit' , color = 'C1' ) ax . set_xlabel ( '$T$ [K]' ) ax . set_ylabel ( '$C/k_\\mathrm {B} $' ) ax . set_ylim (( 0 , . 3 )) ax . set_xlim (( 0 , 25 )) ax . set_title ( 'Heat capacity of silver at low temperature' ); ax . legend () plt . savefig ( '02_heat_capacity_cubic.svg' , facecolor = 'white' , transparent = False ) plt . show () The heat capacity of silver at low temperature, T<25~\\mathrm{K} , follows well a cubic relationship in temperature. 2.2 How does C predicted by the Einstein model behave at low T ? From the Einstein model, we have C = 3k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} which means that as T \\rightarrow 0 , \\beta \\rightarrow \\infty and thus C \\propto \\beta^2/\\exp(\\beta\\hbar\\omega) , which is exponentially small - and definitely not cubic!","title":"Shortcomings of the Einstein model"},{"location":"1-intoduction/1-2-specificheatII/#the-debye-model","text":"In the years following the development of Einstein's model, with more people delving into the world of quantised oscillators, people were grappling with the links to other areas of physics. A key insight of Peter Debye was that oscillators in a crystal cannot be thought of as isolated identical systems, but rather as a coupled network of oscillators: recognising that oscillations in solids gives rise to sound waves, these waves should be quantised in the same way that Max Plank had done previously for light. Sound wave refresher A sound wave is a collective motion of atoms through a solid. The displacement \\mathbf{\\delta r} of an atom at position \\mathbf{r} and time t is described by \\mathbf{\\delta r} = \\mathbf{\\delta r}_0 e^{i(\\mathbf{k} \\cdot \\mathbf{r}-\\omega t)}, where \\mathbf{\\delta r}_0 is the amplitude of the wave and \\mathbf{k} = (k_x, k_y, k_z) the wave vector . The wavelength \\lambda is related to the wavevector \\mathbf{k} though \\lambda = 2\\pi/|\\mathbf{k}| . The wave depends on time only through the factor e^{-i\\omega t} . Therefore these waves are normal modes : oscillations of a system in which all parts of the system oscillate with the same frequency and fixed phase relation. In addition to direction of the wave k , each sound wave has another degree of freedom: the direction in which the atoms themselves move or the wave polarization . Per wavevector \\mathbf{k} there are three modes in a 3D solid: two transverse (perpendicular to \\mathbf{k} ) and one longitudinal mode (parallel to \\mathbf{k} ). The space containing all possible values of \\mathbf{k} is called the k -space (also named the reciprocal space ). Debye modelled the oscillation modes of a solid as waves with frequency \\omega , which through the dispersion relation is related to the wavevector k , explicitly \\omega(\\mathbf{k}) = v|\\mathbf{k}| where v is the speed of sound in the material. If we then construct a partition function and from this, compute the expectation value of the energy, we arrive at \\langle E \\rangle = 3 \\sum_\\mathbf{k} \\hbar\\omega(\\mathbf{k}) \\left( n_\\mathrm{B}(\\beta\\hbar\\omega(\\mathbf{k}))+\\frac{1}{2} \\right) which is very similar to the equivalent expression from Einstein's treatment, other than the sum over wavevectors. It is also interesting to note that the oscillation modes also obey Bose statistics - we shall discuss this more later. 2.3 From Where does the factor of 3 in the above expression originate? The factor 3 comes from the three possible normal modes of polarization for each wavevector \\mathbf{k} . To reiterate where we are and where we are going: instead of having 3N oscillators with the same frequency \\omega_0 , we now have 3N possible vibrational modes with frequencies depending on \\textbf{k} through the dispersion relation \\omega(\\mathbf{k}) = v_s|\\mathbf{k}| . But the question is now, how do we evaluate the above expression? And there are a few natural questions that arise from what we have done thus far: Don't normal modes depend on the material's shape. What impact does this have on the heat capacity? Which values of \\mathbf{k} are possible? and if all \\mathbf{k} are possible, won't E be infinite?","title":"The Debye model"},{"location":"1-intoduction/1-2-specificheatII/#detour-periodic-boundary-conditions","text":"It is expected that you will have seen periodic boundary conditions in various contexts, mostly likely in studies of differential equations of electromagnetism, but during this course we shall use them regularly. Importantly, we must first establish why we would impose periodic boundary conditions on a system which is not periodic: solids have boundaries! An intuition can be cultivated by considering that C is a macroscopic property : it should not depend on the material's shape and should only be proportional to its volume. Therefore, we can consider making measurements of quantities of interest far from any boundary, and with this, we are free to choose the geometry of material and of course, we pick things that make our life easier! Consider a box of dimension L \\times L \\times L , which then has volume V = L^3 with periodic boundary conditions. These conditions enforce that the atomic displacement \\mathbf{\\delta r} is periodic inside the material. If we consider a translation by L in the x -direction \\mathbf{\\delta r}(\\mathbf{r} + L\\mathbf{\\hat{x}}) = \\mathbf{\\delta r}(\\mathbf{r}) and a wave in this sample \\delta \\mathbf{r_0} e^{i(\\mathbf{k}\\cdot\\mathbf{r}-\\omega t)} must satisfy the above equation, implying \\delta\\mathbf{r_0} e^{i(\\mathbf{k} \\cdot \\mathbf{r}+k_xL-\\omega t)} = \\mathbf{\\delta r}_0 e^{i(\\mathbf{k} \\cdot \\mathbf{r}-\\omega t)} and ultimately e^{i k_x L} = e^{i 0} = 1. This then restricts the possible values of k_x = n_x \\frac{2 \\pi}{L} , for n_x \\in \\mathbb{Z} . Given the same condition holds for the y - and z -direction, the allowed values for \\mathbf{k} are given by \\mathbf{k} = \\frac{2\\pi}{L}(n_x, n_y, n_z), \\quad \\{n_x, n_y, n_z\\} \\in \\mathbb{Z}. A key observation here is that the imposition of periodic boundary conditions results in a discretisation of k -space, where the allowed values of \\mathbf{k} form a regular grid in k -space, and moreover, per volume \\left(\\frac{2\\pi}{L}\\right)^3 in k -space there is exactly one allowed \\mathbf{k} . In the standard way, if we are required to sum over all possible values of \\mathbf{k} and the volume of L is sufficiently large - that is, the volume per allowed mode becomes smaller - we can replace the sum over \\mathbf{k} with an integral \\sum_\\mathbf{k} \\approx \\frac{L^3}{(2\\pi)^3}\\int \\textrm{d} \\textbf{k} Integral over k -space We shall use this result very regularly. The conversion from a sum over the discrete grid of k -space states to a volume integral provides an effective way to count all the possible waves.","title":"Detour: periodic boundary conditions"},{"location":"1-intoduction/1-2-specificheatII/#the-density-of-states","text":"Armed with a shiny new tool (wrapping our sample into a hypertorus to make the maths nicer), we can return to evaluating the expectation value \\langle E \\rangle : \\begin{aligned} \\langle E \\rangle & = 3 \\sum_\\mathbf{k} \\hbar\\omega(\\mathbf{k}) \\left( n_\\mathrm{B}(\\beta\\hbar\\omega(\\mathbf{k}))+\\frac{1}{2} \\right) \\\\ & = 3 \\frac{L^3}{(2\\pi)^3}\\int \\mathrm{d} \\mathbf{k} \\hbar\\omega(\\mathbf{k}) \\left( n_\\mathrm{B}(\\beta\\hbar\\omega(\\mathbf{k}))+\\frac{1}{2} \\right) \\end{aligned} Note that in the above expression, the integrand depends only on \\omega(\\mathbf{k}) \\propto |\\mathbf{k}| , and it is therefore natural to move to spherical coordinates, where the 3-dimensional integral can be collapsed to one dimension since: \\int \\mathrm{d} \\mathbf{k} \\rightarrow 4\\pi\\int_0^\\infty k^2 \\mathrm{d} \\mathbf{k} Spherical coordinate transformation As a refresher, using the coordinate system defined by x = r \\sin(\\theta)\\cos(\\varphi) , y = r \\sin(\\theta)\\sin(\\varphi) , and z = r\\cos(\\theta) , the transformation of the integral can be performed via \\int f(\\mathbf{r}) \\textrm{d} \\textbf{r} \\to \\int\\limits_0^{2\\pi}\\int\\limits_0^{\\pi} \\int\\limits_0^\\infty f(r, \\theta, \\varphi) ~ r^2 \\sin(\\theta) \\textrm{d}r \\textrm{d}\\theta \\textrm{d}\\varphi Performing the change of variables, we obtain the expression for the total energy in spherical coordinates: \\langle E \\rangle = \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ g(\\omega) (\\hbar\\omega) \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + \\frac{1}{2} \\right) where we have introduced g(\\omega) , the density of states g(\\omega) = L^3\\left[\\frac{12\\pi\\omega^2}{(2\\pi)^3 v_s^3} \\right] = n\\left[\\frac{12\\pi\\omega^2}{(2\\pi)^3 n v_s^3} \\right] = N \\frac{9\\omega^2}{\\omega_d^3} and \\omega_d^3 is the Debye frequency \\omega_d^3 = 6\\pi^2 n v_s^3 The (\\hbar\\omega) \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + \\frac{1}{2} \\right) term describes the average energy of an oscillation with frequency \\omega , and g(\\omega) describes the number of modes at the frequency \\omega The density of states Technically, the density of states total is related to number of oscillation modes between frequencies \\omega and \\omega + \\mathrm{d} \\omega via g(\\omega)\\mathrm{d} \\omega It can be convenient to express the density of states in the form g(\\omega) = 3 \\left(\\frac{L}{2\\pi}\\right)^3 \\frac{4\\pi \\omega^2}{v_s^3} which allows for us to see g(\\omega) in terms of its constituent components: 3 comes from the number of possible polarizations in 3D (two transversal, one longitudinal). (\\frac{L}{2\\pi})^3 is the density of \\textbf{k} points in k -space. 4\\pi is the area of a unit sphere. \\omega^2 is due to the area of a sphere in k -space being proportional to its squared radius k^2 and by having a linear dispersion relation \\omega = v_sk . v_s^{-3} is from the linear dispersion relation \\omega = v_sk . So in our case, due to the spherical symmetry, g(\\omega)\\textrm{d} \\omega can be obtained by calculating the density of states of a volume element dV = 4\\pi k^2 dk in k -space and substituting the dispersion relation \\omega(k) . In terms of the calculation of \\langle E \\rangle , we multiply the number of modes g(\\omega) by the average energy of a single mode at a given frequency \\omega and integrate over all frequencies.","title":"The density of states"},{"location":"1-intoduction/1-2-specificheatII/#low-temperature-behaviour","text":"In order to calculate a value of C , we must compute C = \\partial \\langle E \\rangle/\\partial T , which means actually computing \\langle E \\rangle . From above we have \\langle E \\rangle = \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ g(\\omega) (\\hbar\\omega) \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + \\frac{1}{2} \\right) which we are going to separate into two components: \\begin{aligned} \\langle E \\rangle & = \\frac{9N\\hbar}{\\omega_d^3} \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ \\frac{\\omega^3}{\\exp(\\beta\\hbar\\omega)-1} + \\frac{9N\\hbar}{\\omega_d^3} \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ \\frac{\\omega^3}{2} \\\\ & = \\frac{9N\\hbar}{\\omega_d^3} \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ \\frac{\\omega^3}{\\exp(\\beta\\hbar\\omega)-1} + \\textrm{ something independent of } T \\end{aligned} The independent component is the zero-point energy E_{ZP} of the vibrational modes, which despite diverging towards infinity, does not contribute to C . One can then make the substitution x =\\beta\\hbar\\omega to transform the above equation into something a little more palatable: \\langle E \\rangle = \\frac{9N\\hbar}{\\omega_d^3 (\\beta\\hbar)^4} \\int\\limits_0^{\\infty}\\mathrm{d} x ~ \\frac{x^3}{\\exp(x)-1} + E_{ZP} which evaluates to \\langle E \\rangle = 9N\\frac{\\left(k_\\mathrm{B} T\\right)^4}{(\\hbar \\omega_d)^3}\\frac{\\pi^4}{15} + E_{ZP} It should not be conspicuous that this result looks similar to Plank's result that E \\propto T^4 for photons. The above result also yields the result that C = \\frac{\\partial \\langle E\\rangle}{\\partial T} = N k_\\mathrm{B} \\frac{(k_\\mathrm{B} T)}{(\\hbar\\omega_d)^3}\\frac{12\\pi^4}{5} \\propto T^3 which produces the desired T^3 dependence, but critically, and unlike the result of Einstein, does not have any free parameters rather only requiring knowledge material density and the sound velocity. 2.4 What is the physical reason for the difference in the low-temperature behaviour of C for the Einstein and Debye models? In the Einstein model, once the temperature was below the energy associated with the oscillator, there was no capacity for the heat to absorb heat. In contrast, in the Debye model, there exist vibrational modes of the solid do have the capacity to absorb heat.","title":"Low-temperature behaviour"},{"location":"1-intoduction/1-2-specificheatII/#debyes-interpolation-the-duct-tape-solution","text":"Debye successfully constructed a model which quantised the vibrational modes of solids to accurately predict the low-temperature behaviour of the heat capacity. Unfortunately, the model produces a T^3 dependence for all T , not just low temperature and thus does not recover the Dulong-Petit law at high temperature. Debye understood the problem arose from allowing an infinite number of modes, with an implication that there are more modes of oscillation than atoms in the system. The \"quick fix\" for this - which will be revisited with rigour later in the course - to ensure that there are only as many oscillation modes as there are degrees of freedom is to introduce a cutoff frequency \\omega_{\\textrm{cutoff}} such that the total number of oscillation modes equates to the 3N , the number of normal modes in an 3-dimension material. This manifests in the following way: 3N = \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega) which leads to an altered expression for \\langle E \\rangle : \\langle E \\rangle = \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega)~\\hbar\\omega ~ n_{\\textrm{B}}(\\beta\\hbar\\omega) + E_{ZP} 2.5 Verify that the expected high-temperature behaviour for C is recovered when using Debye's interpolation For high temperature: n_{\\textrm{B}}(\\beta\\hbar\\omega) = \\frac{1}{\\exp(\\beta\\hbar\\omega)-1} \\to \\frac{k_{\\textrm{B}}T}{\\hbar\\omega} which when put into the expression above (ignoring the zero-point energy) \\langle E \\rangle = k_{\\textrm{B}}T \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega) which by design returns 3 k_{\\textrm{B}} T N , which is exactly the Dulong-Petit law. 2.6 Explicitly evaluate the cutoff frequency, and express the solution in terms of the Debye frequency An educated guess would probably land you in the correct spot: given the arbitrary definition of the Debye frequency, it is likely that \\omega_{\\textrm{cutoff}} = \\omega_d . Let's have a look: 3N = \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega) = 9N \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ \\frac{\\omega^2}{\\omega_d^3} = 3N \\frac{\\omega_{\\textrm{cutoff}}^3}{\\omega_d^3} That is, \\omega_{\\textrm{cutoff}} = \\omega_d . The question that one must ask: was it worth it? Well, let's look at the Debye model applied to the silver data as shown at the beginning of this section: The heat capacity of silver with fits to both the Einstein and Debye models of solids.","title":"Debye's interpolation (the duct-tape solution)"},{"location":"1-intoduction/1-2-specificheatII/#conclusions","text":"The Debye model assumes that atoms in materials move in a collective fashion, described by quantized normal modes with a dispersion relation \\omega = v_s|\\mathbf{k}| . The oscillation modes have a constant density of (L/2\\pi)^3 in the reciprocal / k -space. The total energy and heat capacity are obtained by integrating the contribution of the individual modes over k -space. The density of states g(\\omega) is the number of states per frequency. With a dispersion relation \u03c9 = v_s|\\mathbf{k}| , g(\\omega) is proportional to \\omega^2 for a 3D bosonic system. At low temperatures the heat capacity due to oscillations is proportional to T^3 . Modes of oscillation exist only up until the Debye frequency \\omega_D , after which there are no modes in the system.","title":"Conclusions"},{"location":"1-intoduction/1-2-specificheatII/#exercises","text":"","title":"Exercises"},{"location":"1-intoduction/1-2-specificheatII/#preliminary-provocations","text":"Why are there only 3 polarizations when there are 6 degrees of freedom in three-dimensions for an oscillator? Express the two-dimensional integral \\int\\mathrm{d}k_x\\mathrm{d}k_y in terms of polar coordinates. You can assume rotational symmetry. The Einstein model has a material-dependent frequency \\omega_0 = k_\\mathrm{B} T_E/\\hbar of the quantum harmonic oscillators as a free fitting parameter. What is the material-dependent parameter that plays a similar role in the Debye model? Derive an expression for the shortest possible wavelength in the Debye model it in terms of the interatomic distance a . Hint: assume that the number of atoms is given by N=V/a^3 . Discuss if the answer is reasonable.","title":"Preliminary provocations"},{"location":"1-intoduction/1-2-specificheatII/#exercise-1-debye-model-concepts","text":"Consider the probability to find an atom of a 1D solid that originally had a position x at a displacement \\delta x shown below: Describe which k -states are occupied. Explain your answer. Describe the concept of k -space. What momenta are allowed in a 2D system with dimensions L\\times L ? Explain the concept of density of states. Calculate the density of states g(\\omega) for oscillations of a 3D, 2D and 1D solid with linear dispersion \\omega=v_s|\\mathbf{k}| .","title":"Exercise 1: Debye model - concepts"},{"location":"1-intoduction/1-2-specificheatII/#exercise-2-debye-model-in-2d","text":"State the assumptions of the Debye model. Determine the energy of a two-dimensional solid as a function of T using the Debye approximation. You do not have to solve the integral. Calculate the heat capacity in the high T limit. At low T , show that C_V=KT^{n} . Find n . Express K as an indefinite integral (similarly to what done during the lecture).","title":"Exercise 2: Debye model in 2D"},{"location":"1-intoduction/1-2-specificheatII/#exercise-3-different-oscillation-modes","text":"(adapted from exercise 2.6a of The Oxford Solid State Basics ) During the lecture we derived the low-temperature heat capacity assuming that all the modes of oscillation have the same sound velocity v . In reality, the longitudinal and transverse modes have different sound velocities (see Wikipedia for an illustration of different sound wave types). Assume that there are two types of excitations: One longitudinal mode with \\omega = v_\\parallel |k| Two transverse modes with \\omega = v_\\bot |k| Write down the total energy of oscillations in this material. Verify that at high T you reproduce the Dulong-Petit law. Compute the behaviour of heat capacity at low T .","title":"Exercise 3: Different oscillation modes"},{"location":"1-intoduction/1-2-specificheatII/#exercise-4-anisotropic-sound-velocities","text":"(adapted from exercise 2.6b of The Oxford Solid State Basics ) Suppose now that the velocity is anisotropic ( v_x \\neq v_y \\neq v_z ) and \\omega = \\sqrt{v_x^2 k_x^2 + v_y^2 k_y^2 + v_z^2 k_z^2} . How does this change the Debye result for the heat capacity? Hint Write down the total energy as an integral over k , then change the integration variables so that the spherical symmetry of the integrand is restored.","title":"Exercise 4: Anisotropic sound velocities"},{"location":"1-intoduction/1-3-emetalsI/","text":"Electrons in metals I \u00b6 Introduction \u00b6 Metals are awesome, and this has long been known. People have been drawn to their shininess since the first chunks of the stuff were found, and it mustn't have been long before it was realised that they were different to other materials. To nail down exactly what is the difference would take some time (of order 10,000 years!) but it boils down to metals being conductive, and this comes from the ability of electrons in the material to move more freely. It turns out that one can apply a somewhat crude kinetic theory to understand the transport of electrons in metals, in the same way kinetic theory can be used to understand the transport of gasses. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Electromagnetism: The influence of fields of charged particles Mechanics: Construct and solve equations of motion Thermal physics: Kinetic theory of gases Text reference The material covered here is discussed in section(s) \\S 3 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: The kinetic theory of gases electrons \u00b6 Ohm's law is an empirical observation of conductors, which states that voltage is proportional to current V=IR . Since we are interested in the properties of materials, we would like to express this in a relation that does not depend on the conductor geometry. We achieve this by expressing the terms in Ohm's law with their microscopic equivalents. Consider a conducting wire with cross-sectional area A and length l . Such a wire has resistance R = \\rho l / A where \\rho is the material-dependent resistivity. An applied voltage V in the wire creates an electric field E = V/l . The resulting current I in the wire is described by the current density j \\equiv I / A . Returning these relations to Ohm's law, we get: V = I \u03c1 \\frac{l}{A} \u21d2 E = \u03c1 j, which relates the local quantities E and j . Our mission is to understand how this relation arises by considering motion of individual electrons in metals. This path was first trodden by Drude, who applied Boltzmann's kinetic theory of gases to a \"gas\" of electrons. The assumptions of Boltzmann's kinetic theory are that: Electrons scatter randomly at uncorrelated times. The average time between scattering is \\tau . Therefore, the probability of scattering in a time interval dt is dt / \\tau After each scattering event, the electron's momentum randomizes with a zero average \u27e8\\mathbf{p}\u27e9=0 It is also important to bake in the difference between neutral atoms or molecules in a gas and electrons, namely: Electrons are charged particles with charge -e , and consequently they respond to electric and magnetic fields through the Lorentz force ( \\mathbf{F}=-e\\left(\\mathbf{E}+\\mathbf{v}\u00d7\\mathbf{B}\\right) ) Even under these simplistic assumptions, the trajectory of the electrons is hard to calculate. As you will have seen elsewhere, with random scattering events, each trajectory is very different. A simple animation below shows several example trajectories, which are markedly different: Equations of motion \u00b6 Keep in your mind that our goal is to find electric current density j . Each electron with charge -e and velocity \\mathbf{v} carries current -e\\mathbf{v} . Therefore, if the electron density is n , the average current they carry is -ne\u27e8\\mathbf{v}\u27e9 . Thus, our goal shifts to compute the average velocity. Underpinning this calculation is idea that although it is difficult to calculate the motion of an individual electron, computing the average motion of the electrons is a much more tractable task. For convenience from now on, we will omit the average brackets, and write \\mathbf{v} instead of \u27e8\\mathbf{v}\u27e9 . This also applies to F . We derive an equation of motion for the \"average\" electron in the following way: Consider everything that happens in an (infinitesimal) time interval dt . A fraction dt/\u03c4 of the electrons scatters, and their average velocity becomes zero. m\\mathbf{v}(t + dt) = 0 The rest of the electrons (1 - dt/\u03c4) are accelerated by the Lorentz force F , so their velocity becomes m\\mathbf{v}(t + dt) = m\\mathbf{v}(t) + \\mathbf{F}\u22c5dt. To find the average velocity, we take a weighted average of these two groups of particles: \\begin{aligned} m\\mathbf{v}(t+dt) &= [m\\mathbf{v}(t) + F dt]\\left(1 - \\frac{dt}{\\tau}\\right) + 0\u22c5\\frac{dt}{\\tau} \\\\ &= [m\\mathbf{v}(t) + \\mathbf{F} dt] \\left(1 - \\frac{dt}{\\tau}\\right) \\\\ &= m\\mathbf{v}(t) + dt \\left[\\mathbf{F} - m\\frac{\\mathbf{v(t)}}{\\tau}\\right] - \\frac{\\mathbf{F}}{\\tau} dt^2 \\end{aligned} We now neglect the term proportional to dt\u00b2 (it vanishes when dt \u2192 0 ). Finally, we recognize that \\left[\\mathbf{v}(t+dt) - \\mathbf{v}(t)\\right]/dt = d\\mathbf{v}(t)/dt , which results in m\\frac{d\\mathbf{v}}{dt} = -m\\frac{\\mathbf{v}}{\u03c4} + \\mathbf{F}. Observe that the first term on the right-hand side has the same form as a drag force: it always decelerates the electrons. This equation equation of motion of the average electron is our main result: now we only need to apply it. Consequences of the Drude model \u00b6 Let us first consider the case without magnetic fields, \\mathbf{B} = 0 , and with constant electric field \\mathbf{E} . For many properties of interest, we are interested in the steady-state behaviour of the system, i.e. d\\mathbf{v}/dt = 0 , which upon solving the equation of motion yields: \\mathbf{v}=\\frac{-e\u03c4}{m}\\mathbf{E}=-|\u03bc|\\mathbf{E}. In the above equation, we define the mobility \u03bc\\equiv |e|\u03c4/m to be the ratio between the electron drift velocity and the electric field. The mobility is an extremely important quantity: it describes the response of the electron to an electric field. We can then substitute the steady-state velocity into the definition of current density: \\mathbf{j}=-en\\mathbf{v}=\\frac{n e^2\u03c4}{m}\\mathbf{E}=\\sigma\\mathbf{E} where \\sigma is the conductivity \\sigma=\\frac{ne^2\u03c4}{m}=ne\\mu such that \u03c1=1/\\sigma . The Hall effect \u00b6 Let us now consider the case where both and electric field \\mathbf{E} and magnetic field \\mathbf{B} are non-zero. Imagine a conductor with a current I flowing perpendicular to an external magnetic field \\mathbf{B} as shown below: Looking at the equations of motion: m\\frac{d\\mathbf{v}}{dt} = -m\\frac{\\mathbf{v}}{\u03c4} - e(\\mathbf{E} + \\mathbf{v}\\times\\mathbf{B}) we once again consider the steady state d\\mathbf{v}/dt = 0 . After substituting \\mathbf{v} = -\\mathbf{j}/ne , we arrive at \\mathbf{E}=\\frac{m}{ne^2\u03c4}\\mathbf{j} + \\frac{1}{ne}\\mathbf{j}\\times\\mathbf{B}. The first term is the same as before and describes the electric field parallel to the current, while the second is the electric field perpendicular to the current flow. In other words, if we send a current through a sample and apply a magnetic field, a voltage develops in the direction perpendicular to the current. This phenomenon is known as the Hall effect , with the perpendicular voltage called the Hall voltage , and the proportionality coefficient B/ne the Hall resistivity . Because of the Lorentz force, the electrons are deflected in a direction perpendicular to \\mathbf{B} and \\mathbf{j} . The deflection creates a charge imbalance, which in turn creates the electric field \\mathbf{E}_\\mathrm{H} compensating the Lorentz force. The above relation between the electric field and the current is linear, which allows us to write it in matrix form \\mathbf{E} = \\bar{\\rho} \\mathbf{j} where \\bar{\\rho} the resistivity matrix . Its diagonal elements are \\rho_{xx}=\\rho_{yy}=\\rho_{zz}=m/ne^2\\tau , which is the same as without magnetic field. The only nonzero off-diagonal elements when \\mathbf{B} points in the z -direction are \u03c1_{xy}=-\u03c1_{yx}=\\frac{B}{ne}\\equiv -R_\\mathrm{H}B, where R_H=-1/ne is the Hall coefficient . So by measuring the Hall voltage and knowing the electron charge, we can determine the density of free electrons in a material. While most materials have R_\\mathrm{H}<0 , interestingly some materials are found to have R_\\mathrm{H}>0 . 3.1 What would be the implications of a negative hall coefficient? With the density is negative, or that the charge carrier has a positive charge and thus not an electron. Thermal transport \u00b6 Given that Drude had cobbled together a kinetic theory for electrons, he decided that it was worthwhile to keep going, and make predictions about the properties of the \"gas\" in the same way as Boltzmann. Thermal conductivity \u00b6 Explicitly, Boltzmann derived the thermal conductivity \\kappa : \\kappa = \\frac{1}{3}n c_v \\langle v \\rangle \\lambda where c_v is the heat capacity per particle, \\langle v \\rangle is the average thermal velocity and \\lambda = \\langle v \\rangle \\tau is the scattering length. For a monatomic gas c_v = \\frac{3}{2} k_{\\mathrm{B}} and \\langle v \\rangle = \\sqrt{\\frac{8 k_{\\mathrm{B}} T}{\\pi m}} Without any grounds for justification, we can just give these a whirl and see how they go for electrons: \\kappa = \\frac{4}{\\pi}\\frac{n \\tau k_{\\mathrm{B}} T}{m} which still holds the phenomenological scattering rate \\tau , meaning makes independent measurement impossible. However, the conductivity \\sigma also contains \\tau , so by measuring their ratio we can eliminate the dependence and define the Lorenz number: L = \\frac{\\kappa}{T\\sigma} = \\frac{4}{\\pi}\\left(\\frac{k_{\\mathrm{B}}}{e}\\right) \\approx 1 \\times 10^{-8}~\\mathrm{W\\Omega~K^{-2}} which was close to the measured values of the Lorenz number (within a factor of 2 or so), and given there had never been any explanation at all for this behaviour, this result served was hailed as a major accomplishment. The Peltier effect \u00b6 A stack of peltier (thermoelectric cooling) devices can be used for highly-effective heat transfer Aside from being amazingly cool, we look at the Peltier effect to see that whilst the model well predicts the Lorenz number, this is actually somewhat of a fluke. The Peltier effect arises when a current flows through a material, as that current necessarily transports heat. The Peltier coefficient is defined by \\mathbf{j}^q = \\Pi \\mathbf{j} Where \\mathbf{j}^q is the heat current density and \\mathbf{j} is the electrical current density. In kinetic theory, the thermal current is \\mathbf{j}^q = \\frac{1}{3} (c_v T) n \\mathbf{v} where c_v T is the heat carried by one particle and c_v = 3 k_{\\mathrm{B}}/2 the heat capacity per particle, n the density of particles, and the factor of 1/3 from geometry. The electric current is \\mathbf{j} = -en\\mathbf{v} and thus \\Pi = \\frac{-c_v T}{3 e} = \\frac{-k_\\mathrm{B} T}{2 e} which yield the temperature independent ratio S = \\frac{\\Pi}{T} = \\frac{-k_{\\textrm{B}}}{2e} = -4.3 \\times 10^{-4} \\mathrm{V/K} where S is Seebeck coefficient (also the thermopower). And how well does this agree? Well, unlike \\kappa which was only a factor of 2 from the measure values, most values of S are on the order of 10^{-6} \\mathrm{V/K} . So unfortunately, whilst the Drude model gives us a \"broad brushstrokes\" picture of what is happening in a metal, it fails to accurately predict the specific behaviour and the quantities about which we care. Conclusions \u00b6 Drude theory is a kinetic theory and microscopic justification of the Ohm's law. We can calculate the resistivity from the characteristic scattering time \\tau . The Lorentz force leads to the Hall voltage that is perpendicular to the direction of both the electric current and the magnetic field. Drude theory does some things well, but ultimately falls short. Exercises \u00b6 Preliminary provocations \u00b6 How does the resistance of a purely 2D material depend on its size? Check that the units of mobility and the Hall coefficient are correct. (As you should always do!) Explain why the scattering times due to different types of scattering events add up in a reciprocal way. Exercise 1: Extracting quantities from basic Hall measurements \u00b6 We apply a magnetic field \\bf B along the z -direction to a planar (two-dimensional) sample that sits in the xy plane. The sample has width W in the y -direction, length L in the x -direction and we apply a current I along the x -direction. What is the relation between the electric field and the electric potential? V_b - V_a = -\\int_{\\Gamma} \\mathbf{E} \\cdot d\\mathbf{\\ell} if \\Gamma is a path from a to b . Suppose we measure a Hall voltage V_H . Express the Hall resistance R_{xy} = V_H/I as a function of magnetic field. Does R_{xy} depend on the geometry of the sample? Also express R_{xy} in terms of the Hall coefficient R_H . Assuming we control the magnetic field \\mathbf{B} , what quantity can we extract from a measurement of the Hall resistance? Would a large or a small magnetic field give a Hall voltage that is easier to measure? Express the longitudinal resistance R=V/I , where V is the voltage difference over the sample along the x direction, in terms of the longitudinal resistivity \u03c1_{xx} . Suppose we extracted n from a measurement of the Hall resistance, what quantity can we extract from a measurement of the longitudinal resistance? Does the result depend on the geometry of the sample? Exercise 2: Motion of an electron in a magnetic and an electric field. \u00b6 Consider an electron in free space experiencing a magnetic field \\mathbf{B} along the z -direction. Assume that the electron starts at the origin with a velocity v_0 along the x -direction. Write down the Newton's equation of motion for the electron, compute \\frac{d\\mathbf{v}}{{dt}} . What is the shape of the motion of the electron? Calculate the characteristic frequency and time-period T_c of this motion for B=1 Tesla. Now we accelerate the electron by adding an electric field \\mathbf{E} = E \\hat{x} . Adjust the differential equation for \\frac{d\\mathbf{v}}{{dt}} found in (1) to include \\mathbf{E} . Sketch the motion of the electron. Exercise 3: Temperature dependence of resistance in the Drude model \u00b6 We consider copper, which has a density of 8960 kg/m ^3 , an atomic weight of 63.55 g/mol, and a room-temperature resistivity of \u03c1=1.68\\cdot 10^{-8} \\Omega m. Each copper atom provides one free electron. Calculate the Drude scattering time \u03c4 at room temperature. Assuming that electrons move with the thermal velocity \\langle v \\rangle = \\sqrt{\\frac{8k_BT}{\\pi m}} , calculate the electron mean free path \\lambda , defined as the average distance an electron travels in between scattering events. The Drude model assumes that \\lambda is independent of temperature. How does the electrical resistivity \u03c1 depend on temperature under this assumption? Sketch \u03c1(T) . The empirical observation known as Matthiessen's Rule states that \u03c1(T) \\propto T . Discuss this result with reference to your answer above. Exercise 4: The Hall conductivity matrix and the Hall coefficient \u00b6 We apply a magnetic field \\bf B along the z -direction to a current carrying 2D sample in the xy plane. In this situation, the electric field \\mathbf{E} is related to the current density \\mathbf{j} by the resistivity matrix: \\mathbf{E} = \\begin{pmatrix} \u03c1_{xx} & \u03c1_{xy} \\\\ \u03c1_{yx} & \u03c1_{yy} \\end{pmatrix} \\mathbf{j} Sketch the expressions for \u03c1_{xx} and \u03c1_{xy} derived in the lecture notes as a function of the magnetic field \\bf{B} . Invert the resistivity matrix to obtain the conductivity matrix, \\begin{pmatrix} \\sigma_{xx} & \\sigma_{xy} \\\\ \\sigma_{yx} & \\sigma_{yy} \\end{pmatrix} allowing you to express \\mathbf{j} as a function of \\mathbf{E} .","title":"1.3: Electrons in metals I"},{"location":"1-intoduction/1-3-emetalsI/#electrons-in-metals-i","text":"","title":"Electrons in metals I"},{"location":"1-intoduction/1-3-emetalsI/#introduction","text":"Metals are awesome, and this has long been known. People have been drawn to their shininess since the first chunks of the stuff were found, and it mustn't have been long before it was realised that they were different to other materials. To nail down exactly what is the difference would take some time (of order 10,000 years!) but it boils down to metals being conductive, and this comes from the ability of electrons in the material to move more freely. It turns out that one can apply a somewhat crude kinetic theory to understand the transport of electrons in metals, in the same way kinetic theory can be used to understand the transport of gasses. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Electromagnetism: The influence of fields of charged particles Mechanics: Construct and solve equations of motion Thermal physics: Kinetic theory of gases Text reference The material covered here is discussed in section(s) \\S 3 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"1-intoduction/1-3-emetalsI/#the-kinetic-theory-of-gases-electrons","text":"Ohm's law is an empirical observation of conductors, which states that voltage is proportional to current V=IR . Since we are interested in the properties of materials, we would like to express this in a relation that does not depend on the conductor geometry. We achieve this by expressing the terms in Ohm's law with their microscopic equivalents. Consider a conducting wire with cross-sectional area A and length l . Such a wire has resistance R = \\rho l / A where \\rho is the material-dependent resistivity. An applied voltage V in the wire creates an electric field E = V/l . The resulting current I in the wire is described by the current density j \\equiv I / A . Returning these relations to Ohm's law, we get: V = I \u03c1 \\frac{l}{A} \u21d2 E = \u03c1 j, which relates the local quantities E and j . Our mission is to understand how this relation arises by considering motion of individual electrons in metals. This path was first trodden by Drude, who applied Boltzmann's kinetic theory of gases to a \"gas\" of electrons. The assumptions of Boltzmann's kinetic theory are that: Electrons scatter randomly at uncorrelated times. The average time between scattering is \\tau . Therefore, the probability of scattering in a time interval dt is dt / \\tau After each scattering event, the electron's momentum randomizes with a zero average \u27e8\\mathbf{p}\u27e9=0 It is also important to bake in the difference between neutral atoms or molecules in a gas and electrons, namely: Electrons are charged particles with charge -e , and consequently they respond to electric and magnetic fields through the Lorentz force ( \\mathbf{F}=-e\\left(\\mathbf{E}+\\mathbf{v}\u00d7\\mathbf{B}\\right) ) Even under these simplistic assumptions, the trajectory of the electrons is hard to calculate. As you will have seen elsewhere, with random scattering events, each trajectory is very different. A simple animation below shows several example trajectories, which are markedly different:","title":"The kinetic theory of gases electrons"},{"location":"1-intoduction/1-3-emetalsI/#equations-of-motion","text":"Keep in your mind that our goal is to find electric current density j . Each electron with charge -e and velocity \\mathbf{v} carries current -e\\mathbf{v} . Therefore, if the electron density is n , the average current they carry is -ne\u27e8\\mathbf{v}\u27e9 . Thus, our goal shifts to compute the average velocity. Underpinning this calculation is idea that although it is difficult to calculate the motion of an individual electron, computing the average motion of the electrons is a much more tractable task. For convenience from now on, we will omit the average brackets, and write \\mathbf{v} instead of \u27e8\\mathbf{v}\u27e9 . This also applies to F . We derive an equation of motion for the \"average\" electron in the following way: Consider everything that happens in an (infinitesimal) time interval dt . A fraction dt/\u03c4 of the electrons scatters, and their average velocity becomes zero. m\\mathbf{v}(t + dt) = 0 The rest of the electrons (1 - dt/\u03c4) are accelerated by the Lorentz force F , so their velocity becomes m\\mathbf{v}(t + dt) = m\\mathbf{v}(t) + \\mathbf{F}\u22c5dt. To find the average velocity, we take a weighted average of these two groups of particles: \\begin{aligned} m\\mathbf{v}(t+dt) &= [m\\mathbf{v}(t) + F dt]\\left(1 - \\frac{dt}{\\tau}\\right) + 0\u22c5\\frac{dt}{\\tau} \\\\ &= [m\\mathbf{v}(t) + \\mathbf{F} dt] \\left(1 - \\frac{dt}{\\tau}\\right) \\\\ &= m\\mathbf{v}(t) + dt \\left[\\mathbf{F} - m\\frac{\\mathbf{v(t)}}{\\tau}\\right] - \\frac{\\mathbf{F}}{\\tau} dt^2 \\end{aligned} We now neglect the term proportional to dt\u00b2 (it vanishes when dt \u2192 0 ). Finally, we recognize that \\left[\\mathbf{v}(t+dt) - \\mathbf{v}(t)\\right]/dt = d\\mathbf{v}(t)/dt , which results in m\\frac{d\\mathbf{v}}{dt} = -m\\frac{\\mathbf{v}}{\u03c4} + \\mathbf{F}. Observe that the first term on the right-hand side has the same form as a drag force: it always decelerates the electrons. This equation equation of motion of the average electron is our main result: now we only need to apply it.","title":"Equations of motion"},{"location":"1-intoduction/1-3-emetalsI/#consequences-of-the-drude-model","text":"Let us first consider the case without magnetic fields, \\mathbf{B} = 0 , and with constant electric field \\mathbf{E} . For many properties of interest, we are interested in the steady-state behaviour of the system, i.e. d\\mathbf{v}/dt = 0 , which upon solving the equation of motion yields: \\mathbf{v}=\\frac{-e\u03c4}{m}\\mathbf{E}=-|\u03bc|\\mathbf{E}. In the above equation, we define the mobility \u03bc\\equiv |e|\u03c4/m to be the ratio between the electron drift velocity and the electric field. The mobility is an extremely important quantity: it describes the response of the electron to an electric field. We can then substitute the steady-state velocity into the definition of current density: \\mathbf{j}=-en\\mathbf{v}=\\frac{n e^2\u03c4}{m}\\mathbf{E}=\\sigma\\mathbf{E} where \\sigma is the conductivity \\sigma=\\frac{ne^2\u03c4}{m}=ne\\mu such that \u03c1=1/\\sigma .","title":"Consequences of the Drude model"},{"location":"1-intoduction/1-3-emetalsI/#the-hall-effect","text":"Let us now consider the case where both and electric field \\mathbf{E} and magnetic field \\mathbf{B} are non-zero. Imagine a conductor with a current I flowing perpendicular to an external magnetic field \\mathbf{B} as shown below: Looking at the equations of motion: m\\frac{d\\mathbf{v}}{dt} = -m\\frac{\\mathbf{v}}{\u03c4} - e(\\mathbf{E} + \\mathbf{v}\\times\\mathbf{B}) we once again consider the steady state d\\mathbf{v}/dt = 0 . After substituting \\mathbf{v} = -\\mathbf{j}/ne , we arrive at \\mathbf{E}=\\frac{m}{ne^2\u03c4}\\mathbf{j} + \\frac{1}{ne}\\mathbf{j}\\times\\mathbf{B}. The first term is the same as before and describes the electric field parallel to the current, while the second is the electric field perpendicular to the current flow. In other words, if we send a current through a sample and apply a magnetic field, a voltage develops in the direction perpendicular to the current. This phenomenon is known as the Hall effect , with the perpendicular voltage called the Hall voltage , and the proportionality coefficient B/ne the Hall resistivity . Because of the Lorentz force, the electrons are deflected in a direction perpendicular to \\mathbf{B} and \\mathbf{j} . The deflection creates a charge imbalance, which in turn creates the electric field \\mathbf{E}_\\mathrm{H} compensating the Lorentz force. The above relation between the electric field and the current is linear, which allows us to write it in matrix form \\mathbf{E} = \\bar{\\rho} \\mathbf{j} where \\bar{\\rho} the resistivity matrix . Its diagonal elements are \\rho_{xx}=\\rho_{yy}=\\rho_{zz}=m/ne^2\\tau , which is the same as without magnetic field. The only nonzero off-diagonal elements when \\mathbf{B} points in the z -direction are \u03c1_{xy}=-\u03c1_{yx}=\\frac{B}{ne}\\equiv -R_\\mathrm{H}B, where R_H=-1/ne is the Hall coefficient . So by measuring the Hall voltage and knowing the electron charge, we can determine the density of free electrons in a material. While most materials have R_\\mathrm{H}<0 , interestingly some materials are found to have R_\\mathrm{H}>0 . 3.1 What would be the implications of a negative hall coefficient? With the density is negative, or that the charge carrier has a positive charge and thus not an electron.","title":"The Hall effect"},{"location":"1-intoduction/1-3-emetalsI/#thermal-transport","text":"Given that Drude had cobbled together a kinetic theory for electrons, he decided that it was worthwhile to keep going, and make predictions about the properties of the \"gas\" in the same way as Boltzmann.","title":"Thermal transport"},{"location":"1-intoduction/1-3-emetalsI/#thermal-conductivity","text":"Explicitly, Boltzmann derived the thermal conductivity \\kappa : \\kappa = \\frac{1}{3}n c_v \\langle v \\rangle \\lambda where c_v is the heat capacity per particle, \\langle v \\rangle is the average thermal velocity and \\lambda = \\langle v \\rangle \\tau is the scattering length. For a monatomic gas c_v = \\frac{3}{2} k_{\\mathrm{B}} and \\langle v \\rangle = \\sqrt{\\frac{8 k_{\\mathrm{B}} T}{\\pi m}} Without any grounds for justification, we can just give these a whirl and see how they go for electrons: \\kappa = \\frac{4}{\\pi}\\frac{n \\tau k_{\\mathrm{B}} T}{m} which still holds the phenomenological scattering rate \\tau , meaning makes independent measurement impossible. However, the conductivity \\sigma also contains \\tau , so by measuring their ratio we can eliminate the dependence and define the Lorenz number: L = \\frac{\\kappa}{T\\sigma} = \\frac{4}{\\pi}\\left(\\frac{k_{\\mathrm{B}}}{e}\\right) \\approx 1 \\times 10^{-8}~\\mathrm{W\\Omega~K^{-2}} which was close to the measured values of the Lorenz number (within a factor of 2 or so), and given there had never been any explanation at all for this behaviour, this result served was hailed as a major accomplishment.","title":"Thermal conductivity"},{"location":"1-intoduction/1-3-emetalsI/#the-peltier-effect","text":"A stack of peltier (thermoelectric cooling) devices can be used for highly-effective heat transfer Aside from being amazingly cool, we look at the Peltier effect to see that whilst the model well predicts the Lorenz number, this is actually somewhat of a fluke. The Peltier effect arises when a current flows through a material, as that current necessarily transports heat. The Peltier coefficient is defined by \\mathbf{j}^q = \\Pi \\mathbf{j} Where \\mathbf{j}^q is the heat current density and \\mathbf{j} is the electrical current density. In kinetic theory, the thermal current is \\mathbf{j}^q = \\frac{1}{3} (c_v T) n \\mathbf{v} where c_v T is the heat carried by one particle and c_v = 3 k_{\\mathrm{B}}/2 the heat capacity per particle, n the density of particles, and the factor of 1/3 from geometry. The electric current is \\mathbf{j} = -en\\mathbf{v} and thus \\Pi = \\frac{-c_v T}{3 e} = \\frac{-k_\\mathrm{B} T}{2 e} which yield the temperature independent ratio S = \\frac{\\Pi}{T} = \\frac{-k_{\\textrm{B}}}{2e} = -4.3 \\times 10^{-4} \\mathrm{V/K} where S is Seebeck coefficient (also the thermopower). And how well does this agree? Well, unlike \\kappa which was only a factor of 2 from the measure values, most values of S are on the order of 10^{-6} \\mathrm{V/K} . So unfortunately, whilst the Drude model gives us a \"broad brushstrokes\" picture of what is happening in a metal, it fails to accurately predict the specific behaviour and the quantities about which we care.","title":"The Peltier effect"},{"location":"1-intoduction/1-3-emetalsI/#conclusions","text":"Drude theory is a kinetic theory and microscopic justification of the Ohm's law. We can calculate the resistivity from the characteristic scattering time \\tau . The Lorentz force leads to the Hall voltage that is perpendicular to the direction of both the electric current and the magnetic field. Drude theory does some things well, but ultimately falls short.","title":"Conclusions"},{"location":"1-intoduction/1-3-emetalsI/#exercises","text":"","title":"Exercises"},{"location":"1-intoduction/1-3-emetalsI/#preliminary-provocations","text":"How does the resistance of a purely 2D material depend on its size? Check that the units of mobility and the Hall coefficient are correct. (As you should always do!) Explain why the scattering times due to different types of scattering events add up in a reciprocal way.","title":"Preliminary provocations"},{"location":"1-intoduction/1-3-emetalsI/#exercise-1-extracting-quantities-from-basic-hall-measurements","text":"We apply a magnetic field \\bf B along the z -direction to a planar (two-dimensional) sample that sits in the xy plane. The sample has width W in the y -direction, length L in the x -direction and we apply a current I along the x -direction. What is the relation between the electric field and the electric potential? V_b - V_a = -\\int_{\\Gamma} \\mathbf{E} \\cdot d\\mathbf{\\ell} if \\Gamma is a path from a to b . Suppose we measure a Hall voltage V_H . Express the Hall resistance R_{xy} = V_H/I as a function of magnetic field. Does R_{xy} depend on the geometry of the sample? Also express R_{xy} in terms of the Hall coefficient R_H . Assuming we control the magnetic field \\mathbf{B} , what quantity can we extract from a measurement of the Hall resistance? Would a large or a small magnetic field give a Hall voltage that is easier to measure? Express the longitudinal resistance R=V/I , where V is the voltage difference over the sample along the x direction, in terms of the longitudinal resistivity \u03c1_{xx} . Suppose we extracted n from a measurement of the Hall resistance, what quantity can we extract from a measurement of the longitudinal resistance? Does the result depend on the geometry of the sample?","title":"Exercise 1: Extracting quantities from basic Hall measurements"},{"location":"1-intoduction/1-3-emetalsI/#exercise-2-motion-of-an-electron-in-a-magnetic-and-an-electric-field","text":"Consider an electron in free space experiencing a magnetic field \\mathbf{B} along the z -direction. Assume that the electron starts at the origin with a velocity v_0 along the x -direction. Write down the Newton's equation of motion for the electron, compute \\frac{d\\mathbf{v}}{{dt}} . What is the shape of the motion of the electron? Calculate the characteristic frequency and time-period T_c of this motion for B=1 Tesla. Now we accelerate the electron by adding an electric field \\mathbf{E} = E \\hat{x} . Adjust the differential equation for \\frac{d\\mathbf{v}}{{dt}} found in (1) to include \\mathbf{E} . Sketch the motion of the electron.","title":"Exercise 2: Motion of an electron in a magnetic and an electric field."},{"location":"1-intoduction/1-3-emetalsI/#exercise-3-temperature-dependence-of-resistance-in-the-drude-model","text":"We consider copper, which has a density of 8960 kg/m ^3 , an atomic weight of 63.55 g/mol, and a room-temperature resistivity of \u03c1=1.68\\cdot 10^{-8} \\Omega m. Each copper atom provides one free electron. Calculate the Drude scattering time \u03c4 at room temperature. Assuming that electrons move with the thermal velocity \\langle v \\rangle = \\sqrt{\\frac{8k_BT}{\\pi m}} , calculate the electron mean free path \\lambda , defined as the average distance an electron travels in between scattering events. The Drude model assumes that \\lambda is independent of temperature. How does the electrical resistivity \u03c1 depend on temperature under this assumption? Sketch \u03c1(T) . The empirical observation known as Matthiessen's Rule states that \u03c1(T) \\propto T . Discuss this result with reference to your answer above.","title":"Exercise 3: Temperature dependence of resistance in the Drude model"},{"location":"1-intoduction/1-3-emetalsI/#exercise-4-the-hall-conductivity-matrix-and-the-hall-coefficient","text":"We apply a magnetic field \\bf B along the z -direction to a current carrying 2D sample in the xy plane. In this situation, the electric field \\mathbf{E} is related to the current density \\mathbf{j} by the resistivity matrix: \\mathbf{E} = \\begin{pmatrix} \u03c1_{xx} & \u03c1_{xy} \\\\ \u03c1_{yx} & \u03c1_{yy} \\end{pmatrix} \\mathbf{j} Sketch the expressions for \u03c1_{xx} and \u03c1_{xy} derived in the lecture notes as a function of the magnetic field \\bf{B} . Invert the resistivity matrix to obtain the conductivity matrix, \\begin{pmatrix} \\sigma_{xx} & \\sigma_{xy} \\\\ \\sigma_{yx} & \\sigma_{yy} \\end{pmatrix} allowing you to express \\mathbf{j} as a function of \\mathbf{E} .","title":"Exercise 4: The Hall conductivity matrix and the Hall coefficient"},{"location":"1-intoduction/1-4-emetalsII/","text":"Electrons in metals II \u00b6 Introduction \u00b6 The kinetic theory of Drude was a great first step in trying to answer the question \"metals, how do they work?\", but it was clear from the outset that there were problems with the theory. But with the discovery of the Pauli exclusion principle, it seemed only logical that with the inclusion of the fundamental property of the electrons, life would improve. Indeed it does... Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Statistical mechanics: familiarity with Fermi-Dirac statistics Quantum mechanics: Wavefunction of a free (unbound) electron Text reference The material covered here is discussed in section(s) \\S 4 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: The free electron model \u00b6 Fermi statistics \u00b6 In studying the Debye model , we saw the properties and physical behavior which arise from considering modes of oscillation in a material. The model we look at there, the Sommerfeld model, applies the same conceptual approach to electrons in metals. Sommerfeld considered the electrons as free particles that are not interacting with atomic nuclei, which is why the model is also called the free electron model . Similar to the Debye model, we consider a cubic box of size L \\times L \\times L with periodic boundary conditions. The solutions to the Schr\u00f6dinger equation of a free particle are plane waves: \\psi \\propto \\exp(i\\mathbf{k} \\cdot \\mathbf{r}), where \\mathbf{k} is the electron wave vector. Because we impose periodic boundary conditions, \\mathbf{k} must take discrete values \\frac{2\\pi}{L} (n_x, n_y, n_z) . The plane waves have eigenenergies given by the dispersion relation \\varepsilon(\\mathbf{k}) = \\frac{\\hbar^2 \\mathbf{k}^2}{2m}, with m being the mass of the electron. Let us plot \\varepsilon(k) as a function of k for a 1D system: A plot of the energy $\\varepsilon$ as a function of $k$, where black dot represent possible electron states In is worth highlighting the differences between the modes of oscillation in a solid we have considered previously, and the states of electrons which we now consider: electrons have a quadratic dispersion dispersion relation, and critically, electrons obey fermionic statistics. Conseqeuntly, the occupation of electron states is described by the Fermi-Dirac distribution n_{F}(\\beta(\\varepsilon-\\mu)) = \\frac{1}{e^{\\beta(\\varepsilon-\\mu)}+1} where \\beta = 1/k_{\\textrm{B}} T , \\varepsilon is the energy, and \\mu the chemical potential of an electron. The Fermi-Dirac distribution defines the number of electrons in the system: \\begin{align} N &= 2 \\sum_{\\mathbf{k}} n_{F}(\\beta(\\varepsilon-\\mu))\\\\ &= 2 \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ n_{F}(\\beta(\\varepsilon(\\mathbf{k})-\\mu)). \\end{align} Where we have again replaced a discrete sum over k with a volume integral. From where does the factor of 2 come from in the above equation? Contrast this to the Debye model. The factor 2 accounts for the spin degeneracy, whereas in the Debye model we had a factor of 3 to account for the distinct polarisations. To keep track of the origin of this term we will denote the spin degeneracy as 2_s . In the same way that we compute the number of electrons, we can compute the total energy of the electrons via E = 2_s \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ \\varepsilon(\\mathbf{k}) n_{F}(\\beta(\\varepsilon(\\mathbf{k})-\\mu)) . To cement the differences between Debye theory and Sommerfeld theory, different parameters are listed below: Crystal oscillations Electrons Dispersion relation \\omega = v_s \\lvert\\mathbf{k}\\rvert \\varepsilon = \\frac{\\hbar^2\\mathbf{k}^2}{2m} Statistics Bose-Einstein Fermi-Dirac n(\\varepsilon) = 1/(e^{\\beta \\varepsilon} - 1) 1/(e^{\\beta(\\varepsilon - \\mu)} + 1) Degeneracy per \\mathbf{k} 3 (polarization) 2 (spin) Total particle number temperature-dependent constant Note that the last element is important: in the case of oscillations within a material, warming said material creates more more oscillations. In contrast, the number of electrons stays generally remains the same. The Fermi sea \u00b6 To determine the chemical potential \\mu let us consider a 2D system with zero temperature and a finite number of electrons. At T=0 , the Fermi-Dirac distribution is step function n_{F}(\\beta(\\varepsilon-\\mu)) = \\Theta(-(\\varepsilon-\\varepsilon_F)). The chemical potential at T=0 is called the Fermi energy \\varepsilon_F and in this scenario, all electronic states with lower energies are occupied and all the states with higher energies are empty. In the reciprocal space, the occupied \\mathbf{k} -states form a circle (in 1D it is a line and in 3D a sphere). Reciprical space in two dimensions at $T=0$: states within the circle are occuplied and those outside are not A all-pervasive metaphor for describing this state of many electrons is the idea of the Fermi sea : electrons occupy a finite area in reciprocal space, starting from the \"deepest\" points with the lowest energy all the way up to the chemical potential. The border of the Fermi sea is called the Fermi surface , and in the free electron model it is a sphere with the radius equal to the Fermi wave vector . Can you identify the pattern in the nomencalture of important concepts? Pick an object or concept x, and name it the Fermi x. In an attempt to clarify the relationship between these concepts, let us take a look at the dispersion relation in 1D: The dispersion relation for electrons in one dimension By using the dispersion relation, we arrive to the relation \\varepsilon_F = \\frac{\\hbar^2 \\mathbf{k}_F^2}{2m}. The Fermi wavevector \\mathbf{k}_F also defines the Fermi momentum \\mathbf{p}_F = \\hbar \\mathbf{k}_F and the Fermi velocity : \\mathbf{v}_F = \\frac{\\mathbf{p}_F}{m} = \\frac{\\hbar \\mathbf{k}_F}{m}. The Fermi energy of copper is ~7 eV. What is the corresponding Fermi velocity? The Fermi velocity v_F\\approx 1700 km/s or 0.3% of the speed of light! Heat capacity \u00b6 Density of states \u00b6 As were have done previously, we want to compute the heat capacity, and to do this, we need to find the density of states: the number of states per energy interval. We have expression for both the number N and the energy E from above : \\begin{aligned} N & = 2_s \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ n_{F}(\\beta(\\varepsilon-\\mu)) \\\\ E & = 2_s \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ \\varepsilon(\\mathbf{k}) n_{F}(\\beta(\\varepsilon-\\mu)) \\end{aligned} which we seek to evaluate. Using the same tricks as last time, we move to spherical coordinates to reduce the inegral over three dimension to an intergral over one dimension, we can arrive at the expressions \\begin{aligned} N & = 2_s \\frac{V}{(2 \\pi)^3} \\int_0^\\infty \\mathrm{d} k ~ 4\\pi k^2 ~ n_{F}(\\beta(\\varepsilon(k)-\\mu)) \\\\ E & = 2_s \\frac{V}{(2 \\pi)^3} \\int_0^\\infty \\mathrm{d} k ~ 4\\pi k^2 ~ \\varepsilon(k) ~ n_{F}(\\beta(\\varepsilon(k)-\\mu)) \\end{aligned} We rewrite the expression above by substituting k=\\sqrt{2m\\varepsilon/\\hbar^2} and \\mathrm{d}k=\\sqrt{m/2\\varepsilon\\hbar^2} d\\varepsilon : \\begin{aligned} N & = V \\int_0^\\infty \\mathrm{d} \\varepsilon ~ g(\\varepsilon) ~ n_{F}(\\beta(\\varepsilon-\\mu)) \\\\ E & = V \\int_0^\\infty \\mathrm{d} \\varepsilon ~ \\varepsilon ~ g(\\varepsilon) ~ n_{F}(\\beta(\\varepsilon-\\mu)) \\end{aligned} where the density of states per unit volume is given by \\begin{aligned} g(\\varepsilon) \\mathrm{d} \\varepsilon & = \\frac{2}{(2\\pi)^3} 4 \\pi k^2 \\mathrm{d} k \\\\ & = \\frac{(2m)^{3/2}}{2 \\pi^2 \\hbar^3} \\varepsilon^{1/2} \\mathrm{d} \\varepsilon \\end{aligned} and quantifies the number of energy eigenstates between \\varepsilon and \\varepsilon + \\mathrm{d} \\varepsilon . This expression can be more cleanly written as g(\\varepsilon) \\mathrm{d} \\varepsilon = \\frac{3n}{2\\varepsilon_F}\\left(\\frac{\\varepsilon}{\\varepsilon_F}\\right)^{1/2} Verify the above expression. A good starting point would be to find a value for the number of electrons inside the sphere defined by k_F for T=0 Do the maths , integral of the Heaviside just means intergral is the volume of a sphere. We observe that the density of states of a 3D solid is proportional to a square root of energy: g(\\varepsilon) \\mathrm{d} \\varepsilon \\propto\\sqrt{\\varepsilon} Repeating the similar derivations, we find the density of states of 1D and 2D systems: 1D: g(\\varepsilon) = \\frac{2 L}{\\pi} \\frac{ \\mathrm{d}k}{ \\mathrm{d}\\varepsilon} \\propto 1/\\sqrt{\\varepsilon} 2D: g(\\varepsilon) = \\frac{k L^2}{\\pi} \\frac{ \\mathrm{d}k}{ \\mathrm{d}\\varepsilon} \\propto \\text{constant} which we can plot for a comparison of the behaviour of the system with different dimensionalities: Crank the handle \u00b6 Given we have an expression for E , we can set about computing the heat capacity. Sommerfeld expansion To explicitly calculate the heat capacity is a lot of work, and is the definition of a mathematical persuit with little reward. With a bit of hand waving, we can arrive at the same point, so that is what we are going to do. Some will think this lazy, and by all means, feel free to pursue the full calcuation - the Sommerfeld expansion may be of use. To effectively hand wave, let us begin by taking a closer look at the Fermi-Dirac distribution n_{F}(\\beta(\\varepsilon-\\mu)) = \\frac{1}{e^{\\beta(\\varepsilon-\\mu)}+1} which is plotted below for T = 0 and T > 0 with the same chemical potential \\mu = \\varepsilon_F . The Fermi-Dirac distribution at both $T = 0$ and $T > 0$ with the same chemical potential With a finite temperature T>0 , thermal excitations smear out the sharp change in the number of occupied electrons near \\varepsilon_F . Because the Fermi energy is typically in the range of electronvolts, the temperature of \\sim 10 000 ~\\mathrm{K} would be required in order for thermal excitations to give an electron a similar amount of energy! Therefore at room temperature T = 300~\\mathrm{K} the electron distribution over energies is very similar to that at T=0 . Below we compare the number of occupied electron states at each energy g(\\varepsilon) n_{F}(\\beta(\\varepsilon-\\mu)) at T = 0 (blue shaded area) with T > 0 (orange shaded area). In order to estimate the electron energy increase, we approximate difference between the blue and orange areas by triangles, as shown in the figure. This approximation is appropriate because the thermal smearing happens at the energies E \u223c k_B T , and it is much smaller than the Fermi energy \\varepsilon_{F} . At a finite temperature, the electrons occupying the top triangle (blue) are thermally excited to occupy the bottom triangle (orange). The base of the triangle is proportional to k_\\mathrm{B}T and the height is \\sim g(\\varepsilon_F) . Hence the number of excited electrons is N_\\mathrm{exc} \\approx g(\\varepsilon_F)k_BT (neglecting constants not depending on \\varepsilon_{F} ). These electrons gain k_BT of thermal energy, such that the total extra energy is \\begin{align} E(T) &= E(T = 0) + N_\\mathrm{exc}k_BT\\\\ &\\approx E(T = 0) + g(\\varepsilon_F)k_B^2T^2. \\end{align} Therefore, the electron heat capacity C_e is \\begin{align} C_e &= \\frac{ \\mathrm{d}E}{ \\mathrm{d}T}\\\\ &\\approx 2 g(\u03b5_F)k_B^2T\\\\ &\\overset{\\mathrm{3D}}{=} 3 Nk_B\\frac{T}{T_F}\\\\ &\\propto T, \\end{align} where we used N=\\frac{2}{3}\\varepsilon_Fg(\\varepsilon_F) and defined the Fermi temperature T_F \\equiv \\varepsilon_F/k_B . So what does all of this mean? Well, our algebraic journey has left us with a heat capacity that is linear in T , which to contrast to the model of Debye, had dependence on T^3 , which fitted the measurements pretty well. Does this mean that we have made things worse? Well let's look closely: The heat capacity of Silver divided by temperature versus the square of the temperature as taken from Atomic Heats of Copper, Silver, and Gold from 1\u00b0K to 5\u00b0K What is the expected behaviour of the plot above as predicted by the Debye model and the Sommerfeld model? We now have two contributions to the heat capacity C_{\\textrm{Sommerfeld}} = \\gamma T and C_{\\textrm{Debye}} = \\alpha T^3 , which should manifest as C = \\gamma T + \\alpha T^3. In the plot above, if we plot C/T , any offest at T=0 is indicative of free-electron behaviour, which we indeed observe. It is worth noting that At room temperature C_{\\textrm{Debye}}\\approx 3Nk_B\\gg C_{\\textrm{Sommerfeld}} \\propto k_B T / T_F , because T \\ll T_F . Near T=0 , the heat capacity due to ocillations C_{\\textrm{Debye}} \\propto k_B (T/T_D)^3 , which becomes smaller than the electron heat capacity at T \\lesssim \\sqrt{T_D^3/T_F} The scaling of C \u00b6 The behavior of contribution to C from the free-electron component at low temperature can be intuited via particles within an energy range of \\sim k_{B}T to the Fermi energy \\varepsilon_F become thermally excited, and each carries an extra energy k_{B}T : N_\\mathrm{exc} \\approx g(\\varepsilon_F)k_BT \\\\ E \\sim N_\\mathrm{exc} k_\\mathrm{B} T Example 1: 3D free electrons \u00b6 In 3D, g(\\varepsilon_F) is roughly constant. Thus the total energy obtained through thermal excitation is proportional to T \\times \\left( T\\times g(\\varepsilon_F) \\right) , from which it follows that C_e \\propto T . Example 2: graphene \u00b6 Graphene has a Fermi energy \\varepsilon_F = 0 and a density of states g(\\varepsilon) \\propto \\varepsilon . Therefore, within the energy range of k_BT , g(\\varepsilon) \\propto k_BT . Thus the total energy is proportional to T \\times T^2 and the heat capacity C_e \\propto T^2 . Conclusions \u00b6 The Sommerfeld free-electron model treats electrons as free particles with energy dispersion \\varepsilon = \\frac{\\hbar^2k^2}{2m} . The Fermi-Dirac distribution gives the probability of an electron state to be occupied. The electron contribution to the heat capacity is proportional to T . It is much lower than the heat capacity due to oscillations at high temperatures, and much higher at low temperatures. The scaling of heat capacity with T can be quickly estimated by estimating the number of particles in an energy range k_\\mathrm{B}T from the Fermi energy. Exercises \u00b6 Preliminary provocations \u00b6 Write down the expression for the total energy of particles with the density of states g(\\varepsilon) and the occupation number n_{F}(\\beta(\\varepsilon - \\mu)) . Explain what happens if a material is heated up to its Fermi temperature (assuming that material where this is possible exists). Why can we not use the Sommerfeld expansion with a Fermi energy of the order of the thermal energy? Is the heat capacity of a solid at temperatures near T=0 dominated by electrons or vibrations? Exercise 1: potassium \u00b6 The Sommerfeld model provides a good description of free electrons in alkali metals such as potassium (element K), which has a Fermi energy of \\varepsilon_{F} = 2.12 eV (data from Ashcroft, N. W. and Mermin, N. D., Solid State Physics, Saunders, 1976.). Check the Fermi surface database . Explain why potassium and (most) other alkali metals can be described well with the Sommerfeld model. Calculate the Fermi temperature, Fermi wave vector and Fermi velocity for potassium. Why is the Fermi temperature much higher than room temperature? Calculate the free electron density n in potassium. Compare this with the actual electron density of potassium, which can be calculated by using the density, atomic mass and atomic number of potassium. What can you conclude from this? Exercise 2: a hypothetical material \u00b6 A hypothetical metal has a Fermi energy \\varepsilon_F = 5.2 \\, \\mathrm{eV} and a density of states g(\\varepsilon) = 2 \\times 10^{10} \\, \\mathrm{eV}^{-\\frac{3}{2}} \\sqrt{\\varepsilon} . Give an integral expression for the total energy of the electrons in this hypothetical material in terms of the density of states g(\\varepsilon) , the temperature T and the chemical potential \\mu = \\varepsilon_F . Find the ground state energy at T = 0 . In order to obtain a good approximation of the integral for non-zero T , one can make use of the Sommerfeld expansion (the first equation is all you need and you can neglect the O\\left(\\frac{1}{\\beta \\mu}\\right)^{4} term). Using this expansion, find the difference between the total energy of the electrons for T = 1000 \\, \\mathrm{K} with that of the ground state. Now, find this difference in energy by calculating the integral found in 1 numerically. Compare your result with 3. Hint You can do numerical integration in python with scipy.integrate.quad(func, xmin, xmax) Calculate the heat capacity for T = 1000 \\, \\mathrm{K} in eV/K. Numerically compute the heat capacity by approximating the derivative of energy difference found in 4 with respect to T . To this end, make use of the fact that \\frac{dy}{dx}=\\lim_{\\Delta x \\to 0} \\frac{y(x + \\Delta x) - y(x - \\Delta x)}{2 \\Delta x}. Compare your result with 5. Exercise 3: graphene \u00b6 One of the most famous recently discovered materials is graphene . It consists of carbon atoms arranged in a 2D honeycomb structure. In this exercise, we will focus on the electrons in bulk graphene. Unlike in metals, electrons in graphene cannot be treated as 'free'. However, close to the Fermi level, the dispersion relation can be approximated by a linear relation: \\varepsilon(\\mathbf{k}) = \\pm c|\\mathbf{k}|. Note that the \\pm here means that there are two energy levels at a specified \\mathbf{k} . The Fermi level is set at \\varepsilon_F = 0 . Make a sketch of the dispersion relation. What other well-known particles have a linear dispersion relation? Using the dispersion relation and assuming periodic boundary conditions, derive an expression for the density of states of graphene. Do not forget spin degeneracy, and take into account that graphene has an additional two-fold 'valley degeneracy' (hence there is a total of a fourfold degeneracy instead of two). Your result should be linear with |\\varepsilon| . Hint It is convenient to first start by only considering the positive energy contributions \\varepsilon(\\mathbf{k}) = + c|\\mathbf{k}| and calculate the density of states for it. Then account for the negative energy contributions \\varepsilon(\\mathbf{k}) = - c|\\mathbf{k}| by adding it to the density of states for the positive energies. You can also make use of \\frac{\\rm{d} |k|}{\\rm{d}k} = \\frac{k}{|k|} . At finite temperatures, assume that electrons close to the Fermi level (i.e. not more than k_B T below the Fermi level) will get thermally excited, thereby increasing their energy by k_B T . Calculate the difference between the energy of the thermally excited state and that of the ground state E(T)-E_0 . To do so, show first that the number of electrons that will get excited is given by n_{ex} = \\frac{1}{2} g(-k_B T) k_B T. Calculate the heat capacity C_e as a function of the temperature T .","title":"1.4: Electrons in metals II"},{"location":"1-intoduction/1-4-emetalsII/#electrons-in-metals-ii","text":"","title":"Electrons in metals II"},{"location":"1-intoduction/1-4-emetalsII/#introduction","text":"The kinetic theory of Drude was a great first step in trying to answer the question \"metals, how do they work?\", but it was clear from the outset that there were problems with the theory. But with the discovery of the Pauli exclusion principle, it seemed only logical that with the inclusion of the fundamental property of the electrons, life would improve. Indeed it does... Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Statistical mechanics: familiarity with Fermi-Dirac statistics Quantum mechanics: Wavefunction of a free (unbound) electron Text reference The material covered here is discussed in section(s) \\S 4 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"1-intoduction/1-4-emetalsII/#the-free-electron-model","text":"","title":"The free electron model"},{"location":"1-intoduction/1-4-emetalsII/#fermi-statistics","text":"In studying the Debye model , we saw the properties and physical behavior which arise from considering modes of oscillation in a material. The model we look at there, the Sommerfeld model, applies the same conceptual approach to electrons in metals. Sommerfeld considered the electrons as free particles that are not interacting with atomic nuclei, which is why the model is also called the free electron model . Similar to the Debye model, we consider a cubic box of size L \\times L \\times L with periodic boundary conditions. The solutions to the Schr\u00f6dinger equation of a free particle are plane waves: \\psi \\propto \\exp(i\\mathbf{k} \\cdot \\mathbf{r}), where \\mathbf{k} is the electron wave vector. Because we impose periodic boundary conditions, \\mathbf{k} must take discrete values \\frac{2\\pi}{L} (n_x, n_y, n_z) . The plane waves have eigenenergies given by the dispersion relation \\varepsilon(\\mathbf{k}) = \\frac{\\hbar^2 \\mathbf{k}^2}{2m}, with m being the mass of the electron. Let us plot \\varepsilon(k) as a function of k for a 1D system: A plot of the energy $\\varepsilon$ as a function of $k$, where black dot represent possible electron states In is worth highlighting the differences between the modes of oscillation in a solid we have considered previously, and the states of electrons which we now consider: electrons have a quadratic dispersion dispersion relation, and critically, electrons obey fermionic statistics. Conseqeuntly, the occupation of electron states is described by the Fermi-Dirac distribution n_{F}(\\beta(\\varepsilon-\\mu)) = \\frac{1}{e^{\\beta(\\varepsilon-\\mu)}+1} where \\beta = 1/k_{\\textrm{B}} T , \\varepsilon is the energy, and \\mu the chemical potential of an electron. The Fermi-Dirac distribution defines the number of electrons in the system: \\begin{align} N &= 2 \\sum_{\\mathbf{k}} n_{F}(\\beta(\\varepsilon-\\mu))\\\\ &= 2 \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ n_{F}(\\beta(\\varepsilon(\\mathbf{k})-\\mu)). \\end{align} Where we have again replaced a discrete sum over k with a volume integral. From where does the factor of 2 come from in the above equation? Contrast this to the Debye model. The factor 2 accounts for the spin degeneracy, whereas in the Debye model we had a factor of 3 to account for the distinct polarisations. To keep track of the origin of this term we will denote the spin degeneracy as 2_s . In the same way that we compute the number of electrons, we can compute the total energy of the electrons via E = 2_s \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ \\varepsilon(\\mathbf{k}) n_{F}(\\beta(\\varepsilon(\\mathbf{k})-\\mu)) . To cement the differences between Debye theory and Sommerfeld theory, different parameters are listed below: Crystal oscillations Electrons Dispersion relation \\omega = v_s \\lvert\\mathbf{k}\\rvert \\varepsilon = \\frac{\\hbar^2\\mathbf{k}^2}{2m} Statistics Bose-Einstein Fermi-Dirac n(\\varepsilon) = 1/(e^{\\beta \\varepsilon} - 1) 1/(e^{\\beta(\\varepsilon - \\mu)} + 1) Degeneracy per \\mathbf{k} 3 (polarization) 2 (spin) Total particle number temperature-dependent constant Note that the last element is important: in the case of oscillations within a material, warming said material creates more more oscillations. In contrast, the number of electrons stays generally remains the same.","title":"Fermi statistics"},{"location":"1-intoduction/1-4-emetalsII/#the-fermi-sea","text":"To determine the chemical potential \\mu let us consider a 2D system with zero temperature and a finite number of electrons. At T=0 , the Fermi-Dirac distribution is step function n_{F}(\\beta(\\varepsilon-\\mu)) = \\Theta(-(\\varepsilon-\\varepsilon_F)). The chemical potential at T=0 is called the Fermi energy \\varepsilon_F and in this scenario, all electronic states with lower energies are occupied and all the states with higher energies are empty. In the reciprocal space, the occupied \\mathbf{k} -states form a circle (in 1D it is a line and in 3D a sphere). Reciprical space in two dimensions at $T=0$: states within the circle are occuplied and those outside are not A all-pervasive metaphor for describing this state of many electrons is the idea of the Fermi sea : electrons occupy a finite area in reciprocal space, starting from the \"deepest\" points with the lowest energy all the way up to the chemical potential. The border of the Fermi sea is called the Fermi surface , and in the free electron model it is a sphere with the radius equal to the Fermi wave vector . Can you identify the pattern in the nomencalture of important concepts? Pick an object or concept x, and name it the Fermi x. In an attempt to clarify the relationship between these concepts, let us take a look at the dispersion relation in 1D: The dispersion relation for electrons in one dimension By using the dispersion relation, we arrive to the relation \\varepsilon_F = \\frac{\\hbar^2 \\mathbf{k}_F^2}{2m}. The Fermi wavevector \\mathbf{k}_F also defines the Fermi momentum \\mathbf{p}_F = \\hbar \\mathbf{k}_F and the Fermi velocity : \\mathbf{v}_F = \\frac{\\mathbf{p}_F}{m} = \\frac{\\hbar \\mathbf{k}_F}{m}. The Fermi energy of copper is ~7 eV. What is the corresponding Fermi velocity? The Fermi velocity v_F\\approx 1700 km/s or 0.3% of the speed of light!","title":"The Fermi sea"},{"location":"1-intoduction/1-4-emetalsII/#heat-capacity","text":"","title":"Heat capacity"},{"location":"1-intoduction/1-4-emetalsII/#density-of-states","text":"As were have done previously, we want to compute the heat capacity, and to do this, we need to find the density of states: the number of states per energy interval. We have expression for both the number N and the energy E from above : \\begin{aligned} N & = 2_s \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ n_{F}(\\beta(\\varepsilon-\\mu)) \\\\ E & = 2_s \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ \\varepsilon(\\mathbf{k}) n_{F}(\\beta(\\varepsilon-\\mu)) \\end{aligned} which we seek to evaluate. Using the same tricks as last time, we move to spherical coordinates to reduce the inegral over three dimension to an intergral over one dimension, we can arrive at the expressions \\begin{aligned} N & = 2_s \\frac{V}{(2 \\pi)^3} \\int_0^\\infty \\mathrm{d} k ~ 4\\pi k^2 ~ n_{F}(\\beta(\\varepsilon(k)-\\mu)) \\\\ E & = 2_s \\frac{V}{(2 \\pi)^3} \\int_0^\\infty \\mathrm{d} k ~ 4\\pi k^2 ~ \\varepsilon(k) ~ n_{F}(\\beta(\\varepsilon(k)-\\mu)) \\end{aligned} We rewrite the expression above by substituting k=\\sqrt{2m\\varepsilon/\\hbar^2} and \\mathrm{d}k=\\sqrt{m/2\\varepsilon\\hbar^2} d\\varepsilon : \\begin{aligned} N & = V \\int_0^\\infty \\mathrm{d} \\varepsilon ~ g(\\varepsilon) ~ n_{F}(\\beta(\\varepsilon-\\mu)) \\\\ E & = V \\int_0^\\infty \\mathrm{d} \\varepsilon ~ \\varepsilon ~ g(\\varepsilon) ~ n_{F}(\\beta(\\varepsilon-\\mu)) \\end{aligned} where the density of states per unit volume is given by \\begin{aligned} g(\\varepsilon) \\mathrm{d} \\varepsilon & = \\frac{2}{(2\\pi)^3} 4 \\pi k^2 \\mathrm{d} k \\\\ & = \\frac{(2m)^{3/2}}{2 \\pi^2 \\hbar^3} \\varepsilon^{1/2} \\mathrm{d} \\varepsilon \\end{aligned} and quantifies the number of energy eigenstates between \\varepsilon and \\varepsilon + \\mathrm{d} \\varepsilon . This expression can be more cleanly written as g(\\varepsilon) \\mathrm{d} \\varepsilon = \\frac{3n}{2\\varepsilon_F}\\left(\\frac{\\varepsilon}{\\varepsilon_F}\\right)^{1/2} Verify the above expression. A good starting point would be to find a value for the number of electrons inside the sphere defined by k_F for T=0 Do the maths , integral of the Heaviside just means intergral is the volume of a sphere. We observe that the density of states of a 3D solid is proportional to a square root of energy: g(\\varepsilon) \\mathrm{d} \\varepsilon \\propto\\sqrt{\\varepsilon} Repeating the similar derivations, we find the density of states of 1D and 2D systems: 1D: g(\\varepsilon) = \\frac{2 L}{\\pi} \\frac{ \\mathrm{d}k}{ \\mathrm{d}\\varepsilon} \\propto 1/\\sqrt{\\varepsilon} 2D: g(\\varepsilon) = \\frac{k L^2}{\\pi} \\frac{ \\mathrm{d}k}{ \\mathrm{d}\\varepsilon} \\propto \\text{constant} which we can plot for a comparison of the behaviour of the system with different dimensionalities:","title":"Density of states"},{"location":"1-intoduction/1-4-emetalsII/#crank-the-handle","text":"Given we have an expression for E , we can set about computing the heat capacity. Sommerfeld expansion To explicitly calculate the heat capacity is a lot of work, and is the definition of a mathematical persuit with little reward. With a bit of hand waving, we can arrive at the same point, so that is what we are going to do. Some will think this lazy, and by all means, feel free to pursue the full calcuation - the Sommerfeld expansion may be of use. To effectively hand wave, let us begin by taking a closer look at the Fermi-Dirac distribution n_{F}(\\beta(\\varepsilon-\\mu)) = \\frac{1}{e^{\\beta(\\varepsilon-\\mu)}+1} which is plotted below for T = 0 and T > 0 with the same chemical potential \\mu = \\varepsilon_F . The Fermi-Dirac distribution at both $T = 0$ and $T > 0$ with the same chemical potential With a finite temperature T>0 , thermal excitations smear out the sharp change in the number of occupied electrons near \\varepsilon_F . Because the Fermi energy is typically in the range of electronvolts, the temperature of \\sim 10 000 ~\\mathrm{K} would be required in order for thermal excitations to give an electron a similar amount of energy! Therefore at room temperature T = 300~\\mathrm{K} the electron distribution over energies is very similar to that at T=0 . Below we compare the number of occupied electron states at each energy g(\\varepsilon) n_{F}(\\beta(\\varepsilon-\\mu)) at T = 0 (blue shaded area) with T > 0 (orange shaded area). In order to estimate the electron energy increase, we approximate difference between the blue and orange areas by triangles, as shown in the figure. This approximation is appropriate because the thermal smearing happens at the energies E \u223c k_B T , and it is much smaller than the Fermi energy \\varepsilon_{F} . At a finite temperature, the electrons occupying the top triangle (blue) are thermally excited to occupy the bottom triangle (orange). The base of the triangle is proportional to k_\\mathrm{B}T and the height is \\sim g(\\varepsilon_F) . Hence the number of excited electrons is N_\\mathrm{exc} \\approx g(\\varepsilon_F)k_BT (neglecting constants not depending on \\varepsilon_{F} ). These electrons gain k_BT of thermal energy, such that the total extra energy is \\begin{align} E(T) &= E(T = 0) + N_\\mathrm{exc}k_BT\\\\ &\\approx E(T = 0) + g(\\varepsilon_F)k_B^2T^2. \\end{align} Therefore, the electron heat capacity C_e is \\begin{align} C_e &= \\frac{ \\mathrm{d}E}{ \\mathrm{d}T}\\\\ &\\approx 2 g(\u03b5_F)k_B^2T\\\\ &\\overset{\\mathrm{3D}}{=} 3 Nk_B\\frac{T}{T_F}\\\\ &\\propto T, \\end{align} where we used N=\\frac{2}{3}\\varepsilon_Fg(\\varepsilon_F) and defined the Fermi temperature T_F \\equiv \\varepsilon_F/k_B . So what does all of this mean? Well, our algebraic journey has left us with a heat capacity that is linear in T , which to contrast to the model of Debye, had dependence on T^3 , which fitted the measurements pretty well. Does this mean that we have made things worse? Well let's look closely: The heat capacity of Silver divided by temperature versus the square of the temperature as taken from Atomic Heats of Copper, Silver, and Gold from 1\u00b0K to 5\u00b0K What is the expected behaviour of the plot above as predicted by the Debye model and the Sommerfeld model? We now have two contributions to the heat capacity C_{\\textrm{Sommerfeld}} = \\gamma T and C_{\\textrm{Debye}} = \\alpha T^3 , which should manifest as C = \\gamma T + \\alpha T^3. In the plot above, if we plot C/T , any offest at T=0 is indicative of free-electron behaviour, which we indeed observe. It is worth noting that At room temperature C_{\\textrm{Debye}}\\approx 3Nk_B\\gg C_{\\textrm{Sommerfeld}} \\propto k_B T / T_F , because T \\ll T_F . Near T=0 , the heat capacity due to ocillations C_{\\textrm{Debye}} \\propto k_B (T/T_D)^3 , which becomes smaller than the electron heat capacity at T \\lesssim \\sqrt{T_D^3/T_F}","title":"Crank the handle"},{"location":"1-intoduction/1-4-emetalsII/#the-scaling-of-c","text":"The behavior of contribution to C from the free-electron component at low temperature can be intuited via particles within an energy range of \\sim k_{B}T to the Fermi energy \\varepsilon_F become thermally excited, and each carries an extra energy k_{B}T : N_\\mathrm{exc} \\approx g(\\varepsilon_F)k_BT \\\\ E \\sim N_\\mathrm{exc} k_\\mathrm{B} T","title":"The scaling of C"},{"location":"1-intoduction/1-4-emetalsII/#example-1-3d-free-electrons","text":"In 3D, g(\\varepsilon_F) is roughly constant. Thus the total energy obtained through thermal excitation is proportional to T \\times \\left( T\\times g(\\varepsilon_F) \\right) , from which it follows that C_e \\propto T .","title":"Example 1: 3D free electrons"},{"location":"1-intoduction/1-4-emetalsII/#example-2-graphene","text":"Graphene has a Fermi energy \\varepsilon_F = 0 and a density of states g(\\varepsilon) \\propto \\varepsilon . Therefore, within the energy range of k_BT , g(\\varepsilon) \\propto k_BT . Thus the total energy is proportional to T \\times T^2 and the heat capacity C_e \\propto T^2 .","title":"Example 2: graphene"},{"location":"1-intoduction/1-4-emetalsII/#conclusions","text":"The Sommerfeld free-electron model treats electrons as free particles with energy dispersion \\varepsilon = \\frac{\\hbar^2k^2}{2m} . The Fermi-Dirac distribution gives the probability of an electron state to be occupied. The electron contribution to the heat capacity is proportional to T . It is much lower than the heat capacity due to oscillations at high temperatures, and much higher at low temperatures. The scaling of heat capacity with T can be quickly estimated by estimating the number of particles in an energy range k_\\mathrm{B}T from the Fermi energy.","title":"Conclusions"},{"location":"1-intoduction/1-4-emetalsII/#exercises","text":"","title":"Exercises"},{"location":"1-intoduction/1-4-emetalsII/#preliminary-provocations","text":"Write down the expression for the total energy of particles with the density of states g(\\varepsilon) and the occupation number n_{F}(\\beta(\\varepsilon - \\mu)) . Explain what happens if a material is heated up to its Fermi temperature (assuming that material where this is possible exists). Why can we not use the Sommerfeld expansion with a Fermi energy of the order of the thermal energy? Is the heat capacity of a solid at temperatures near T=0 dominated by electrons or vibrations?","title":"Preliminary provocations"},{"location":"1-intoduction/1-4-emetalsII/#exercise-1-potassium","text":"The Sommerfeld model provides a good description of free electrons in alkali metals such as potassium (element K), which has a Fermi energy of \\varepsilon_{F} = 2.12 eV (data from Ashcroft, N. W. and Mermin, N. D., Solid State Physics, Saunders, 1976.). Check the Fermi surface database . Explain why potassium and (most) other alkali metals can be described well with the Sommerfeld model. Calculate the Fermi temperature, Fermi wave vector and Fermi velocity for potassium. Why is the Fermi temperature much higher than room temperature? Calculate the free electron density n in potassium. Compare this with the actual electron density of potassium, which can be calculated by using the density, atomic mass and atomic number of potassium. What can you conclude from this?","title":"Exercise 1: potassium"},{"location":"1-intoduction/1-4-emetalsII/#exercise-2-a-hypothetical-material","text":"A hypothetical metal has a Fermi energy \\varepsilon_F = 5.2 \\, \\mathrm{eV} and a density of states g(\\varepsilon) = 2 \\times 10^{10} \\, \\mathrm{eV}^{-\\frac{3}{2}} \\sqrt{\\varepsilon} . Give an integral expression for the total energy of the electrons in this hypothetical material in terms of the density of states g(\\varepsilon) , the temperature T and the chemical potential \\mu = \\varepsilon_F . Find the ground state energy at T = 0 . In order to obtain a good approximation of the integral for non-zero T , one can make use of the Sommerfeld expansion (the first equation is all you need and you can neglect the O\\left(\\frac{1}{\\beta \\mu}\\right)^{4} term). Using this expansion, find the difference between the total energy of the electrons for T = 1000 \\, \\mathrm{K} with that of the ground state. Now, find this difference in energy by calculating the integral found in 1 numerically. Compare your result with 3. Hint You can do numerical integration in python with scipy.integrate.quad(func, xmin, xmax) Calculate the heat capacity for T = 1000 \\, \\mathrm{K} in eV/K. Numerically compute the heat capacity by approximating the derivative of energy difference found in 4 with respect to T . To this end, make use of the fact that \\frac{dy}{dx}=\\lim_{\\Delta x \\to 0} \\frac{y(x + \\Delta x) - y(x - \\Delta x)}{2 \\Delta x}. Compare your result with 5.","title":"Exercise 2: a hypothetical material"},{"location":"1-intoduction/1-4-emetalsII/#exercise-3-graphene","text":"One of the most famous recently discovered materials is graphene . It consists of carbon atoms arranged in a 2D honeycomb structure. In this exercise, we will focus on the electrons in bulk graphene. Unlike in metals, electrons in graphene cannot be treated as 'free'. However, close to the Fermi level, the dispersion relation can be approximated by a linear relation: \\varepsilon(\\mathbf{k}) = \\pm c|\\mathbf{k}|. Note that the \\pm here means that there are two energy levels at a specified \\mathbf{k} . The Fermi level is set at \\varepsilon_F = 0 . Make a sketch of the dispersion relation. What other well-known particles have a linear dispersion relation? Using the dispersion relation and assuming periodic boundary conditions, derive an expression for the density of states of graphene. Do not forget spin degeneracy, and take into account that graphene has an additional two-fold 'valley degeneracy' (hence there is a total of a fourfold degeneracy instead of two). Your result should be linear with |\\varepsilon| . Hint It is convenient to first start by only considering the positive energy contributions \\varepsilon(\\mathbf{k}) = + c|\\mathbf{k}| and calculate the density of states for it. Then account for the negative energy contributions \\varepsilon(\\mathbf{k}) = - c|\\mathbf{k}| by adding it to the density of states for the positive energies. You can also make use of \\frac{\\rm{d} |k|}{\\rm{d}k} = \\frac{k}{|k|} . At finite temperatures, assume that electrons close to the Fermi level (i.e. not more than k_B T below the Fermi level) will get thermally excited, thereby increasing their energy by k_B T . Calculate the difference between the energy of the thermally excited state and that of the ground state E(T)-E_0 . To do so, show first that the number of electrons that will get excited is given by n_{ex} = \\frac{1}{2} g(-k_B T) k_B T. Calculate the heat capacity C_e as a function of the temperature T .","title":"Exercise 3: graphene"},{"location":"2-chemistry/2-1-chemistry/","text":"Chemisty 101 \u00b6 Also known as the physicist's guide to chemistry 1 . Introduction \u00b6 Our journey thus far has been considering the basic properties of solids, and our descriptions have been based around the properties of electrons and vibrations in solids, which can provide useful results but our endeavour is much grander: we would like to explain all the properties of all the solids. The first step on this journey is to no longer consider a collection of free electrons; solids are made up of many atoms, so we best find a way of baking this into whatever we do! Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Quantum mechanics: the Schr\u00f6dinger equation, wavefunction of the hydrogen atom, wavefunction for a particle in a box, Dirac notation Mathematics: the variational principle, linear algebra Text reference The material covered here is discussed in section(s) \\S 5, 6.3 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: A retrospective \u00b6 Up to this point, we have: Introduced reciprocal ( k -space) Postulated the dispersion relations for free electrons and oscillations in a solid Calculated the heat capacity of free electrons and oscillations in a solid which has permitted The understanding of how materials can store heat via oscillations (Debye model) The understanding of how free electrons conduct (Drude model) and store heat/energy (Sommerfeld model) In constructing these models, we have made several approximations and postulations, and there are a few big questions to answer: In the Debye model, why is there a cutoff frequency for oscillations? Why are there no modes of oscillation beyond this frequency? In the Drude model, we have electrons modelled as a gas, but why don't electrons scatter from the atoms in solid? Why are some materials metallic, and others not metallic? If we are to have a hope of understanding any of the above, we are going to have to understand better what is happening on the atomic scale. Atoms, how do they work? \u00b6 It is no exaggeration to say that everything can be pretty-well described by the Schr\u00f6dinger equation: \\hat{H}\\psi = E\\psi, with \\hat{H} the Hamiltonian of the system, that is, the sum of kinetic and potential energies. For the hydrogen atom, the potential is due to the Coulomb interaction between the electron and the nucleus: H=-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial {\\mathbf r^2}} - \\frac{e^2}{4\\pi\\varepsilon_0|r|} which you will have solved in detail elsewhere. If we move to the next-most-simple atom, helium, the Hamiltonian immediately becomes more complex, containing not just the Coulomb interaction between the individual electrons and the nuclei, but also Coulomb repulsion between the two electrons: H=-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial {\\mathbf r_1^2}} -\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial {\\mathbf r_2^2}}- \\frac{2e^2}{4\\pi\\varepsilon_0|r_1|} - \\frac{2e^2}{4\\pi\\varepsilon_0|r_2|} + \\frac{e^2}{4\\pi\\varepsilon_0|r_1 - r_2|}. From a purely mathematical point of view, this means we need find eigenvalues and eigenvectors of a six-dimensional PDE, which is much tougher work than the three-dimensional PDE descirbing hydrogen. You have likely seen the variational method to construct appoximate wavefunctions, any even in this case it is a fair amount of work. Let us now turn to another system, a single copper atom. Copper has 29 electrons, so to find the energy spectrum of copper we would need to solve an 87-dimensional Schr\u00f6dinger equation. Such things are totally intractable: there is simply no way to solve such a thing, even with really big computers. It is this growth in complexity with the number of interacting quantum particles is why many-body quantum physics is very much an active research area in solid-state physics and quantum chemistry. But we need not have an analytic solution for everything in order to be satisfied, on the contrary, we can use physical insight and approximations to construct models of complex systems based on simpler, easier to handle systems. To some extent, this is exactly where chemisty takes over: it is often said that Chemisty is applied physics, but they should simply be thought of as the same thing, but with a focus on different parts of the spectrum: the complexity of solving problems in chemistry is the reason why we need to accept empirical observations as chemical laws , even though they work with limited precision and ultimately consequences of the underlying physics as modelled by the Schr\u00f6dinger equation. Quantum numbers and shell filling \u00b6 The wavefunction for an electron in a hydrogen atom is described by the state |n, l, m_l, m_s\\rangle , where the quantum numbers are: n = 1, 2, \\ldots is the principal (azimuthal) quantum number l = 0, 1, \\ldots, n-1 is the orbital angular momentum (also known as s, p, d, f orbitals) m_l = -l, -l+1 \\ldots, l is the z -component of angular momentum l m_s is the z -component of the electronic spin The low energy states of hydrogen are shown below: Electronic wavefunctions for the low-energy states of Hydrogen. Image source: Wikipedia \u00a9 Geek3 CC-BY-SA To a first approximation, the electronic wavefunctions in other atoms are similar to those of hydrogen; however, the electron energies are very different due to the Coulomb interaction, both between other electrons and the ionic core. Even still, knowledge of the hydrogenic wavefunctions can greatly aid our description of other atoms. If one considers an atom with multiple electrons, as we \"add\" electrons to a given system - for example, if one considers a sequence of elements - it is important to determine the order in which electronic states will be filled. Two empirical rules which can do an excellent job of describing this order are: Aufbau principle : electrons fill the lowest-lying states first and fill these shells to completion Madelung's rule : electrons first occupy shells with the lowest n+l . If there are several orbitals with equal n+l , electrons preferentially occupy those with smaller n Combining the two rules, we obtain the shell filing order: 1s, 2s, 2p, 3s, 3p, 4s, 3d, etc., which is illustrated in the periodic table below: While these rules accurately predict the electronic structure of most elements, they are only approximate, and fail to describe some of the heavier elements . The reason we care about the order in which shells are filled is because as we have seen, the valence electrons, that is, those in the outermost shell dominate the elements properties, including underpinning chemical reactions and conductivity. Inner-shell electrons acts to shield the charge from the valance electrons - resulting in an altered effective nuclear charge as seen by the valance electrons - but beyond this, they are largely inert. Bonds: the ties that bind \u00b6 There exist many different mechanisms for bonding, for example: ionic bonding, covalent bonding, bonding through the van der Waals interaction, and the list continues. At its simplest, one can understand bonding as two (or more) systems combining to reduce the collective energy, that is, reaching a state with lower energy than the sum of energies of the individual states. Ionic bonding \u00b6 The conceptually simplest form of bonding is when an atom that has an electron of which it isn't much fond, meets and atom which really likes collecting electrons. The most common example of this would be the reaction of sodium and chlorine: \\textrm{Na + Cl} \\rightarrow \\textrm{Na}^+ + \\textrm{Cl}^- \\rightarrow \\textrm{NaCl} where one can think of electron transfer from sodium ion to the chlorine ion, rendering sodium and chlorine in their lowest energy states but also leaving them charged, and these ionic species consequently attract and form a compound. To firm up this concept, we can consider the ionisation energy , which is the energy required to remove an electron (and form a positive ion) and the electron affinity , which is the energy associated with the capture of an electron to form a negative ion. Shown below are periodic tables with values for the ionisation energy and the electron affinity indicated. First-ionisation energies for various elements. Image source: LibreTexts which is used under licence CC-BY-NC-SA 3.0 Electron affinity (first electron) for various elements. Image source: LibreTexts which is used under licence CC-BY-NC-SA 3.0 It should be clear that in a situation where an atom A has a low ionisation energy and an atom B with a large electron efficiency, there will be some energy difference between the states of neutral atoms and ions \\Delta E_{A+B \\rightarrow A^+ + B^-} = E_{\\textrm{ionisation},~A} - E_{\\textrm{affinity},~B} It is also important to consider the energy that comes from the attraction between the oppositely charged ionic species E_{\\textrm{cohesive}} = \\Delta E_{A^+ + B^- \\rightarrow AB} which is mostly determined by the Coulomb interaction between the ions. The energy change for the bonding process in then \\Delta E_{A+B \\rightarrow AB} = E_{\\textrm{ionisation},~A} - E_{\\textrm{affinity},~B} - E_{\\textrm{cohesive}} and the reaction will proceed if \\Delta E_{A+B \\rightarrow AB}<0 . The properties of ionic solids are that they tend to have high melting points - the bond is strong! - and they tend to be electrical insulating (something we will discuss later). Covalent bonding \u00b6 Covalent bonding - as the name suggests - involves the sharing of electrons between atoms. Particles in boxes \u00b6 The simplest way one can model the bonding process is to consider the electronic wavefunctions of the atoms at particles in a box, which is likely the first quantum system you ever discussed in detail. Whilst the picture is not going to produce highly-accurate results, the qualitative behaviour goes a long way to understand the process more generally. Let us consider two identical atoms, where we model the atomic potential as a one-dimensional box with length L , which are far apart to ignore the influence of the other potential, then the energy of each state is given by E = \\frac{\\hbar^2\\pi^2}{2mL^2} If our boxed-like atoms are brought close together such that the boxes merge into one large box of length 2L , the energy of this new bonding state will be given by E = \\frac{\\hbar^2\\pi^2}{2m(2L)^2} If we imagine that our atoms were hydrogenic - that is, with a single valance electron - the electrons from both atoms could occupy this ground state due to the spin degeneracy of the state, and thus a new system with lower energy is formed. It is this interaction which results in Hydrogen existing as a the diatomic molecule \\textrm{H}_2 . Now imagine a system with more electrons, for example, 2 electrons. When these atoms come together, the shared electrons will occupy the lower-energy bonded state, but must also occupy the first excited state, the so-called anti-bonded state which will have energy E = \\frac{2^2\\hbar^2\\pi^2}{2m(2L)^2} Under these circumstances, it is not energetically favourable to a covalent bond, and thus molecules are not formed - see the noble gasses here! Probabilities for electrons in bonding and antibonding states as modelled using square-well potentials The linear combination of atomic orbitals ( LCAO ) \u00b6 The mouthful that is LCAO is also commonly called molecular orbital or Tight-binding theory, and is a way to quantitatively justify the handwaving from above. Consider a system of two atoms next to each other from which we want to calculate their propensity to form a molecule, that is, calculate the energy eigenstates as a function of the separation of the nuclei. As the nuclei are big and heavy, we are going to assume that their positions are fixed for all positions of the electrons (aka the Born-Oppenheimer approximation). For an individual electron, the Hamiltonian is given by \\hat{H} = \\hat{V}_1 + \\hat{V}_2 + \\hat{K}, where V_1 the potential due to the first nucleus, V_2 due to the second nucleus, and K is the kinetic energy of the electron. Whilst it is possible to solve the Schr\u00f6dinger equation directly, it is a fair bit of work, and we want to have a mechanism for solving these systems more generally, so we will seek a solution using the variational method. We begin by denoting the wave function of an electron bound to the first and second atom |1\\rangle and |2\\rangle respectively, which we know to be eigenfunctions of the systems: \\begin{align} (\\hat{V}_1 + \\hat{K})|1\\rangle = \\varepsilon_0 |1\\rangle \\\\ (\\hat{V}_2 + \\hat{K})|2\\rangle = \\varepsilon_0 |2\\rangle \\end{align} We are going to search for a solution in the form: |\\psi\\rangle = \\phi_{1}|1\\rangle + \\phi_{2}|2\\rangle. where \\phi_{1} and \\phi_{2} are the probability amplitudes of the respective electronic states. The state |\\psi\\rangle is called a molecular orbital because it describes the state of the diatomic molecule, and the molecular orbital is created as a Linear Combination of Atomic Orbitals . To increase the tractability of the problem, we are assume that the atomic orbitals are orthogonal, that is \\langle1|2\\rangle=0 ; however, this is not strictly necessary. We can then write the Schr\u00f6dinger equation for our system in matrix form \\sum_j H_{ij} \\phi_j = \\epsilon\\phi_i where the matrix elements of the Hamiltonian are given by H_{ij} = \\langle i | \\hat{H} | j \\rangle 5.1 Compute the matrix elements of the Hamiltonian \\hat{H} Crank the handle! \\begin{align} H_{11} = \\langle 1 | \\hat{H} | 1 \\rangle = \\langle 1 | \\hat{V}_1 + \\hat{K} | 1 \\rangle + \\langle 1 | \\hat{V}_2 | 1 \\rangle = \\varepsilon_0 + V_{\\textrm{cross}} \\\\ H_{22} = \\langle 2 | \\hat{H} | 2 \\rangle = \\langle 2 | \\hat{V}_2 + \\hat{K} | 2 \\rangle + \\langle 2 | \\hat{V}_1 | 2 \\rangle = \\varepsilon_0 + V_{\\textrm{cross}} \\\\ H_{12} = \\langle 1 | \\hat{H} | 2 \\rangle = \\langle 1 | \\hat{V}_2 + \\hat{K} | 2 \\rangle + \\langle 1 | \\hat{V}_1 | 2 \\rangle = 0 - t \\\\ H_{21} = \\langle 2 | \\hat{H} | 2 \\rangle = \\langle 2 | \\hat{V}_1 + \\hat{K} | 1 \\rangle + \\langle 2 | \\hat{V}_2 | 1 \\rangle = 0 - t* \\end{align} where V_{\\textrm{cross}} = \\langle 1 | \\hat{V}_2 | 1 \\rangle = \\langle 2 | \\hat{V}_2 | 2 \\rangle is the potential felt by state | 1 \\rangle due to nucleus 2 and vice versa, and the term t = - \\langle 1 | \\hat{V}_1 | 2 \\rangle = - \\langle 2 | \\hat{V}_2 | 1 \\rangle is the energy associated with a movement between states | 1 \\rangle and | 2 \\rangle The matrix is thus defined by two parameters, the onsite energy which gives the energy of an electron occupying either of the orbitals, and the so-called hopping integral which is the energy associated with the exchange of the electronic states 2 . 5.2 Using the matrix computed above, calculate the energy eigenstates for the system and their energies From above, we have the Schr\u00f6dinger equation in the form \\begin{bmatrix} \\varepsilon_0 + V_{\\textrm{cross}} & -t \\\\ -t* & \\varepsilon_0 + V_{\\textrm{cross}} \\end{bmatrix} \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\end{bmatrix} = E \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\end{bmatrix} which has Eigenvalues E_{\\pm} = \\varepsilon_0 + V_{\\textrm{cross}} \\mp t and Eigenvectors \\begin{align} |\\psi_{+}\\rangle = \\tfrac{1}{\\sqrt{2}}(|1\\rangle + |2\\rangle) \\\\ |\\psi_{-}\\rangle = \\tfrac{1}{\\sqrt{2}}(|1\\rangle - |2\\rangle) \\end{align} Bonding vs antibonding \u00b6 In the definition above, we have that \\psi_{+} is symmetric and |\\psi_{-}\\rangle is antisymmetric, so from the node theorem , we can immediately identify that E_+ < E_- , or in a previous terminology, \\psi_{+} is the bonding state and |\\psi_{-}\\rangle is the antibonding state. From the expression for the energy of the states E_{\\pm} = \\varepsilon_0 + V_{\\textrm{cross}} \\mp t by decreasing the interatomic distance, the two atoms get closer and the atomic orbitals have more overlap, resulting in an increased hopping t . Assuming a hopping factor similar in form the potential, we can plot the energies E_{\\pm} as a function of the inter-atomic distance \\Delta x : When an electron occupies the state |\\psi_+\\rangle , the atoms attract (or bond ) because the total energy is lowered. With an electron in the |\\psi_{-}\\rangle state, the molecular energy increases with decreasing interatomic distance and thus the atoms repel each other. Therefore, if each atom has a single electron in the outermost shell, these atoms attract because the bonding orbital hosts two electrons with opposite spins. On the other hand, if each atom has 0 or 2 electrons in the outermost shell, the net force from the bonding and antibonding orbitals cancels out, but Coulomb repulsion remains. Conclusions \u00b6 Electrons in atoms occupy shells, with only electrons in the outermost shell (valence electrons) contributing to interatomic interactions The molecular orbital can be written as a Linear Combination of Atomic Orbitals ( LCAO ) The LCAO method reduces the full Hamiltonian to a finite size problem written in the basis of individual orbitals If two atoms have one orbital and one electron each, they occupy the bonding orbital Exercises \u00b6 Preliminary provocations \u00b6 Is the assumption that the atomic orbitals are orthogonal always a reasonable assumption? What happens if the hopping t is chosen to be negative? How does the size of the Hamiltonian matrix change with the number of atoms? How does the size of the Hamiltonian matrix change if each atom now has two orbitals? Assuming that we have two atoms with a single orbital each, what is the size of the Hamiltonian matrix if we also consider the spin of the electron? Exercise 1: Shell-filling model of atoms \u00b6 Describe the shell-filling model of atoms. Use Madelung\u2019s rule to deduce the atomic shellfilling configuration of the element tungsten, which has atomic number 74. Although Madelung\u2019s rule for the filling of electronic shells holds extremely well, there are a number of exceptions to the rule. Here are a few of them: \\textrm{Cu} = [\\textrm{Ar}] 4\\textrm{s}^1 3\\textrm{d}^{10} \\textrm{Pd} = [\\textrm{Kr}] 5\\textrm{s}^0 4\\textrm{d}^{10} \\textrm{Ag} = [\\textrm{Kr}] 5\\textrm{s}^1 4\\textrm{d}^{10} \\textrm{Au} = [\\textrm{Xe}] 6\\textrm{s}^1 4\\textrm{f}^{14} 5\\textrm{d}^{10} What should the electron configurations be if these elements followed Madelung\u2019s rule and the Aufbau principle? What could be the reason for the deficiency of Madelung's rule? Exercise 2: Application of the LCAO model to the delta-function potential \u00b6 Consider an electron moving in 1D between two negative delta-function shaped potential wells. The complete Hamiltonian of this one-dimensional system is then: \\hat{H} = \\frac{\\hat{p}^2}{2m}-V_0\\delta(x_1-x)-V_0\\delta(x_2-x), where V_0>0 is the potential strength, \\hat{p} the momentum of the electron, and x_1 , x_2 the positions of the potential wells. Properties of a single \\delta -function potential A delta function \\delta(x_0 - x) centered at x_0 is defined to be zero everywhere except for x_0 , where it tends to infinity. Further, a delta function has the property: \\int_{-\\infty}^{\\infty} f(x)\\delta(x_0-x)dx = f(x_0). The procedure to find the energy and a wave function of a state bound in a \\delta -function potential, V=-V_0\\delta(x-x_0) , is similar to that of a quantum well: Assume that we have a bound state with energy E<0 . Compute the wave function \\phi in different regions of space: namely x < x_0 and x > x_0 . Apply the boundary conditions at x = x_0 . The wave function \\phi must be continuous, but d\\phi/dx is not. Instead due to the presence of the delta-function: \\frac{d\\phi}{dx}\\Bigr|_{x_0+\\epsilon} - \\frac{d\\phi}{dx}\\Bigr|_{x_0-\\epsilon}= -\\frac{2mV_0}{\\hbar^2}\\phi(x_0). Find at which energy the boundary conditions at x = x_0 are satisfied. This is the energy of the bound state. Normalize the wave function. Let us apply the LCAO model to solve this problem. Consider the trial wave function for the ground state to be a linear combination of two orbitals |1\\rangle and |2\\rangle : |\\psi\\rangle = \\phi_1|1\\rangle + \\phi_2|2\\rangle. The orbitals |1\\rangle and |2\\rangle correspond to the wave functions of the electron when there is only a single delta peak present: H_1 |1\\rangle = \\epsilon_1 |1\\rangle, H_2 |2\\rangle = \\epsilon_2 |2\\rangle. We start of by calculating the wavefunction of an electron bound to a single delta-peak. To do so, you first need to set up the Schr\u00f6dinger equation of a single electron bound to a single delta-peak. You do not have to solve the Schr\u00f6dinger equation twice\u2014you can use the symmetry of the system to calculate the wavefunction of the other electron bound to the second delta-peak. Find the expressions for the wave functions of the states |1\\rangle and |2\\rangle : \\psi_1(x) and \\psi_2(x) . Also find an expression for their energies \\epsilon_1 and \\epsilon_2 . Remember that you need to normalize the wave functions. Construct the LCAO Hamiltonian. To simplify the calculations, assume that the orbitals are orthogonal. Diagonalize the LCAO Hamiltonian and find an expression for the eigenenergies of the system. It was previously mentioned that V_0>0 . Using this, determine which energy corresponds to the bonding energy. Exercise 3: Polarisation of a hydrogen molecule \u00b6 Consider a hydrogen molecule as a one-dimensional system with two identical nuclei at x=-\\frac{d}{2} and x=+\\frac{d}{2} , so that the center of the molecule is at x=0 . Each atom contains a single electron with charge -e . The LCAO Hamiltonian of the system is given by H_{\\textrm{eff}} = \\begin{pmatrix} E_0&&-t \\\\ -t&&E_0 \\end{pmatrix}. The electric potential is given by V_{\\mathbf{E}}=-\\int_{C} \\mathbf{E} \\cdot \\mathrm{d} \\boldsymbol{\\ell} Let us add an electric field \\mathcal{E} \\hat{\\bf{x}} to the system. Which term needs to be added to the Hamiltonian of each electron? Compute the LCAO Hamiltonian of the system in presence of the electric field. What are the new onsite energies of the two orbitals? Diagonalize the modified LCAO Hamiltonian. Find the ground state wavefunction \\psi . Find the polarisation P of the molecule in the ground state. Reminder: polarisation The polarisation P of a molecule with n\\leq 2 electrons at its ground state |\\psi\\rangle is: P = n e \\langle\\psi|\\hat{x}|\\psi\\rangle. Use that ground state you found in 3.2 is a linear superposition of two orthogonal orbitals centered at -\\frac{d}{2} and +\\frac{d}{2} . For reasons I simply cannot understand, there is often a tension between chemistry and physics: they are both fantastic and any erosion of barriers between the two disciplines is to be commended. \u21a9 In atomic physics, these terms are called the direct and exchange energies, which makes (in my opinion) makes much more sense; however, solid-state physics reserves these names for other things... \u21a9","title":"2: Chemistry 101"},{"location":"2-chemistry/2-1-chemistry/#chemisty-101","text":"Also known as the physicist's guide to chemistry 1 .","title":"Chemisty 101"},{"location":"2-chemistry/2-1-chemistry/#introduction","text":"Our journey thus far has been considering the basic properties of solids, and our descriptions have been based around the properties of electrons and vibrations in solids, which can provide useful results but our endeavour is much grander: we would like to explain all the properties of all the solids. The first step on this journey is to no longer consider a collection of free electrons; solids are made up of many atoms, so we best find a way of baking this into whatever we do! Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Quantum mechanics: the Schr\u00f6dinger equation, wavefunction of the hydrogen atom, wavefunction for a particle in a box, Dirac notation Mathematics: the variational principle, linear algebra Text reference The material covered here is discussed in section(s) \\S 5, 6.3 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"2-chemistry/2-1-chemistry/#a-retrospective","text":"Up to this point, we have: Introduced reciprocal ( k -space) Postulated the dispersion relations for free electrons and oscillations in a solid Calculated the heat capacity of free electrons and oscillations in a solid which has permitted The understanding of how materials can store heat via oscillations (Debye model) The understanding of how free electrons conduct (Drude model) and store heat/energy (Sommerfeld model) In constructing these models, we have made several approximations and postulations, and there are a few big questions to answer: In the Debye model, why is there a cutoff frequency for oscillations? Why are there no modes of oscillation beyond this frequency? In the Drude model, we have electrons modelled as a gas, but why don't electrons scatter from the atoms in solid? Why are some materials metallic, and others not metallic? If we are to have a hope of understanding any of the above, we are going to have to understand better what is happening on the atomic scale.","title":"A retrospective"},{"location":"2-chemistry/2-1-chemistry/#atoms-how-do-they-work","text":"It is no exaggeration to say that everything can be pretty-well described by the Schr\u00f6dinger equation: \\hat{H}\\psi = E\\psi, with \\hat{H} the Hamiltonian of the system, that is, the sum of kinetic and potential energies. For the hydrogen atom, the potential is due to the Coulomb interaction between the electron and the nucleus: H=-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial {\\mathbf r^2}} - \\frac{e^2}{4\\pi\\varepsilon_0|r|} which you will have solved in detail elsewhere. If we move to the next-most-simple atom, helium, the Hamiltonian immediately becomes more complex, containing not just the Coulomb interaction between the individual electrons and the nuclei, but also Coulomb repulsion between the two electrons: H=-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial {\\mathbf r_1^2}} -\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial {\\mathbf r_2^2}}- \\frac{2e^2}{4\\pi\\varepsilon_0|r_1|} - \\frac{2e^2}{4\\pi\\varepsilon_0|r_2|} + \\frac{e^2}{4\\pi\\varepsilon_0|r_1 - r_2|}. From a purely mathematical point of view, this means we need find eigenvalues and eigenvectors of a six-dimensional PDE, which is much tougher work than the three-dimensional PDE descirbing hydrogen. You have likely seen the variational method to construct appoximate wavefunctions, any even in this case it is a fair amount of work. Let us now turn to another system, a single copper atom. Copper has 29 electrons, so to find the energy spectrum of copper we would need to solve an 87-dimensional Schr\u00f6dinger equation. Such things are totally intractable: there is simply no way to solve such a thing, even with really big computers. It is this growth in complexity with the number of interacting quantum particles is why many-body quantum physics is very much an active research area in solid-state physics and quantum chemistry. But we need not have an analytic solution for everything in order to be satisfied, on the contrary, we can use physical insight and approximations to construct models of complex systems based on simpler, easier to handle systems. To some extent, this is exactly where chemisty takes over: it is often said that Chemisty is applied physics, but they should simply be thought of as the same thing, but with a focus on different parts of the spectrum: the complexity of solving problems in chemistry is the reason why we need to accept empirical observations as chemical laws , even though they work with limited precision and ultimately consequences of the underlying physics as modelled by the Schr\u00f6dinger equation.","title":"Atoms, how do they work?"},{"location":"2-chemistry/2-1-chemistry/#quantum-numbers-and-shell-filling","text":"The wavefunction for an electron in a hydrogen atom is described by the state |n, l, m_l, m_s\\rangle , where the quantum numbers are: n = 1, 2, \\ldots is the principal (azimuthal) quantum number l = 0, 1, \\ldots, n-1 is the orbital angular momentum (also known as s, p, d, f orbitals) m_l = -l, -l+1 \\ldots, l is the z -component of angular momentum l m_s is the z -component of the electronic spin The low energy states of hydrogen are shown below: Electronic wavefunctions for the low-energy states of Hydrogen. Image source: Wikipedia \u00a9 Geek3 CC-BY-SA To a first approximation, the electronic wavefunctions in other atoms are similar to those of hydrogen; however, the electron energies are very different due to the Coulomb interaction, both between other electrons and the ionic core. Even still, knowledge of the hydrogenic wavefunctions can greatly aid our description of other atoms. If one considers an atom with multiple electrons, as we \"add\" electrons to a given system - for example, if one considers a sequence of elements - it is important to determine the order in which electronic states will be filled. Two empirical rules which can do an excellent job of describing this order are: Aufbau principle : electrons fill the lowest-lying states first and fill these shells to completion Madelung's rule : electrons first occupy shells with the lowest n+l . If there are several orbitals with equal n+l , electrons preferentially occupy those with smaller n Combining the two rules, we obtain the shell filing order: 1s, 2s, 2p, 3s, 3p, 4s, 3d, etc., which is illustrated in the periodic table below: While these rules accurately predict the electronic structure of most elements, they are only approximate, and fail to describe some of the heavier elements . The reason we care about the order in which shells are filled is because as we have seen, the valence electrons, that is, those in the outermost shell dominate the elements properties, including underpinning chemical reactions and conductivity. Inner-shell electrons acts to shield the charge from the valance electrons - resulting in an altered effective nuclear charge as seen by the valance electrons - but beyond this, they are largely inert.","title":"Quantum numbers and shell filling"},{"location":"2-chemistry/2-1-chemistry/#bonds-the-ties-that-bind","text":"There exist many different mechanisms for bonding, for example: ionic bonding, covalent bonding, bonding through the van der Waals interaction, and the list continues. At its simplest, one can understand bonding as two (or more) systems combining to reduce the collective energy, that is, reaching a state with lower energy than the sum of energies of the individual states.","title":"Bonds: the ties that bind"},{"location":"2-chemistry/2-1-chemistry/#ionic-bonding","text":"The conceptually simplest form of bonding is when an atom that has an electron of which it isn't much fond, meets and atom which really likes collecting electrons. The most common example of this would be the reaction of sodium and chlorine: \\textrm{Na + Cl} \\rightarrow \\textrm{Na}^+ + \\textrm{Cl}^- \\rightarrow \\textrm{NaCl} where one can think of electron transfer from sodium ion to the chlorine ion, rendering sodium and chlorine in their lowest energy states but also leaving them charged, and these ionic species consequently attract and form a compound. To firm up this concept, we can consider the ionisation energy , which is the energy required to remove an electron (and form a positive ion) and the electron affinity , which is the energy associated with the capture of an electron to form a negative ion. Shown below are periodic tables with values for the ionisation energy and the electron affinity indicated. First-ionisation energies for various elements. Image source: LibreTexts which is used under licence CC-BY-NC-SA 3.0 Electron affinity (first electron) for various elements. Image source: LibreTexts which is used under licence CC-BY-NC-SA 3.0 It should be clear that in a situation where an atom A has a low ionisation energy and an atom B with a large electron efficiency, there will be some energy difference between the states of neutral atoms and ions \\Delta E_{A+B \\rightarrow A^+ + B^-} = E_{\\textrm{ionisation},~A} - E_{\\textrm{affinity},~B} It is also important to consider the energy that comes from the attraction between the oppositely charged ionic species E_{\\textrm{cohesive}} = \\Delta E_{A^+ + B^- \\rightarrow AB} which is mostly determined by the Coulomb interaction between the ions. The energy change for the bonding process in then \\Delta E_{A+B \\rightarrow AB} = E_{\\textrm{ionisation},~A} - E_{\\textrm{affinity},~B} - E_{\\textrm{cohesive}} and the reaction will proceed if \\Delta E_{A+B \\rightarrow AB}<0 . The properties of ionic solids are that they tend to have high melting points - the bond is strong! - and they tend to be electrical insulating (something we will discuss later).","title":"Ionic bonding"},{"location":"2-chemistry/2-1-chemistry/#covalent-bonding","text":"Covalent bonding - as the name suggests - involves the sharing of electrons between atoms.","title":"Covalent bonding"},{"location":"2-chemistry/2-1-chemistry/#particles-in-boxes","text":"The simplest way one can model the bonding process is to consider the electronic wavefunctions of the atoms at particles in a box, which is likely the first quantum system you ever discussed in detail. Whilst the picture is not going to produce highly-accurate results, the qualitative behaviour goes a long way to understand the process more generally. Let us consider two identical atoms, where we model the atomic potential as a one-dimensional box with length L , which are far apart to ignore the influence of the other potential, then the energy of each state is given by E = \\frac{\\hbar^2\\pi^2}{2mL^2} If our boxed-like atoms are brought close together such that the boxes merge into one large box of length 2L , the energy of this new bonding state will be given by E = \\frac{\\hbar^2\\pi^2}{2m(2L)^2} If we imagine that our atoms were hydrogenic - that is, with a single valance electron - the electrons from both atoms could occupy this ground state due to the spin degeneracy of the state, and thus a new system with lower energy is formed. It is this interaction which results in Hydrogen existing as a the diatomic molecule \\textrm{H}_2 . Now imagine a system with more electrons, for example, 2 electrons. When these atoms come together, the shared electrons will occupy the lower-energy bonded state, but must also occupy the first excited state, the so-called anti-bonded state which will have energy E = \\frac{2^2\\hbar^2\\pi^2}{2m(2L)^2} Under these circumstances, it is not energetically favourable to a covalent bond, and thus molecules are not formed - see the noble gasses here! Probabilities for electrons in bonding and antibonding states as modelled using square-well potentials","title":"Particles in boxes"},{"location":"2-chemistry/2-1-chemistry/#the-linear-combination-of-atomic-orbitals-lcao","text":"The mouthful that is LCAO is also commonly called molecular orbital or Tight-binding theory, and is a way to quantitatively justify the handwaving from above. Consider a system of two atoms next to each other from which we want to calculate their propensity to form a molecule, that is, calculate the energy eigenstates as a function of the separation of the nuclei. As the nuclei are big and heavy, we are going to assume that their positions are fixed for all positions of the electrons (aka the Born-Oppenheimer approximation). For an individual electron, the Hamiltonian is given by \\hat{H} = \\hat{V}_1 + \\hat{V}_2 + \\hat{K}, where V_1 the potential due to the first nucleus, V_2 due to the second nucleus, and K is the kinetic energy of the electron. Whilst it is possible to solve the Schr\u00f6dinger equation directly, it is a fair bit of work, and we want to have a mechanism for solving these systems more generally, so we will seek a solution using the variational method. We begin by denoting the wave function of an electron bound to the first and second atom |1\\rangle and |2\\rangle respectively, which we know to be eigenfunctions of the systems: \\begin{align} (\\hat{V}_1 + \\hat{K})|1\\rangle = \\varepsilon_0 |1\\rangle \\\\ (\\hat{V}_2 + \\hat{K})|2\\rangle = \\varepsilon_0 |2\\rangle \\end{align} We are going to search for a solution in the form: |\\psi\\rangle = \\phi_{1}|1\\rangle + \\phi_{2}|2\\rangle. where \\phi_{1} and \\phi_{2} are the probability amplitudes of the respective electronic states. The state |\\psi\\rangle is called a molecular orbital because it describes the state of the diatomic molecule, and the molecular orbital is created as a Linear Combination of Atomic Orbitals . To increase the tractability of the problem, we are assume that the atomic orbitals are orthogonal, that is \\langle1|2\\rangle=0 ; however, this is not strictly necessary. We can then write the Schr\u00f6dinger equation for our system in matrix form \\sum_j H_{ij} \\phi_j = \\epsilon\\phi_i where the matrix elements of the Hamiltonian are given by H_{ij} = \\langle i | \\hat{H} | j \\rangle 5.1 Compute the matrix elements of the Hamiltonian \\hat{H} Crank the handle! \\begin{align} H_{11} = \\langle 1 | \\hat{H} | 1 \\rangle = \\langle 1 | \\hat{V}_1 + \\hat{K} | 1 \\rangle + \\langle 1 | \\hat{V}_2 | 1 \\rangle = \\varepsilon_0 + V_{\\textrm{cross}} \\\\ H_{22} = \\langle 2 | \\hat{H} | 2 \\rangle = \\langle 2 | \\hat{V}_2 + \\hat{K} | 2 \\rangle + \\langle 2 | \\hat{V}_1 | 2 \\rangle = \\varepsilon_0 + V_{\\textrm{cross}} \\\\ H_{12} = \\langle 1 | \\hat{H} | 2 \\rangle = \\langle 1 | \\hat{V}_2 + \\hat{K} | 2 \\rangle + \\langle 1 | \\hat{V}_1 | 2 \\rangle = 0 - t \\\\ H_{21} = \\langle 2 | \\hat{H} | 2 \\rangle = \\langle 2 | \\hat{V}_1 + \\hat{K} | 1 \\rangle + \\langle 2 | \\hat{V}_2 | 1 \\rangle = 0 - t* \\end{align} where V_{\\textrm{cross}} = \\langle 1 | \\hat{V}_2 | 1 \\rangle = \\langle 2 | \\hat{V}_2 | 2 \\rangle is the potential felt by state | 1 \\rangle due to nucleus 2 and vice versa, and the term t = - \\langle 1 | \\hat{V}_1 | 2 \\rangle = - \\langle 2 | \\hat{V}_2 | 1 \\rangle is the energy associated with a movement between states | 1 \\rangle and | 2 \\rangle The matrix is thus defined by two parameters, the onsite energy which gives the energy of an electron occupying either of the orbitals, and the so-called hopping integral which is the energy associated with the exchange of the electronic states 2 . 5.2 Using the matrix computed above, calculate the energy eigenstates for the system and their energies From above, we have the Schr\u00f6dinger equation in the form \\begin{bmatrix} \\varepsilon_0 + V_{\\textrm{cross}} & -t \\\\ -t* & \\varepsilon_0 + V_{\\textrm{cross}} \\end{bmatrix} \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\end{bmatrix} = E \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\end{bmatrix} which has Eigenvalues E_{\\pm} = \\varepsilon_0 + V_{\\textrm{cross}} \\mp t and Eigenvectors \\begin{align} |\\psi_{+}\\rangle = \\tfrac{1}{\\sqrt{2}}(|1\\rangle + |2\\rangle) \\\\ |\\psi_{-}\\rangle = \\tfrac{1}{\\sqrt{2}}(|1\\rangle - |2\\rangle) \\end{align}","title":"The linear combination of atomic orbitals (LCAO)"},{"location":"2-chemistry/2-1-chemistry/#bonding-vs-antibonding","text":"In the definition above, we have that \\psi_{+} is symmetric and |\\psi_{-}\\rangle is antisymmetric, so from the node theorem , we can immediately identify that E_+ < E_- , or in a previous terminology, \\psi_{+} is the bonding state and |\\psi_{-}\\rangle is the antibonding state. From the expression for the energy of the states E_{\\pm} = \\varepsilon_0 + V_{\\textrm{cross}} \\mp t by decreasing the interatomic distance, the two atoms get closer and the atomic orbitals have more overlap, resulting in an increased hopping t . Assuming a hopping factor similar in form the potential, we can plot the energies E_{\\pm} as a function of the inter-atomic distance \\Delta x : When an electron occupies the state |\\psi_+\\rangle , the atoms attract (or bond ) because the total energy is lowered. With an electron in the |\\psi_{-}\\rangle state, the molecular energy increases with decreasing interatomic distance and thus the atoms repel each other. Therefore, if each atom has a single electron in the outermost shell, these atoms attract because the bonding orbital hosts two electrons with opposite spins. On the other hand, if each atom has 0 or 2 electrons in the outermost shell, the net force from the bonding and antibonding orbitals cancels out, but Coulomb repulsion remains.","title":"Bonding vs antibonding"},{"location":"2-chemistry/2-1-chemistry/#conclusions","text":"Electrons in atoms occupy shells, with only electrons in the outermost shell (valence electrons) contributing to interatomic interactions The molecular orbital can be written as a Linear Combination of Atomic Orbitals ( LCAO ) The LCAO method reduces the full Hamiltonian to a finite size problem written in the basis of individual orbitals If two atoms have one orbital and one electron each, they occupy the bonding orbital","title":"Conclusions"},{"location":"2-chemistry/2-1-chemistry/#exercises","text":"","title":"Exercises"},{"location":"2-chemistry/2-1-chemistry/#preliminary-provocations","text":"Is the assumption that the atomic orbitals are orthogonal always a reasonable assumption? What happens if the hopping t is chosen to be negative? How does the size of the Hamiltonian matrix change with the number of atoms? How does the size of the Hamiltonian matrix change if each atom now has two orbitals? Assuming that we have two atoms with a single orbital each, what is the size of the Hamiltonian matrix if we also consider the spin of the electron?","title":"Preliminary provocations"},{"location":"2-chemistry/2-1-chemistry/#exercise-1-shell-filling-model-of-atoms","text":"Describe the shell-filling model of atoms. Use Madelung\u2019s rule to deduce the atomic shellfilling configuration of the element tungsten, which has atomic number 74. Although Madelung\u2019s rule for the filling of electronic shells holds extremely well, there are a number of exceptions to the rule. Here are a few of them: \\textrm{Cu} = [\\textrm{Ar}] 4\\textrm{s}^1 3\\textrm{d}^{10} \\textrm{Pd} = [\\textrm{Kr}] 5\\textrm{s}^0 4\\textrm{d}^{10} \\textrm{Ag} = [\\textrm{Kr}] 5\\textrm{s}^1 4\\textrm{d}^{10} \\textrm{Au} = [\\textrm{Xe}] 6\\textrm{s}^1 4\\textrm{f}^{14} 5\\textrm{d}^{10} What should the electron configurations be if these elements followed Madelung\u2019s rule and the Aufbau principle? What could be the reason for the deficiency of Madelung's rule?","title":"Exercise 1: Shell-filling model of atoms"},{"location":"2-chemistry/2-1-chemistry/#exercise-2-application-of-the-lcao-model-to-the-delta-function-potential","text":"Consider an electron moving in 1D between two negative delta-function shaped potential wells. The complete Hamiltonian of this one-dimensional system is then: \\hat{H} = \\frac{\\hat{p}^2}{2m}-V_0\\delta(x_1-x)-V_0\\delta(x_2-x), where V_0>0 is the potential strength, \\hat{p} the momentum of the electron, and x_1 , x_2 the positions of the potential wells. Properties of a single \\delta -function potential A delta function \\delta(x_0 - x) centered at x_0 is defined to be zero everywhere except for x_0 , where it tends to infinity. Further, a delta function has the property: \\int_{-\\infty}^{\\infty} f(x)\\delta(x_0-x)dx = f(x_0). The procedure to find the energy and a wave function of a state bound in a \\delta -function potential, V=-V_0\\delta(x-x_0) , is similar to that of a quantum well: Assume that we have a bound state with energy E<0 . Compute the wave function \\phi in different regions of space: namely x < x_0 and x > x_0 . Apply the boundary conditions at x = x_0 . The wave function \\phi must be continuous, but d\\phi/dx is not. Instead due to the presence of the delta-function: \\frac{d\\phi}{dx}\\Bigr|_{x_0+\\epsilon} - \\frac{d\\phi}{dx}\\Bigr|_{x_0-\\epsilon}= -\\frac{2mV_0}{\\hbar^2}\\phi(x_0). Find at which energy the boundary conditions at x = x_0 are satisfied. This is the energy of the bound state. Normalize the wave function. Let us apply the LCAO model to solve this problem. Consider the trial wave function for the ground state to be a linear combination of two orbitals |1\\rangle and |2\\rangle : |\\psi\\rangle = \\phi_1|1\\rangle + \\phi_2|2\\rangle. The orbitals |1\\rangle and |2\\rangle correspond to the wave functions of the electron when there is only a single delta peak present: H_1 |1\\rangle = \\epsilon_1 |1\\rangle, H_2 |2\\rangle = \\epsilon_2 |2\\rangle. We start of by calculating the wavefunction of an electron bound to a single delta-peak. To do so, you first need to set up the Schr\u00f6dinger equation of a single electron bound to a single delta-peak. You do not have to solve the Schr\u00f6dinger equation twice\u2014you can use the symmetry of the system to calculate the wavefunction of the other electron bound to the second delta-peak. Find the expressions for the wave functions of the states |1\\rangle and |2\\rangle : \\psi_1(x) and \\psi_2(x) . Also find an expression for their energies \\epsilon_1 and \\epsilon_2 . Remember that you need to normalize the wave functions. Construct the LCAO Hamiltonian. To simplify the calculations, assume that the orbitals are orthogonal. Diagonalize the LCAO Hamiltonian and find an expression for the eigenenergies of the system. It was previously mentioned that V_0>0 . Using this, determine which energy corresponds to the bonding energy.","title":"Exercise 2: Application of the LCAO model to the delta-function potential"},{"location":"2-chemistry/2-1-chemistry/#exercise-3-polarisation-of-a-hydrogen-molecule","text":"Consider a hydrogen molecule as a one-dimensional system with two identical nuclei at x=-\\frac{d}{2} and x=+\\frac{d}{2} , so that the center of the molecule is at x=0 . Each atom contains a single electron with charge -e . The LCAO Hamiltonian of the system is given by H_{\\textrm{eff}} = \\begin{pmatrix} E_0&&-t \\\\ -t&&E_0 \\end{pmatrix}. The electric potential is given by V_{\\mathbf{E}}=-\\int_{C} \\mathbf{E} \\cdot \\mathrm{d} \\boldsymbol{\\ell} Let us add an electric field \\mathcal{E} \\hat{\\bf{x}} to the system. Which term needs to be added to the Hamiltonian of each electron? Compute the LCAO Hamiltonian of the system in presence of the electric field. What are the new onsite energies of the two orbitals? Diagonalize the modified LCAO Hamiltonian. Find the ground state wavefunction \\psi . Find the polarisation P of the molecule in the ground state. Reminder: polarisation The polarisation P of a molecule with n\\leq 2 electrons at its ground state |\\psi\\rangle is: P = n e \\langle\\psi|\\hat{x}|\\psi\\rangle. Use that ground state you found in 3.2 is a linear superposition of two orthogonal orbitals centered at -\\frac{d}{2} and +\\frac{d}{2} . For reasons I simply cannot understand, there is often a tension between chemistry and physics: they are both fantastic and any erosion of barriers between the two disciplines is to be commended. \u21a9 In atomic physics, these terms are called the direct and exchange energies, which makes (in my opinion) makes much more sense; however, solid-state physics reserves these names for other things... \u21a9","title":"Exercise 3: Polarisation of a hydrogen molecule"},{"location":"3-1d/3-1-vibrations/","text":"Vibrations \u00b6 Introduction \u00b6 We can consider our journey thus far a preparation: we have seen how quantum mechanics forced its way into statistical mechanics with much success, but also started a discussion of using quantum mechanics to describe complex systems, rather than just applying a finishing touch of quantum to a classical system. Beginning here, we are going to set out to conquer solid-stead state systems from a quantum standpoint; if someone ever asks: have you studied solid-state physics? it is the content in this, and the following sections, about which they are asking. The concepts are core to understanding basically everything we do from here on out, so strap in and let's get into it! Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Quantum mechanics: Mechanics: Normal modes of an oscillator Mathematics: Series expansions of functions (Maclaurin/Taylor series) Text reference The material covered here is discussed in section(s) \\S 8, 9 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: A program used to visualise oscillations in one dimension ( chainplot as written by Mike Glazer ) can be downloaded here Material properties from the interatomic potential \u00b6 In the previous section, we looked at the energy eigenstates of neighbouring atoms which were sufficiently close to display bonding behaviour. The energy eigenstates \\psi_{+} and |\\psi_{-}\\rangle had energies as a function of atomic separation as shown below: Such a model is a good start, but it is unphysical as the bonding energy does continue to decrease with decreasing interatomic distance, rather, it reaches some minimum value before it starts to increase as the nuclei begin to repel. The exact nature of this repulsion is not so important, but it its existence is extremely important. A more realistic form of the energy for the bonding state is shown below, which includes a repulsion term when the atoms are sufficiently close: The physics in which we are interested, that is, solids, necessitates that we are interested in the regime where bonding occurs, so we are going to be most interested in the region of the energy minimum. To begin, let us consider two atoms that are in equilibrium and at the bottom of the potential well as shown in the image above. We are going to denote the spacing which corresponds to this minimum \\delta x = x_{\\mathrm{eq}} , the equilibrium separation. As the potential is nicely behaved in this region (and indeed everywhere else!), we can then express the potential V \\approx V_0(x_{\\mathrm{eq}}) + \\frac{\\kappa}{2!} (r - x_{\\mathrm{eq}})^2 - \\frac{\\kappa_3}{3!} (r - x_{\\mathrm{eq}})^3 + \\ldots we have deliberately excluded term linear in x_{\\mathrm{eq}} , as its inclusion would remove the minimum! In the usual way, we are only going to consider small displacements around x_{\\mathrm{eq}} and then throw all terms other than the constant and the quadratic in the bin, greatly simplifying our life. 3.1.1: With a potential of the form V(x) = V_0 + a x^2 , what will be the motion of an atom in the potential? Compressibility \u00b6 With our potential approximated as a quadratic, let us consider what happens when we compress or stretch the system. Consider a material with length L ; if the equilibrium distance between atoms is x_{\\mathrm{eq}} , then there are N = L/x_{\\mathrm{eq}} atoms in the material. Now lets change the material to length L + \\delta L , which will lead to a change in the interatomic distance of \\delta x = \\delta L/N . With only small changes in x we can then write F = - \\frac{\\mathrm{d} V}{\\mathrm{d} x} \\Bigr|_{x = x_{\\mathrm{eq}} + \\delta x} = \\kappa a \\frac{\\delta L}{L}. This is a description of the compressibility (or elasticity) of a solid, and from a mechanics, the compressibility \\beta 1 is usually defined through the relation \\beta = -\\frac{1}{V} \\frac{\\partial V}{\\partial P} for a three dimensional system. In the one dimensional case, this simplifies to \\beta = -\\frac{1}{L} \\frac{\\partial L}{\\partial F}= \\frac{1}{\\kappa x_{\\mathrm{eq}}} \\equiv \\frac{1}{\\kappa a} where we have introduced a , the symbol used to denote the distance between identical particles, which in this case is our equilibrium interatomic spacing x_{\\mathrm{eq}} Sound velocity \u00b6 The compressibility \\beta allows us to calculate the speed of sound by using the usual relation between the speed v and the density \\rho , and the bulk modulus B : v = \\sqrt{\\frac{B}{\\rho}} = \\sqrt{\\frac{1}{\\rho \\beta}} and because in one-dimension \\rho = m/a (in 1D), we can predict that v = \\sqrt{\\frac{\\kappa a^2}{m}}. Let us put a pin in this result, as we are about to construct a detailed microscopic model for a solid which will allow for calculation of v , it will be useful to compare the results. Vibrations in one dimension \u00b6 To emphasize the similarities and the differences between electrons and phonons, we will deal with both types of particles at once. Up to this point, we have seen the Boltzmann, Einstein, and Debye models of solids, which at the core are models of vibrations in a solid. In the case of Debye, the language around this was much more explicit; however, in all cases, we were considering the motion of the constituent atoms and how ultimately, this allowed for the storage of energy. Here we are going to construct an atomic-scale model of an atomic lattice with an attempt to better understand both the successes and failures of these early models of solids An infinite one-dimensional chain of identical atoms \u00b6 We begin by considering the amalgamation of the concepts ripped from physicists' bingo , namely: a low-dimensional model, and infinite model, a model stripped of unnecessary complexity, and a model centred around a harmonic potential. But in all seriousness, the model is incredibly rich and goes a long way to understanding solids! Consider a chain of identical atoms of mass m with equilibrium spacing a , connected to nearest-neighbours by springs with spring constant \\kappa . Hopefully it is clear from the discussion above, that by using a spring model, we are explicitly using a Harmonic potential which we assume well-approximates the inter-atomic potential for bound systems and small oscillation amplitudes, that is, low temperature. We define the position of the n^{\\textrm{th}} atom to be x_n and the equilibrium position of the n^{\\textrm{th}} atom to be x_{n, \\textrm{eq}}=na . We then write the displacement of an atom from its equilibrium position \\delta x_n = x_n - x_{n, \\textrm{eq}}. The potential energy for this harmonic chain is then the sum over all harmonic potentials \\begin{align} V & = \\sum_i V(x_{i+1}-x_i) = \\sum_i \\kappa/2 (x_{i+1}-x_i-a)^2 \\\\ & = \\sum_i \\kappa/2 (\\delta x_{i+1}- \\delta x_i)^2 \\end{align} from which we can write the force on atom n as F_n = - \\frac{\\partial V}{\\partial x_n} = \\kappa (\\delta x_{n+1}- \\delta x_n) + \\kappa (\\delta x_{n-1}- \\delta x_n) which can be expressed as a 2^{\\mathrm{nd}} -order ODE in \\delta x_n : m (\\ddot{\\delta x_n}) = \\kappa (\\delta x_{n+1} + \\delta x_{n-1} - 2\\delta x_n). With systems such as this, one is usually interested in the normal modes of the system: oscillations where all particles move with the same frequency. Given we are persuing vibrations in the systems this is a worthwhile thing in which to be interested, but moreover, there is a deep connection between normal modes of a classical system and the energy Eigenstates of the equivalent quantum system, so let's consider progress in this direction an down payment of sorts. Given that the structure of the equation of motion is the identical no matter what value of n we choose, and since these equations define the solutions, one can reason that the solutions should also be independent of the choice of n . This, combined with us actively looking for wave solutions, leads us to assume solutions in the form of plane waves, with the same amplitude for each atom: \\delta x_n = A e^{i \\omega t - i k x_{n, \\textrm{eq}}} = A e^{i \\omega t - i k n a} 3.1.2: Assuming plain-wave solutions, show that \\omega = 2 \\sqrt{\\kappa/m} |\\sin(ka/2)| It is worth stopping to emphasise that we now have a non-trivial dispersion relation \\omega = 2 \\sqrt{\\kappa/m} |\\sin(ka/2)| which in contrast to that we have seen previously in the Debye model, \\omega = v_s |k| , is very different. 3.1.3: What can one say about the relationship between the dispersion relationship for a 1D chain of oscillators and that of the Debye model? The reciprocal lattice \u00b6 Let's go ahead an plot the dispersion relation: where we can see that the dispersion relationship is periodic . This is clearly evident from the functional form of the relation, but just let that idea marinate for a minute: the dispersion, that is, the relationship between the oscillation frequency \\omega and k is periodic. What does that even mean? Well in the same way that we saw periodic boundary conditions resulting an a discretisation of k- space in the Debye model, a more general principle is that if a system is period in real space (that is, in position space) with a periodicity a , it will also be periodic in k- space with periodicity 2\\pi/a . Mathematically, it is clear why this happens, but what is the physical interpretation? Let's start by taking advantage of this periodicity by considering only the \"unit cell\" - the periodic unit in reciprocal ( k- ) space - which is called the Brillouin zone . A plot of the Brillouin zone for the dispersion curve above is shown below: As the dispersion is periodic with period 2\\pi/a , we need only look at the Brillouin zone to understand how a material behaver for all k . But how does this all work? For example, consider our oscillation modes \\delta x_n = A e^{i \\omega t - i k n a} 3.1.4: What happens to a given mode of oscillation under the transformation k \\rightarrow k + 2\\pi/a ? Given the periodicity of k space, we can think about the set of points which are equivalent to the point k=0 ( k = \\pm n \\times 2\\pi/a ), which is called the reciprocal lattice , in contrast to the real-space lattice x_n = n a . A useful observation for all points G_m in the reciprocal lattice and points x_n in the real-space lattice is that e^{i G_m x_n} = 1 BUT WHAT IS ACTUALLY GOING ON?!?! \u00b6 If we think about life outside of periodic systems, for example, the propagation of light in a vacuum, one of our favourite relationships is c = \\frac{\\omega}{k} So what does it mean to talk about a (phase) velocity when k is periodic? And what about the wavelength? We are pretty fond on the relation \\lambda = \\frac{2\\pi}{k}, but if k and k + n \\times 2\\pi/a are equivalent, what is heck is going on? 3.1.5: What the heck is going on? Counting normal modes \u00b6 As is (hopefully) becoming a normal question to ask when we consider modes of oscillation: how many modes are there? Well let us consider a chain of N masses, and once again we are going to assume periodic boundary conditions, with the same justification as last time, namely that we are interested in bulk quantities and we will make sure that we don't take look at things happening near boundaries. In a periodic system, we can consider the chain wrapped around back onto itself, such that e^{i\\omega t - ikna} = e^{i\\omega t - ik(N+n)a} which enforces the relation e^{ikNa} = 1 and subsequently restricts the possible values of k : k = \\frac{2\\pi q}{N a} = \\frac{2\\pi q}{L} \\textrm{ where } q \\in \\mathbb{Z}. This is exactly as we saw with the Debye model , where the imposition of periodic boundary conditions discretised reciprocal space. There we saw that points were equally spaced with a separation of 2\\pi/L , which in this case is equivalent to 2\\pi/Na . Now, to count all the modes, we can simply compute the ration of the possible values of k by the spacing between modes. As discussed above, we need only consider the Brillouin zone, as the modes are periodic in k with a period of 2\\pi/a , therefore the number of modes is \\frac{2\\pi/a}{2\\pi/Na} = N which again, is what Debye had predicted - although he just plucked the result from the air - whereas we now have a grounding to say that there is one normal mode per mass in the system. \"Quantum vibrations\" \u00b6 Our discussion up to this point has been purely classical, but thanks to firm foundations, mapping our classical system onto the equivalent quantum system is not so difficult. Explicitly, classical harmonic systems - and thus normal modes - map directly onto \"equivalent\" quantum systems, and a normal mode of oscillation at frequency \\omega will have eigenstates with energy E_n = \\hbar \\omega \\left(n + \\frac{1}{2}\\right). This means that for a given mode with wavevector k , there are multiple energy eigenstates, each separated in energy by \\hbar\\omega(k) . We can now start to think of excitations (and de-excitations) of this particular mode, a quantum of energy \\hbar\\omega(k) , which are known as phonons . Phonons are much like photons - especially in the under the formalism of second quantisation - but obviously are not quanta of the electromagnetic field, but rather the oscillation modes of a solid. In the same way phonons can occupy the same state - i.e. they are bosons - phonons are also bosons, so we can describe the occupation of a given mode though the Bose factor n_\\mathrm{B} . Therefore we can write an expression for the energy associated with the wavevector k as E_k = \\hbar\\omega(k)\\left(n_\\mathrm{B}(\\beta\\hbar\\omega(k))+\\frac{1}{2}\\right) 3.1.6: Use the energy E_k to get obtain an expression for the total energy in the system in terms of an integral over k With this, we now have a well-formulated quantum system from which we can calculate quantities of interest, but also extend to model systems that are not just infinite chains of identical particles. Conclusions \u00b6 The interatomic potential describes the compressibility of a material A system which is periodic in real space with period a in periodic in reciprocal space with period 2\\pi/a The Brillouin zone contains all values of k , as modes separated by 2\\pi/a are identical A normal modes of frequency \\omega is mapped to the eigenstate with energy $E_n = \\hbar \\omega \\left( n + 1/2 \\right) Exercises \u00b6 Preliminary provocations \u00b6 What is the motion of adjacent masses when the chain is oscillating at the its maximum frequency? Exercise 1: Lennard-Jones potential \u00b6 A simple model approximating the interaction between a pair of noble gas atoms such as Argon is the Lennard-Jones potential , in which the potential energy as a function of interatomic distance is U(r) = 4\\epsilon \\left[\\big(\\frac{\\sigma}{r}\\big)^{12}-\\big(\\frac{\\sigma}{r}\\big)^6\\right] where r is the distance between two atoms, \\epsilon is the depth of the potential well, and \\sigma is the distance at which the inter-particle potential is zero. Sketch U(r) as a function of interatomic distance and mark the regions of repulsive and attractive forces acting between the atoms. Find the distance, r_0 (bond length), at which the potential energy is minimal and find the value of the potential energy at this distance (binding energy of the molecule). Expand U(r) in a Taylor series around r_0 up to second order. By considering a second-order (=harmonic) potential approximation around the minimum ( r_0 ), find an expression for the spring constant, \\kappa , in terms of \\epsilon and \\sigma . Using the spring constant \\kappa you found earlier, find the ground state energy of the molecule by comparing the molecule to a quantum harmonic oscillator. What is the energy required to break the molecule apart? What is the approximate number of phonons that can occupy this mode before the potential becomes anharmonic? Hint Because the diatomic molecule is modelled as a one-body problem (in the center of mass rest frame of the molecule), the mass should be replaced by the reduced mass . Exercise 2: Vibrational heat capacity of a 1D monatomic chain \u00b6 Give an integral expression for the heat capacity C . Compute the heat capacity numerically, using e.g. Python. Do the same for C in the Debye model and compare the two. What differences do you see? Exercise 3: A finite chain \u00b6 Consider a chain of only 3 atoms. We can then write the equations of motion \\begin{aligned} m \\ddot{\\delta x}_1 &= - \\kappa (\\delta x_1 - \\delta x_2) \\\\ m \\ddot{\\delta x}_2 &= - \\kappa (\\delta x_2 - \\delta x_1) - \\kappa (\\delta x_2 - \\delta x_3) \\\\ m \\ddot{\\delta x}_3 &= - \\kappa (\\delta x_3 - \\delta x_2) \\end{aligned}, and write this system of equations in matrix form m \\ddot{\\mathbf{u}} = -\\kappa \\begin{pmatrix} 1 & -1 & 0\\\\ -1 & 2 & -1\\\\ 0 & -1 & 1 \\end{pmatrix}\\mathbf{u} Hint In Python use the function numpy.diag Define a matrix that relates forces to displacements in a linear 1D chain containing N=5 atoms. Repeat for N=200 . You may assume that the masses and spring constants are equivalent throughout the chain. Using numerical diagonalization ( numpy.linalg.eigvalsh ), compute the eigenfrequencies of this atomic chain. Plot a histogram of these eigenfrequencies. Make the masses of every even atom different from the masses of every odd atom. Compute the eigenfrequencies of this atomic chain and plot a histogram. Now make the masses of even and odd atoms equivalent again. Furthermore, make the spring constants of every even spring different from the odd spring. Compute the eigenfrequencies of this atomic chain and plot a histogram. It is unfortunately that there are only so many letters in the Greek alphabet, especially when looking at mechanical and thermodynamic quantities! \u21a9","title":"3.1: Vibrations"},{"location":"3-1d/3-1-vibrations/#vibrations","text":"","title":"Vibrations"},{"location":"3-1d/3-1-vibrations/#introduction","text":"We can consider our journey thus far a preparation: we have seen how quantum mechanics forced its way into statistical mechanics with much success, but also started a discussion of using quantum mechanics to describe complex systems, rather than just applying a finishing touch of quantum to a classical system. Beginning here, we are going to set out to conquer solid-stead state systems from a quantum standpoint; if someone ever asks: have you studied solid-state physics? it is the content in this, and the following sections, about which they are asking. The concepts are core to understanding basically everything we do from here on out, so strap in and let's get into it! Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Quantum mechanics: Mechanics: Normal modes of an oscillator Mathematics: Series expansions of functions (Maclaurin/Taylor series) Text reference The material covered here is discussed in section(s) \\S 8, 9 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: A program used to visualise oscillations in one dimension ( chainplot as written by Mike Glazer ) can be downloaded here","title":"Introduction"},{"location":"3-1d/3-1-vibrations/#material-properties-from-the-interatomic-potential","text":"In the previous section, we looked at the energy eigenstates of neighbouring atoms which were sufficiently close to display bonding behaviour. The energy eigenstates \\psi_{+} and |\\psi_{-}\\rangle had energies as a function of atomic separation as shown below: Such a model is a good start, but it is unphysical as the bonding energy does continue to decrease with decreasing interatomic distance, rather, it reaches some minimum value before it starts to increase as the nuclei begin to repel. The exact nature of this repulsion is not so important, but it its existence is extremely important. A more realistic form of the energy for the bonding state is shown below, which includes a repulsion term when the atoms are sufficiently close: The physics in which we are interested, that is, solids, necessitates that we are interested in the regime where bonding occurs, so we are going to be most interested in the region of the energy minimum. To begin, let us consider two atoms that are in equilibrium and at the bottom of the potential well as shown in the image above. We are going to denote the spacing which corresponds to this minimum \\delta x = x_{\\mathrm{eq}} , the equilibrium separation. As the potential is nicely behaved in this region (and indeed everywhere else!), we can then express the potential V \\approx V_0(x_{\\mathrm{eq}}) + \\frac{\\kappa}{2!} (r - x_{\\mathrm{eq}})^2 - \\frac{\\kappa_3}{3!} (r - x_{\\mathrm{eq}})^3 + \\ldots we have deliberately excluded term linear in x_{\\mathrm{eq}} , as its inclusion would remove the minimum! In the usual way, we are only going to consider small displacements around x_{\\mathrm{eq}} and then throw all terms other than the constant and the quadratic in the bin, greatly simplifying our life. 3.1.1: With a potential of the form V(x) = V_0 + a x^2 , what will be the motion of an atom in the potential?","title":"Material properties from the interatomic potential"},{"location":"3-1d/3-1-vibrations/#compressibility","text":"With our potential approximated as a quadratic, let us consider what happens when we compress or stretch the system. Consider a material with length L ; if the equilibrium distance between atoms is x_{\\mathrm{eq}} , then there are N = L/x_{\\mathrm{eq}} atoms in the material. Now lets change the material to length L + \\delta L , which will lead to a change in the interatomic distance of \\delta x = \\delta L/N . With only small changes in x we can then write F = - \\frac{\\mathrm{d} V}{\\mathrm{d} x} \\Bigr|_{x = x_{\\mathrm{eq}} + \\delta x} = \\kappa a \\frac{\\delta L}{L}. This is a description of the compressibility (or elasticity) of a solid, and from a mechanics, the compressibility \\beta 1 is usually defined through the relation \\beta = -\\frac{1}{V} \\frac{\\partial V}{\\partial P} for a three dimensional system. In the one dimensional case, this simplifies to \\beta = -\\frac{1}{L} \\frac{\\partial L}{\\partial F}= \\frac{1}{\\kappa x_{\\mathrm{eq}}} \\equiv \\frac{1}{\\kappa a} where we have introduced a , the symbol used to denote the distance between identical particles, which in this case is our equilibrium interatomic spacing x_{\\mathrm{eq}}","title":"Compressibility"},{"location":"3-1d/3-1-vibrations/#sound-velocity","text":"The compressibility \\beta allows us to calculate the speed of sound by using the usual relation between the speed v and the density \\rho , and the bulk modulus B : v = \\sqrt{\\frac{B}{\\rho}} = \\sqrt{\\frac{1}{\\rho \\beta}} and because in one-dimension \\rho = m/a (in 1D), we can predict that v = \\sqrt{\\frac{\\kappa a^2}{m}}. Let us put a pin in this result, as we are about to construct a detailed microscopic model for a solid which will allow for calculation of v , it will be useful to compare the results.","title":"Sound velocity"},{"location":"3-1d/3-1-vibrations/#vibrations-in-one-dimension","text":"To emphasize the similarities and the differences between electrons and phonons, we will deal with both types of particles at once. Up to this point, we have seen the Boltzmann, Einstein, and Debye models of solids, which at the core are models of vibrations in a solid. In the case of Debye, the language around this was much more explicit; however, in all cases, we were considering the motion of the constituent atoms and how ultimately, this allowed for the storage of energy. Here we are going to construct an atomic-scale model of an atomic lattice with an attempt to better understand both the successes and failures of these early models of solids","title":"Vibrations in one dimension"},{"location":"3-1d/3-1-vibrations/#an-infinite-one-dimensional-chain-of-identical-atoms","text":"We begin by considering the amalgamation of the concepts ripped from physicists' bingo , namely: a low-dimensional model, and infinite model, a model stripped of unnecessary complexity, and a model centred around a harmonic potential. But in all seriousness, the model is incredibly rich and goes a long way to understanding solids! Consider a chain of identical atoms of mass m with equilibrium spacing a , connected to nearest-neighbours by springs with spring constant \\kappa . Hopefully it is clear from the discussion above, that by using a spring model, we are explicitly using a Harmonic potential which we assume well-approximates the inter-atomic potential for bound systems and small oscillation amplitudes, that is, low temperature. We define the position of the n^{\\textrm{th}} atom to be x_n and the equilibrium position of the n^{\\textrm{th}} atom to be x_{n, \\textrm{eq}}=na . We then write the displacement of an atom from its equilibrium position \\delta x_n = x_n - x_{n, \\textrm{eq}}. The potential energy for this harmonic chain is then the sum over all harmonic potentials \\begin{align} V & = \\sum_i V(x_{i+1}-x_i) = \\sum_i \\kappa/2 (x_{i+1}-x_i-a)^2 \\\\ & = \\sum_i \\kappa/2 (\\delta x_{i+1}- \\delta x_i)^2 \\end{align} from which we can write the force on atom n as F_n = - \\frac{\\partial V}{\\partial x_n} = \\kappa (\\delta x_{n+1}- \\delta x_n) + \\kappa (\\delta x_{n-1}- \\delta x_n) which can be expressed as a 2^{\\mathrm{nd}} -order ODE in \\delta x_n : m (\\ddot{\\delta x_n}) = \\kappa (\\delta x_{n+1} + \\delta x_{n-1} - 2\\delta x_n). With systems such as this, one is usually interested in the normal modes of the system: oscillations where all particles move with the same frequency. Given we are persuing vibrations in the systems this is a worthwhile thing in which to be interested, but moreover, there is a deep connection between normal modes of a classical system and the energy Eigenstates of the equivalent quantum system, so let's consider progress in this direction an down payment of sorts. Given that the structure of the equation of motion is the identical no matter what value of n we choose, and since these equations define the solutions, one can reason that the solutions should also be independent of the choice of n . This, combined with us actively looking for wave solutions, leads us to assume solutions in the form of plane waves, with the same amplitude for each atom: \\delta x_n = A e^{i \\omega t - i k x_{n, \\textrm{eq}}} = A e^{i \\omega t - i k n a} 3.1.2: Assuming plain-wave solutions, show that \\omega = 2 \\sqrt{\\kappa/m} |\\sin(ka/2)| It is worth stopping to emphasise that we now have a non-trivial dispersion relation \\omega = 2 \\sqrt{\\kappa/m} |\\sin(ka/2)| which in contrast to that we have seen previously in the Debye model, \\omega = v_s |k| , is very different. 3.1.3: What can one say about the relationship between the dispersion relationship for a 1D chain of oscillators and that of the Debye model?","title":"An infinite one-dimensional chain of identical atoms"},{"location":"3-1d/3-1-vibrations/#the-reciprocal-lattice","text":"Let's go ahead an plot the dispersion relation: where we can see that the dispersion relationship is periodic . This is clearly evident from the functional form of the relation, but just let that idea marinate for a minute: the dispersion, that is, the relationship between the oscillation frequency \\omega and k is periodic. What does that even mean? Well in the same way that we saw periodic boundary conditions resulting an a discretisation of k- space in the Debye model, a more general principle is that if a system is period in real space (that is, in position space) with a periodicity a , it will also be periodic in k- space with periodicity 2\\pi/a . Mathematically, it is clear why this happens, but what is the physical interpretation? Let's start by taking advantage of this periodicity by considering only the \"unit cell\" - the periodic unit in reciprocal ( k- ) space - which is called the Brillouin zone . A plot of the Brillouin zone for the dispersion curve above is shown below: As the dispersion is periodic with period 2\\pi/a , we need only look at the Brillouin zone to understand how a material behaver for all k . But how does this all work? For example, consider our oscillation modes \\delta x_n = A e^{i \\omega t - i k n a} 3.1.4: What happens to a given mode of oscillation under the transformation k \\rightarrow k + 2\\pi/a ? Given the periodicity of k space, we can think about the set of points which are equivalent to the point k=0 ( k = \\pm n \\times 2\\pi/a ), which is called the reciprocal lattice , in contrast to the real-space lattice x_n = n a . A useful observation for all points G_m in the reciprocal lattice and points x_n in the real-space lattice is that e^{i G_m x_n} = 1","title":"The reciprocal lattice"},{"location":"3-1d/3-1-vibrations/#but-what-is-actually-going-on","text":"If we think about life outside of periodic systems, for example, the propagation of light in a vacuum, one of our favourite relationships is c = \\frac{\\omega}{k} So what does it mean to talk about a (phase) velocity when k is periodic? And what about the wavelength? We are pretty fond on the relation \\lambda = \\frac{2\\pi}{k}, but if k and k + n \\times 2\\pi/a are equivalent, what is heck is going on? 3.1.5: What the heck is going on?","title":"BUT WHAT IS ACTUALLY GOING ON?!?!"},{"location":"3-1d/3-1-vibrations/#counting-normal-modes","text":"As is (hopefully) becoming a normal question to ask when we consider modes of oscillation: how many modes are there? Well let us consider a chain of N masses, and once again we are going to assume periodic boundary conditions, with the same justification as last time, namely that we are interested in bulk quantities and we will make sure that we don't take look at things happening near boundaries. In a periodic system, we can consider the chain wrapped around back onto itself, such that e^{i\\omega t - ikna} = e^{i\\omega t - ik(N+n)a} which enforces the relation e^{ikNa} = 1 and subsequently restricts the possible values of k : k = \\frac{2\\pi q}{N a} = \\frac{2\\pi q}{L} \\textrm{ where } q \\in \\mathbb{Z}. This is exactly as we saw with the Debye model , where the imposition of periodic boundary conditions discretised reciprocal space. There we saw that points were equally spaced with a separation of 2\\pi/L , which in this case is equivalent to 2\\pi/Na . Now, to count all the modes, we can simply compute the ration of the possible values of k by the spacing between modes. As discussed above, we need only consider the Brillouin zone, as the modes are periodic in k with a period of 2\\pi/a , therefore the number of modes is \\frac{2\\pi/a}{2\\pi/Na} = N which again, is what Debye had predicted - although he just plucked the result from the air - whereas we now have a grounding to say that there is one normal mode per mass in the system.","title":"Counting normal modes"},{"location":"3-1d/3-1-vibrations/#quantum-vibrations","text":"Our discussion up to this point has been purely classical, but thanks to firm foundations, mapping our classical system onto the equivalent quantum system is not so difficult. Explicitly, classical harmonic systems - and thus normal modes - map directly onto \"equivalent\" quantum systems, and a normal mode of oscillation at frequency \\omega will have eigenstates with energy E_n = \\hbar \\omega \\left(n + \\frac{1}{2}\\right). This means that for a given mode with wavevector k , there are multiple energy eigenstates, each separated in energy by \\hbar\\omega(k) . We can now start to think of excitations (and de-excitations) of this particular mode, a quantum of energy \\hbar\\omega(k) , which are known as phonons . Phonons are much like photons - especially in the under the formalism of second quantisation - but obviously are not quanta of the electromagnetic field, but rather the oscillation modes of a solid. In the same way phonons can occupy the same state - i.e. they are bosons - phonons are also bosons, so we can describe the occupation of a given mode though the Bose factor n_\\mathrm{B} . Therefore we can write an expression for the energy associated with the wavevector k as E_k = \\hbar\\omega(k)\\left(n_\\mathrm{B}(\\beta\\hbar\\omega(k))+\\frac{1}{2}\\right) 3.1.6: Use the energy E_k to get obtain an expression for the total energy in the system in terms of an integral over k With this, we now have a well-formulated quantum system from which we can calculate quantities of interest, but also extend to model systems that are not just infinite chains of identical particles.","title":"\"Quantum vibrations\""},{"location":"3-1d/3-1-vibrations/#conclusions","text":"The interatomic potential describes the compressibility of a material A system which is periodic in real space with period a in periodic in reciprocal space with period 2\\pi/a The Brillouin zone contains all values of k , as modes separated by 2\\pi/a are identical A normal modes of frequency \\omega is mapped to the eigenstate with energy $E_n = \\hbar \\omega \\left( n + 1/2 \\right)","title":"Conclusions"},{"location":"3-1d/3-1-vibrations/#exercises","text":"","title":"Exercises"},{"location":"3-1d/3-1-vibrations/#preliminary-provocations","text":"What is the motion of adjacent masses when the chain is oscillating at the its maximum frequency?","title":"Preliminary provocations"},{"location":"3-1d/3-1-vibrations/#exercise-1-lennard-jones-potential","text":"A simple model approximating the interaction between a pair of noble gas atoms such as Argon is the Lennard-Jones potential , in which the potential energy as a function of interatomic distance is U(r) = 4\\epsilon \\left[\\big(\\frac{\\sigma}{r}\\big)^{12}-\\big(\\frac{\\sigma}{r}\\big)^6\\right] where r is the distance between two atoms, \\epsilon is the depth of the potential well, and \\sigma is the distance at which the inter-particle potential is zero. Sketch U(r) as a function of interatomic distance and mark the regions of repulsive and attractive forces acting between the atoms. Find the distance, r_0 (bond length), at which the potential energy is minimal and find the value of the potential energy at this distance (binding energy of the molecule). Expand U(r) in a Taylor series around r_0 up to second order. By considering a second-order (=harmonic) potential approximation around the minimum ( r_0 ), find an expression for the spring constant, \\kappa , in terms of \\epsilon and \\sigma . Using the spring constant \\kappa you found earlier, find the ground state energy of the molecule by comparing the molecule to a quantum harmonic oscillator. What is the energy required to break the molecule apart? What is the approximate number of phonons that can occupy this mode before the potential becomes anharmonic? Hint Because the diatomic molecule is modelled as a one-body problem (in the center of mass rest frame of the molecule), the mass should be replaced by the reduced mass .","title":"Exercise 1: Lennard-Jones potential"},{"location":"3-1d/3-1-vibrations/#exercise-2-vibrational-heat-capacity-of-a-1d-monatomic-chain","text":"Give an integral expression for the heat capacity C . Compute the heat capacity numerically, using e.g. Python. Do the same for C in the Debye model and compare the two. What differences do you see?","title":"Exercise 2: Vibrational heat capacity of a 1D monatomic chain"},{"location":"3-1d/3-1-vibrations/#exercise-3-a-finite-chain","text":"Consider a chain of only 3 atoms. We can then write the equations of motion \\begin{aligned} m \\ddot{\\delta x}_1 &= - \\kappa (\\delta x_1 - \\delta x_2) \\\\ m \\ddot{\\delta x}_2 &= - \\kappa (\\delta x_2 - \\delta x_1) - \\kappa (\\delta x_2 - \\delta x_3) \\\\ m \\ddot{\\delta x}_3 &= - \\kappa (\\delta x_3 - \\delta x_2) \\end{aligned}, and write this system of equations in matrix form m \\ddot{\\mathbf{u}} = -\\kappa \\begin{pmatrix} 1 & -1 & 0\\\\ -1 & 2 & -1\\\\ 0 & -1 & 1 \\end{pmatrix}\\mathbf{u} Hint In Python use the function numpy.diag Define a matrix that relates forces to displacements in a linear 1D chain containing N=5 atoms. Repeat for N=200 . You may assume that the masses and spring constants are equivalent throughout the chain. Using numerical diagonalization ( numpy.linalg.eigvalsh ), compute the eigenfrequencies of this atomic chain. Plot a histogram of these eigenfrequencies. Make the masses of every even atom different from the masses of every odd atom. Compute the eigenfrequencies of this atomic chain and plot a histogram. Now make the masses of even and odd atoms equivalent again. Furthermore, make the spring constants of every even spring different from the odd spring. Compute the eigenfrequencies of this atomic chain and plot a histogram. It is unfortunately that there are only so many letters in the Greek alphabet, especially when looking at mechanical and thermodynamic quantities! \u21a9","title":"Exercise 3: A finite chain"},{"location":"3-1d/3-2-diatomic/","text":"The diatomic chain \u00b6 Vibrations in one-dimension with extra pizzazz. Introduction \u00b6 Modelling a solid as an infinite one-dimensional chain of identical atoms went a long way to Accurately predicting the observed behaviour of solids whilst both incorporating ideas from previous models, and reconciling many of their shortcomings Providing a microscopic picture of what is going on, both in a classical and quantum mechanical sense Unfortunately for us, not everything is an infinite 1D chain of the same atom. In order to make our model more realistic and more broadly applicable, we are going to make some changes: let's look at an infinite one-dimensional chain of identical pairs of atoms. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Mathematics: Eigenvalue equations Text reference The material covered here is discussed in section(s) \\S 10 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: A program used to visualise oscillations in one dimension ( chainplot as written by Mike Glazer ) can be downloaded here Expanding the circle of concern \u00b6 In the previous , we modelled vibrations in a solid using a one-dimensional homogeneous chain of atoms. The model gave us insight into the vibrational modes of a solid, and in the quantum mechanical case precipitated the identification of phonons, quanta of vibrations. To expand our model to include more solids, we shall consider a chain of atoms with two distinct particles, but the method we use here is applicable to the inclusion of more particles and interactions. In the case of a diatomic system, the masses of each of the constituent atoms can be different ( m_1 and m_2 ), but as can the harmonic potential between the atoms ( \\kappa_1 and \\kappa_2 ). Following the text, we are going to consider the case of identical masses but varying spring constants as the algebra is ever so slightly simpler, and the qualitative behaviour we are trying to observe is present in all cases. A schematic of the system in shown below: The first observation to make is that the repeated pattern, that is, the unit cell is not longer just one atom, but rather to length which characterises the periodicity is now 2a . We note that within this unit cell, there two degrees of freedom, namely the position of particle x and particle y , which are distinguished by their relative location to harmonic potentials with force constants \\kappa_1 and \\kappa_2 as illustrated above. This is in contrast to the monatomic chain model, whereby there was one degree of freedom per unit cell. Equations of motion \u00b6 Following a similar path as the monatomic case, we begin be writing the equations of motion: \\begin{aligned} m (\\ddot{\\delta x_n}) & = \\kappa_2 (\\delta y_{n} - \\delta x_{n}) + \\kappa_1 (\\delta y_{n-1} - \\delta x_{n}) \\\\ m (\\ddot{\\delta y_n}) & = \\kappa_1 (\\delta x_{n+1} - \\delta y_{n}) + \\kappa_2 (\\delta x_{n} - \\delta y_{n}) \\end{aligned} and employing the same intuition as last time, namely that we expect wave solutions that should have unit cells oscillating at the same frequency, we seek solutions of the form: \\begin{pmatrix} \\delta x_n\\\\ \\delta y_n \\end{pmatrix} = e^{i\\omega t - ik na} \\begin{pmatrix} A_{x}\\\\ A_{y} \\end{pmatrix}. When these solutions are deployed, one obtains the matrix equation m\\omega^2\\begin{pmatrix} A_x\\\\ A_y \\end{pmatrix} = \\begin{pmatrix} \\kappa_1 + \\kappa_2 & -\\kappa_2 - \\kappa_1 e^{ika}\\\\ -\\kappa_2 - \\kappa_1 e^{-ika} & \\kappa_1 + \\kappa_2 \\end{pmatrix} \\begin{pmatrix} A_x\\\\ A_y \\end{pmatrix} 3.2.1: Explicitly verify that the trail solutions lead to the above matrix equation This is an eigenvalue equation, and this can be solved in the usual way to obtain \\omega_\\pm^2 = \\frac{\\kappa_1+\\kappa_2}{m} \\pm \\sqrt{\\frac{(\\kappa_1+\\kappa_2)^2 - 4\\kappa_1\\kappa_2\\sin^2(ka/2)}{m^2}} 3.2.2: Solve the eigenvalue equation and verify the result for \\omega_\\pm Dispersion \u00b6 Immediately we notice that as compared to the monatomic chain, there are now two modes of oscillation for each value of k , and we denote the frequencies associated with the modes \\omega_{\\pm} , as shown in the plot below: Looking at the associated eigenvectors, we can see that these frequencies correspond to in-phase ( \\omega_- ) and out-of-phase ( \\omega_+ ) motion. Just like last time, the plot is periodic in k with values of k shifted by 2 \\pi / a corresponding to the same solution, and therefore look only at the unique values of k , that is, those in the Brillouin zone, shown below: One may ask why the second branch appears, which is an good question, and is understood as with a diatomic system, we have added a second degree of freedom per unit cell. Indeed, had we a unit cell with 3 atoms, that is 3 degrees of freedom, we would find that there are three branches, or three frequencies, for each value of k . Looking at the \\omega_- mode, we notice that as k \\rightarrow 0 , the dispersion relationship is linear. In this regime, the modes behave like sound waves (i.e. \\omega = v |k| ) and consequently, the mode is referred to as the acoustic mode . On the other hand, modes which intersect with optical dispersion modes (i.e. \\omega = c|k| ) are referred to as optical modes . 3.2.3: Plot the dispersion modes, along with an optical dispersion curve for visible light. What are the implications intersecting curves? We can look at the behaviour of the curves at specific values of k , for example at the zone boundary, \\omega_{+}(\\pi/a) = \\sqrt{2\\kappa_1/m} and \\omega_{+}(\\pi/a) = \\sqrt{2\\kappa_2/m} . At this point, it is useful to consider the idea of an extended Brillouin zone: the Brillouin zone houses all unique values of k , but in this case, there are multiple frequencies for each value of k . By extending the Brillouin zone, unfolding the higher-order mode(s) out to a the neighbouring unit cell in reciprocal space, we can obtain a plot which was unique frequencies for each k . A plot illustrating the extended scheme is shown below: But why would we do this? Well, consider the case of \\kappa_1 \\approx \\kappa_2 and the limit of \\kappa_1 = \\kappa_2 . We can clearly observe an \"opening\" between the bands which depends on the values of \\kappa_1 and \\kappa_2 , but physically, what would one expect to happen when \\kappa_1 = \\kappa_2 = \\kappa ? 3.2.4: Make a prediction for what will be the consequences of setting \\kappa_1 = \\kappa_2 and how this relates to the monatomic chain. Hopefully it is clear that with identical spring constants, we are back at the case of a monatomic chain. In the case of the band splitting, this will go to zero as the difference between \\kappa_1 and \\kappa_2 goes to zero: But now we have a bit of a quandary, namely, in order to capture all unique values of k , we now have a plot Brillouin zone that runs from -2\\pi/a to 2\\pi/a (see above), whereas previously this ran from -\\pi/a to \\pi/a . Any what about our number of states? The solution comes in the recognition that in the diatomic case, one has a unit cell length of a , which is 2 \\times a_\\mathrm{monatomic} , so the \"true\" Brillouin zone for the monatomic chain extents from -\\pi/a_\\mathrm{monatomic} to \\pi/a_\\mathrm{monatomic} which is equivalent to -2\\pi/a to 2\\pi/a . Similarly, the number of atoms per unit cell as changed from 2 to 1, meaning that everything doesn't fall apart. YAY! Density of states \u00b6 As we have seen previously, the density of states is g(\\omega)\\textrm{d}\\omega = \\textrm{d}N and thus g(\\omega) = \\frac{\\textrm{d}N}{\\textrm{d}\\omega} = \\frac{L}{2\\pi} \\sum | \\textrm{d}k/\\textrm{d} \\omega| where sum goes over all states at a given energy. In this case, we must ensure that we include the contribution to the DoS from both the positive and negative momenta. Since the energy of this system is symmetric with respect to k , this sum will simply introduce a factor of 2. In an attempt to build an intuition for visualising and understanding the density of states, it is possible to think of the DoS as a histogram of the energy samples drawn from the dispersion relation \u03c9(k) . Remember that by enforcing periodic boundary conditions on our system, we discretise k space with equally spaced points separated by distance 2\\pi/L . Conclusions \u00b6 The normal modes of a diatomic chain of atoms were found by once again intuiting that plane waves in real would be a well-suited solution Systems with more than one degree of freedom per unit cell result in independent oscillation amplitudes for each degree of freedom, leading to multiple bands The density of states can be derived graphically from the dispersion relation Exercises \u00b6 Preliminary provocations \u00b6 Verify that the expression for \\omega^2 is always positive. Why is this important? When calculating the DOS, we only look at the first Brillouin zone. Why? Exercise 1: analysing the diatomic vibrating chain \u00b6 As we have shown, the normal modes of oscillation of a diatomic chain occur at frequencies: \\omega_\\pm^2 = \\frac{\\kappa_1+\\kappa_2}{m} \\pm \\sqrt{\\frac{(\\kappa_1+\\kappa_2)^2 - 4\\kappa_1\\kappa_2\\sin^2(ka/2)}{m^2}} where the plus sign corresponds to the optical branch and the minus sign to the acoustic branch. Hint The final form of \\omega_\\pm as given is not always the most useful: sometimes the complex exponential form - see 3.2.2 - can make life easier Find the magnitude of the group velocity near k=0 for the acoustic branch. Show that the group velocity at k=0 for the optical branch is zero. Derive an expression of the density of states g(\u03c9) for the acoustic branch and small k . Make use of your expression of the group velocity in 1. Compare this expression with that of the derived density of states from exercise 1 of the Debye lecture. Exercise 2: atomic chain with 3 different spring constants \u00b6 Suppose we have a vibrating 1D atomic chain with 3 different spring constants alternating like \\kappa_ 1 , \\kappa_2 , \\kappa_3 , \\kappa_1 , etc. All the atoms in the chain have an equal mass m . Hint To solve the eigenvalue problem quickly, make use of the fact that the mass-spring matrix in that case commutes with the matrix X = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix}. What can be said about eigenvectors of two matrices that commute? Make a sketch of this chain and indicate the length of the unit cell a in this sketch. Derive the equations of motion for this chain. By filling in the trial solutions into the equations of motion (which should be similar to those used in the dual spring constants case), show that the eigenvalue problem is \\omega^2 \\begin{pmatrix} A_1 \\\\ A_2 \\\\ A_3 \\end{pmatrix} = \\frac{1}{m} \\begin{pmatrix} \\kappa_1 + \\kappa_ 3 & -\\kappa_ 1 & -\\kappa_ 3 e^{i k a} \\\\ -\\kappa_ 1 & \\kappa_1+\\kappa_2 & -\\kappa_ 2 \\\\ -\\kappa_ 3 e^{-i k a} & -\\kappa_2 & \\kappa_2 + \\kappa_ 3 \\end{pmatrix} \\begin{pmatrix} A_1 \\\\ A_2 \\\\ A_3 \\end{pmatrix} In general, the eigenvalue problem above cannot be solved analytically, and can only be solved in specific cases. Find the eigenvalues \u03c9^2 when k a = \\pi and \\kappa_1 = \u03ba_2 = q . What will happen to the periodicity of the band structure if \\kappa_ 1 = \\kappa_ 2 = \\kappa_3 ?","title":"3.2: The diatomic chain"},{"location":"3-1d/3-2-diatomic/#the-diatomic-chain","text":"Vibrations in one-dimension with extra pizzazz.","title":"The diatomic chain"},{"location":"3-1d/3-2-diatomic/#introduction","text":"Modelling a solid as an infinite one-dimensional chain of identical atoms went a long way to Accurately predicting the observed behaviour of solids whilst both incorporating ideas from previous models, and reconciling many of their shortcomings Providing a microscopic picture of what is going on, both in a classical and quantum mechanical sense Unfortunately for us, not everything is an infinite 1D chain of the same atom. In order to make our model more realistic and more broadly applicable, we are going to make some changes: let's look at an infinite one-dimensional chain of identical pairs of atoms. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Mathematics: Eigenvalue equations Text reference The material covered here is discussed in section(s) \\S 10 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: A program used to visualise oscillations in one dimension ( chainplot as written by Mike Glazer ) can be downloaded here","title":"Introduction"},{"location":"3-1d/3-2-diatomic/#expanding-the-circle-of-concern","text":"In the previous , we modelled vibrations in a solid using a one-dimensional homogeneous chain of atoms. The model gave us insight into the vibrational modes of a solid, and in the quantum mechanical case precipitated the identification of phonons, quanta of vibrations. To expand our model to include more solids, we shall consider a chain of atoms with two distinct particles, but the method we use here is applicable to the inclusion of more particles and interactions. In the case of a diatomic system, the masses of each of the constituent atoms can be different ( m_1 and m_2 ), but as can the harmonic potential between the atoms ( \\kappa_1 and \\kappa_2 ). Following the text, we are going to consider the case of identical masses but varying spring constants as the algebra is ever so slightly simpler, and the qualitative behaviour we are trying to observe is present in all cases. A schematic of the system in shown below: The first observation to make is that the repeated pattern, that is, the unit cell is not longer just one atom, but rather to length which characterises the periodicity is now 2a . We note that within this unit cell, there two degrees of freedom, namely the position of particle x and particle y , which are distinguished by their relative location to harmonic potentials with force constants \\kappa_1 and \\kappa_2 as illustrated above. This is in contrast to the monatomic chain model, whereby there was one degree of freedom per unit cell.","title":"Expanding the circle of concern"},{"location":"3-1d/3-2-diatomic/#equations-of-motion","text":"Following a similar path as the monatomic case, we begin be writing the equations of motion: \\begin{aligned} m (\\ddot{\\delta x_n}) & = \\kappa_2 (\\delta y_{n} - \\delta x_{n}) + \\kappa_1 (\\delta y_{n-1} - \\delta x_{n}) \\\\ m (\\ddot{\\delta y_n}) & = \\kappa_1 (\\delta x_{n+1} - \\delta y_{n}) + \\kappa_2 (\\delta x_{n} - \\delta y_{n}) \\end{aligned} and employing the same intuition as last time, namely that we expect wave solutions that should have unit cells oscillating at the same frequency, we seek solutions of the form: \\begin{pmatrix} \\delta x_n\\\\ \\delta y_n \\end{pmatrix} = e^{i\\omega t - ik na} \\begin{pmatrix} A_{x}\\\\ A_{y} \\end{pmatrix}. When these solutions are deployed, one obtains the matrix equation m\\omega^2\\begin{pmatrix} A_x\\\\ A_y \\end{pmatrix} = \\begin{pmatrix} \\kappa_1 + \\kappa_2 & -\\kappa_2 - \\kappa_1 e^{ika}\\\\ -\\kappa_2 - \\kappa_1 e^{-ika} & \\kappa_1 + \\kappa_2 \\end{pmatrix} \\begin{pmatrix} A_x\\\\ A_y \\end{pmatrix} 3.2.1: Explicitly verify that the trail solutions lead to the above matrix equation This is an eigenvalue equation, and this can be solved in the usual way to obtain \\omega_\\pm^2 = \\frac{\\kappa_1+\\kappa_2}{m} \\pm \\sqrt{\\frac{(\\kappa_1+\\kappa_2)^2 - 4\\kappa_1\\kappa_2\\sin^2(ka/2)}{m^2}} 3.2.2: Solve the eigenvalue equation and verify the result for \\omega_\\pm","title":"Equations of motion"},{"location":"3-1d/3-2-diatomic/#dispersion","text":"Immediately we notice that as compared to the monatomic chain, there are now two modes of oscillation for each value of k , and we denote the frequencies associated with the modes \\omega_{\\pm} , as shown in the plot below: Looking at the associated eigenvectors, we can see that these frequencies correspond to in-phase ( \\omega_- ) and out-of-phase ( \\omega_+ ) motion. Just like last time, the plot is periodic in k with values of k shifted by 2 \\pi / a corresponding to the same solution, and therefore look only at the unique values of k , that is, those in the Brillouin zone, shown below: One may ask why the second branch appears, which is an good question, and is understood as with a diatomic system, we have added a second degree of freedom per unit cell. Indeed, had we a unit cell with 3 atoms, that is 3 degrees of freedom, we would find that there are three branches, or three frequencies, for each value of k . Looking at the \\omega_- mode, we notice that as k \\rightarrow 0 , the dispersion relationship is linear. In this regime, the modes behave like sound waves (i.e. \\omega = v |k| ) and consequently, the mode is referred to as the acoustic mode . On the other hand, modes which intersect with optical dispersion modes (i.e. \\omega = c|k| ) are referred to as optical modes . 3.2.3: Plot the dispersion modes, along with an optical dispersion curve for visible light. What are the implications intersecting curves? We can look at the behaviour of the curves at specific values of k , for example at the zone boundary, \\omega_{+}(\\pi/a) = \\sqrt{2\\kappa_1/m} and \\omega_{+}(\\pi/a) = \\sqrt{2\\kappa_2/m} . At this point, it is useful to consider the idea of an extended Brillouin zone: the Brillouin zone houses all unique values of k , but in this case, there are multiple frequencies for each value of k . By extending the Brillouin zone, unfolding the higher-order mode(s) out to a the neighbouring unit cell in reciprocal space, we can obtain a plot which was unique frequencies for each k . A plot illustrating the extended scheme is shown below: But why would we do this? Well, consider the case of \\kappa_1 \\approx \\kappa_2 and the limit of \\kappa_1 = \\kappa_2 . We can clearly observe an \"opening\" between the bands which depends on the values of \\kappa_1 and \\kappa_2 , but physically, what would one expect to happen when \\kappa_1 = \\kappa_2 = \\kappa ? 3.2.4: Make a prediction for what will be the consequences of setting \\kappa_1 = \\kappa_2 and how this relates to the monatomic chain. Hopefully it is clear that with identical spring constants, we are back at the case of a monatomic chain. In the case of the band splitting, this will go to zero as the difference between \\kappa_1 and \\kappa_2 goes to zero: But now we have a bit of a quandary, namely, in order to capture all unique values of k , we now have a plot Brillouin zone that runs from -2\\pi/a to 2\\pi/a (see above), whereas previously this ran from -\\pi/a to \\pi/a . Any what about our number of states? The solution comes in the recognition that in the diatomic case, one has a unit cell length of a , which is 2 \\times a_\\mathrm{monatomic} , so the \"true\" Brillouin zone for the monatomic chain extents from -\\pi/a_\\mathrm{monatomic} to \\pi/a_\\mathrm{monatomic} which is equivalent to -2\\pi/a to 2\\pi/a . Similarly, the number of atoms per unit cell as changed from 2 to 1, meaning that everything doesn't fall apart. YAY!","title":"Dispersion"},{"location":"3-1d/3-2-diatomic/#density-of-states","text":"As we have seen previously, the density of states is g(\\omega)\\textrm{d}\\omega = \\textrm{d}N and thus g(\\omega) = \\frac{\\textrm{d}N}{\\textrm{d}\\omega} = \\frac{L}{2\\pi} \\sum | \\textrm{d}k/\\textrm{d} \\omega| where sum goes over all states at a given energy. In this case, we must ensure that we include the contribution to the DoS from both the positive and negative momenta. Since the energy of this system is symmetric with respect to k , this sum will simply introduce a factor of 2. In an attempt to build an intuition for visualising and understanding the density of states, it is possible to think of the DoS as a histogram of the energy samples drawn from the dispersion relation \u03c9(k) . Remember that by enforcing periodic boundary conditions on our system, we discretise k space with equally spaced points separated by distance 2\\pi/L .","title":"Density of states"},{"location":"3-1d/3-2-diatomic/#conclusions","text":"The normal modes of a diatomic chain of atoms were found by once again intuiting that plane waves in real would be a well-suited solution Systems with more than one degree of freedom per unit cell result in independent oscillation amplitudes for each degree of freedom, leading to multiple bands The density of states can be derived graphically from the dispersion relation","title":"Conclusions"},{"location":"3-1d/3-2-diatomic/#exercises","text":"","title":"Exercises"},{"location":"3-1d/3-2-diatomic/#preliminary-provocations","text":"Verify that the expression for \\omega^2 is always positive. Why is this important? When calculating the DOS, we only look at the first Brillouin zone. Why?","title":"Preliminary provocations"},{"location":"3-1d/3-2-diatomic/#exercise-1-analysing-the-diatomic-vibrating-chain","text":"As we have shown, the normal modes of oscillation of a diatomic chain occur at frequencies: \\omega_\\pm^2 = \\frac{\\kappa_1+\\kappa_2}{m} \\pm \\sqrt{\\frac{(\\kappa_1+\\kappa_2)^2 - 4\\kappa_1\\kappa_2\\sin^2(ka/2)}{m^2}} where the plus sign corresponds to the optical branch and the minus sign to the acoustic branch. Hint The final form of \\omega_\\pm as given is not always the most useful: sometimes the complex exponential form - see 3.2.2 - can make life easier Find the magnitude of the group velocity near k=0 for the acoustic branch. Show that the group velocity at k=0 for the optical branch is zero. Derive an expression of the density of states g(\u03c9) for the acoustic branch and small k . Make use of your expression of the group velocity in 1. Compare this expression with that of the derived density of states from exercise 1 of the Debye lecture.","title":"Exercise 1: analysing the diatomic vibrating chain"},{"location":"3-1d/3-2-diatomic/#exercise-2-atomic-chain-with-3-different-spring-constants","text":"Suppose we have a vibrating 1D atomic chain with 3 different spring constants alternating like \\kappa_ 1 , \\kappa_2 , \\kappa_3 , \\kappa_1 , etc. All the atoms in the chain have an equal mass m . Hint To solve the eigenvalue problem quickly, make use of the fact that the mass-spring matrix in that case commutes with the matrix X = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix}. What can be said about eigenvectors of two matrices that commute? Make a sketch of this chain and indicate the length of the unit cell a in this sketch. Derive the equations of motion for this chain. By filling in the trial solutions into the equations of motion (which should be similar to those used in the dual spring constants case), show that the eigenvalue problem is \\omega^2 \\begin{pmatrix} A_1 \\\\ A_2 \\\\ A_3 \\end{pmatrix} = \\frac{1}{m} \\begin{pmatrix} \\kappa_1 + \\kappa_ 3 & -\\kappa_ 1 & -\\kappa_ 3 e^{i k a} \\\\ -\\kappa_ 1 & \\kappa_1+\\kappa_2 & -\\kappa_ 2 \\\\ -\\kappa_ 3 e^{-i k a} & -\\kappa_2 & \\kappa_2 + \\kappa_ 3 \\end{pmatrix} \\begin{pmatrix} A_1 \\\\ A_2 \\\\ A_3 \\end{pmatrix} In general, the eigenvalue problem above cannot be solved analytically, and can only be solved in specific cases. Find the eigenvalues \u03c9^2 when k a = \\pi and \\kappa_1 = \u03ba_2 = q . What will happen to the periodicity of the band structure if \\kappa_ 1 = \\kappa_ 2 = \\kappa_3 ?","title":"Exercise 2: atomic chain with 3 different spring constants"},{"location":"3-1d/3-3-tightbinding/","text":"The tight binding model \u00b6 One-dimensional chains have been doing well, so why stop? Introduction \u00b6 The model of Sommerfeld saw the Fermionic nature of electrons cemented in Drude's kinetic theory of electrons, but as we saw in our discussion of chemistry , electrons tend not to be free, but rather occupy states as governed by surrounding nuclei. It is from this point that we aim to marry our discussion of bonding theory in the LCAO framework with our recent progress on modelling solids in one dimension. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Solid-state physics: 1D model of a solid, LCAO Text reference The material covered here is discussed in section(s) \\S 11 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Tight binding \u00b6 In our discussion of covalent bonding, we were able to obtain a solution to the Schr\u00f6dinger equation using the variational method in the framework known as LCAO . We are going to apply the same framework to an infinite chain of atoms, but before we do this, we are going to consider a gentle extension to the simple 2 atom case that we originally considered. A triatomic molecule \u00b6 Let us consider a one-dimensional triatomic system as illustrated below: We are going to make the exact same assumptions that we made when we formulated LCAO, namely we are looking at a frozen molecule (no vibrations) with fixed nuclear positions (the Born-Oppenheimer approximation). We assume that the atoms are identical, and the electrons are tightly bound to a given nucleus so that we have (\\hat{K} + \\hat{V}_n)|n\\rangle = \\epsilon_\\textrm{atomic} |n\\rangle where \\hat{K} is the usual kinetic energy operator, \\hat{V}_n is the potential due to the nucleus n and \\epsilon is the energy of the bound electronic state. We again make the crude approximate that our orbitals are orthogonal 1 , such that \\langle i|j\\rangle = \\delta_{i,j}. Now, we can do exactly what we did for a diatomic molecule, namely, consider the trail wavefunction: \\vert \\psi \\rangle = \\varphi_1 |1\\rangle + \\varphi_2 |2\\rangle + \\varphi_3 |3\\rangle. By performing the exact same logic and mathematics we arrive at the matrix equation E \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3 \\end{pmatrix} = \\begin{pmatrix} \\epsilon_0 & -t & 0 \\\\ -t & \\epsilon_0 & -t \\\\ 0 & -t & \\epsilon_0 \\end{pmatrix} \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3. \\end{pmatrix} where \\epsilon_0 = \\epsilon_\\textrm{atomic} + V_0 with V_0 = \\sum_{m \\ne j} \\langle m | V_j | m \\rangle the energy shift due to the presence of all other nuclei. It is assumed that hopping occurs only between the nearest neighbours such that \\langle1|H|2\\rangle = \\langle2|H|3\\rangle = -t. 3.3.1: Apply the exact same logic and mathematics to arrive at the above matrix equation Obtaining solutions to such a problem can be unwieldy, but fortunately, we have the perfect tool for the job. Number cruncher \u00b6 We can use our computational toolkit to solve for the eigenvalues of a matrix. For example, with \\epsilon_0 = 2 and t=1 , we find that the eigenvalues for the matrix above are approximately (0.59, 2.00, 3.41) . But what use is this? Well, let's look at a simple case of varying t . 3.3.2: Make a plot of the energy eigenvalues as a function of t running from (0, \\epsilon) and interpret your result. Ensure to include your code. Perhaps the most useful thing to do is similar to that which was introduced in the last section , which is using to produce a histogram sampling of the energy eigenvalues in order to visualise the density of states. If we go ahead an do this: The density of states is not a particularly useful concept with few states, but fortunately, this system scales well and \"grows diagonally\" with an increased number of atoms: as the hopping is limited to nearest neighbours and states are orthogonal, only the diagonal and first-off diagonal elements will be non-zero. So let's go ahead and crank the handle! To infinity and beyond n = 3 n = 10 n = 100 n = 1000 n = 10000 n = 100000 LOL: good luck! That would be one big matrix, much large than my meagre computer can handle. 3.3.3: How do these results compare with what we have seen previously? An infinite chain \u00b6 The above extension of the diatomic system to a triatomic system, especially when coupled with some computational assistance to hone our intuition, we are now going to consider the case of an infinite number of atoms, as depicted below: The potential for the above system, assuming that the locations of atoms are x_n = na with n \\in \\mathbb{Z} , is then: Now we can redeploy the machinery that we used for the diatomic and triatomic systems, namely we formulate the molecular orbital via the LCAO model: \\vert \\Psi \\rangle = \\sum_n \\phi_n |n \\rangle and write the effective Schr\u00f6dinger as \\sum_m H_{nm} \\phi_m = E \\phi_n where the matrix elements of the Hamiltonian are given by H_{nm} = \\langle n | \\hat{H} | m \\rangle It is a bit of work, but one can show that these matrix elements evaluate to H_{nm} = \\epsilon_0 \\delta_{n,m} - t \\left(\\delta_{n+1,m} + \\delta_{n-1,m}\\right) with the same definitions for \\epsilon_0 = \\epsilon_\\textrm{atomic} + V_0 and V_0 = \\sum_{m \\ne j} \\langle m | V_j | m \\rangle as in the triatomic case. Solving the tight-binding chain \u00b6 It will perhaps come as little surprise that the method that we are going to use to solve our effective Schr\u00f6dinger equation is similar to the method we have use to solve for vibrations in a chain, namely to look for plane-wave solutions: \\phi_n = \\frac{e^{-ikna}}{\\sqrt{N}} 3.3.4: How does this form of solution compare to the assumed form of solutions for oscillations? We then crank the handle: \\begin{align} E \\phi_n & = E \\times \\frac{e^{-ikna}}{\\sqrt{N}} \\\\ & = \\sum_m H_{nm} \\phi_m = \\epsilon_0 \\frac{e^{-ikna}}{\\sqrt{N}} - t \\left(\\frac{e^{-ik(n+1)a}}{\\sqrt{N}} - \\frac{e^{-ik(n-1)a}}{\\sqrt{N}}\\right) \\end{align} which yields the energy eigenvalue E = \\epsilon_0 - t \\left(e^{-ika} + e^{ika}\\right) = \\epsilon_0 - 2t\\cos(ka) which is plotted below: There are obvious similarities to the dispersion of oscillations, or phonons, but there are stark differences as compared to the Sommerfeld free electron model: now only a range of energies can be occupied, and this range is referred to as an energy band, and the energy difference between the top and the bottom of this band is called the bandwidth 2 . Further considering the band structure as compared to the free electron model, let us focus on the dispersion relation close to the base of the band at k=0 , where we approximate the energy as E \\approx \\epsilon_0 - 2t + t (ka)^2 \\equiv E_0 + t (ka)^2. If we compare this to the free-electron dispersion relation E=\\frac{\\hbar^2 k^2}{2m} we see that the band structure is similar, but with the lowest available energy is E_0 instead of 0 , and the electrons behave as if they had a different effective mass m^*=\\frac{\\hbar^2}{2ta^2}. For the moment, we leave this as an observation, but we shall return to it later in the course. Filling bands \u00b6 Let us now consider a system of N atoms. Due to the spin degeneracy, at each value k there are 2 possible states which can be occupied, which means for the system there are 2N possible states. The implications of this are significant. Band filling Divalent system Let us consider an atom with two valance electrons: there will be as many electrons as states and thus the every state in the band will be occupied, as illustrated below: Monovalent system Now let us consider monovalent atoms: there will be twice as many states as there are electrons, and thus the band will only be half filled: Note that there is now a Fermi surface - well, Fermi points. Monovalent system with electric field A Fermi surface means that nearby electrons can occupy neighbouring states, for example, under the influence of an electric field: 3.3.5: What is the implication of the plots shown in the band filling content block? Conclusions \u00b6 The density of states can be effectively visualised by sampling the dispersion relation and producing a histogram. Moreover, numerical tools are useful, especially when dealing with systems of many particles! Solving Schr\u00f6dinger's equation in the tight-binding framework using LCOA is surprisingly similar to a one-dimensional harmonic chain The tight binding model gives rise to electron bands At the bottom of bands, electrons have a similar dispersion relation to free electrons, but with altered mass Exercises \u00b6 Preliminary provocations \u00b6 Compare the expression of the effective mass with Newton's second law. Do you observe any similarities? Check the units of the effective mass. Is it what you expect? Calculate the effective mass of the free-electron dispersion relation. Is this what was expected? Under what condition is the effective mass the same for each electron? Exercise 1: The next-nearest neighbour chain \u00b6 Let's expand our one-dimensional chain model by extending the range of interaction to include the next-nearest neighbours: \\langle \\phi_n | H | \\phi_{n+2}\\rangle \\equiv -t' \u2260 0. Write down the new Schr\u00f6dinger equation for this system. Solve the Schr\u00f6dinger equation to find the dispersion relation E(k) . Calculate the effective mass m^* . Sketch the effective mass as a function of k for the cases t=2t' , t=4t' and t=10t' . This is not necessary but makes the calculation much simpler, and the qualitative behaviour reproduces that in which we are interested \u21a9 Original, hey? \u21a9","title":"3.3: The tight binding model"},{"location":"3-1d/3-3-tightbinding/#the-tight-binding-model","text":"One-dimensional chains have been doing well, so why stop?","title":"The tight binding model"},{"location":"3-1d/3-3-tightbinding/#introduction","text":"The model of Sommerfeld saw the Fermionic nature of electrons cemented in Drude's kinetic theory of electrons, but as we saw in our discussion of chemistry , electrons tend not to be free, but rather occupy states as governed by surrounding nuclei. It is from this point that we aim to marry our discussion of bonding theory in the LCAO framework with our recent progress on modelling solids in one dimension. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Solid-state physics: 1D model of a solid, LCAO Text reference The material covered here is discussed in section(s) \\S 11 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"3-1d/3-3-tightbinding/#tight-binding","text":"In our discussion of covalent bonding, we were able to obtain a solution to the Schr\u00f6dinger equation using the variational method in the framework known as LCAO . We are going to apply the same framework to an infinite chain of atoms, but before we do this, we are going to consider a gentle extension to the simple 2 atom case that we originally considered.","title":"Tight binding"},{"location":"3-1d/3-3-tightbinding/#a-triatomic-molecule","text":"Let us consider a one-dimensional triatomic system as illustrated below: We are going to make the exact same assumptions that we made when we formulated LCAO, namely we are looking at a frozen molecule (no vibrations) with fixed nuclear positions (the Born-Oppenheimer approximation). We assume that the atoms are identical, and the electrons are tightly bound to a given nucleus so that we have (\\hat{K} + \\hat{V}_n)|n\\rangle = \\epsilon_\\textrm{atomic} |n\\rangle where \\hat{K} is the usual kinetic energy operator, \\hat{V}_n is the potential due to the nucleus n and \\epsilon is the energy of the bound electronic state. We again make the crude approximate that our orbitals are orthogonal 1 , such that \\langle i|j\\rangle = \\delta_{i,j}. Now, we can do exactly what we did for a diatomic molecule, namely, consider the trail wavefunction: \\vert \\psi \\rangle = \\varphi_1 |1\\rangle + \\varphi_2 |2\\rangle + \\varphi_3 |3\\rangle. By performing the exact same logic and mathematics we arrive at the matrix equation E \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3 \\end{pmatrix} = \\begin{pmatrix} \\epsilon_0 & -t & 0 \\\\ -t & \\epsilon_0 & -t \\\\ 0 & -t & \\epsilon_0 \\end{pmatrix} \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3. \\end{pmatrix} where \\epsilon_0 = \\epsilon_\\textrm{atomic} + V_0 with V_0 = \\sum_{m \\ne j} \\langle m | V_j | m \\rangle the energy shift due to the presence of all other nuclei. It is assumed that hopping occurs only between the nearest neighbours such that \\langle1|H|2\\rangle = \\langle2|H|3\\rangle = -t. 3.3.1: Apply the exact same logic and mathematics to arrive at the above matrix equation Obtaining solutions to such a problem can be unwieldy, but fortunately, we have the perfect tool for the job.","title":"A triatomic molecule"},{"location":"3-1d/3-3-tightbinding/#number-cruncher","text":"We can use our computational toolkit to solve for the eigenvalues of a matrix. For example, with \\epsilon_0 = 2 and t=1 , we find that the eigenvalues for the matrix above are approximately (0.59, 2.00, 3.41) . But what use is this? Well, let's look at a simple case of varying t . 3.3.2: Make a plot of the energy eigenvalues as a function of t running from (0, \\epsilon) and interpret your result. Ensure to include your code. Perhaps the most useful thing to do is similar to that which was introduced in the last section , which is using to produce a histogram sampling of the energy eigenvalues in order to visualise the density of states. If we go ahead an do this: The density of states is not a particularly useful concept with few states, but fortunately, this system scales well and \"grows diagonally\" with an increased number of atoms: as the hopping is limited to nearest neighbours and states are orthogonal, only the diagonal and first-off diagonal elements will be non-zero. So let's go ahead and crank the handle! To infinity and beyond n = 3 n = 10 n = 100 n = 1000 n = 10000 n = 100000 LOL: good luck! That would be one big matrix, much large than my meagre computer can handle. 3.3.3: How do these results compare with what we have seen previously?","title":"Number cruncher"},{"location":"3-1d/3-3-tightbinding/#an-infinite-chain","text":"The above extension of the diatomic system to a triatomic system, especially when coupled with some computational assistance to hone our intuition, we are now going to consider the case of an infinite number of atoms, as depicted below: The potential for the above system, assuming that the locations of atoms are x_n = na with n \\in \\mathbb{Z} , is then: Now we can redeploy the machinery that we used for the diatomic and triatomic systems, namely we formulate the molecular orbital via the LCAO model: \\vert \\Psi \\rangle = \\sum_n \\phi_n |n \\rangle and write the effective Schr\u00f6dinger as \\sum_m H_{nm} \\phi_m = E \\phi_n where the matrix elements of the Hamiltonian are given by H_{nm} = \\langle n | \\hat{H} | m \\rangle It is a bit of work, but one can show that these matrix elements evaluate to H_{nm} = \\epsilon_0 \\delta_{n,m} - t \\left(\\delta_{n+1,m} + \\delta_{n-1,m}\\right) with the same definitions for \\epsilon_0 = \\epsilon_\\textrm{atomic} + V_0 and V_0 = \\sum_{m \\ne j} \\langle m | V_j | m \\rangle as in the triatomic case.","title":"An infinite chain"},{"location":"3-1d/3-3-tightbinding/#solving-the-tight-binding-chain","text":"It will perhaps come as little surprise that the method that we are going to use to solve our effective Schr\u00f6dinger equation is similar to the method we have use to solve for vibrations in a chain, namely to look for plane-wave solutions: \\phi_n = \\frac{e^{-ikna}}{\\sqrt{N}} 3.3.4: How does this form of solution compare to the assumed form of solutions for oscillations? We then crank the handle: \\begin{align} E \\phi_n & = E \\times \\frac{e^{-ikna}}{\\sqrt{N}} \\\\ & = \\sum_m H_{nm} \\phi_m = \\epsilon_0 \\frac{e^{-ikna}}{\\sqrt{N}} - t \\left(\\frac{e^{-ik(n+1)a}}{\\sqrt{N}} - \\frac{e^{-ik(n-1)a}}{\\sqrt{N}}\\right) \\end{align} which yields the energy eigenvalue E = \\epsilon_0 - t \\left(e^{-ika} + e^{ika}\\right) = \\epsilon_0 - 2t\\cos(ka) which is plotted below: There are obvious similarities to the dispersion of oscillations, or phonons, but there are stark differences as compared to the Sommerfeld free electron model: now only a range of energies can be occupied, and this range is referred to as an energy band, and the energy difference between the top and the bottom of this band is called the bandwidth 2 . Further considering the band structure as compared to the free electron model, let us focus on the dispersion relation close to the base of the band at k=0 , where we approximate the energy as E \\approx \\epsilon_0 - 2t + t (ka)^2 \\equiv E_0 + t (ka)^2. If we compare this to the free-electron dispersion relation E=\\frac{\\hbar^2 k^2}{2m} we see that the band structure is similar, but with the lowest available energy is E_0 instead of 0 , and the electrons behave as if they had a different effective mass m^*=\\frac{\\hbar^2}{2ta^2}. For the moment, we leave this as an observation, but we shall return to it later in the course.","title":"Solving the tight-binding chain"},{"location":"3-1d/3-3-tightbinding/#filling-bands","text":"Let us now consider a system of N atoms. Due to the spin degeneracy, at each value k there are 2 possible states which can be occupied, which means for the system there are 2N possible states. The implications of this are significant. Band filling Divalent system Let us consider an atom with two valance electrons: there will be as many electrons as states and thus the every state in the band will be occupied, as illustrated below: Monovalent system Now let us consider monovalent atoms: there will be twice as many states as there are electrons, and thus the band will only be half filled: Note that there is now a Fermi surface - well, Fermi points. Monovalent system with electric field A Fermi surface means that nearby electrons can occupy neighbouring states, for example, under the influence of an electric field: 3.3.5: What is the implication of the plots shown in the band filling content block?","title":"Filling bands"},{"location":"3-1d/3-3-tightbinding/#conclusions","text":"The density of states can be effectively visualised by sampling the dispersion relation and producing a histogram. Moreover, numerical tools are useful, especially when dealing with systems of many particles! Solving Schr\u00f6dinger's equation in the tight-binding framework using LCOA is surprisingly similar to a one-dimensional harmonic chain The tight binding model gives rise to electron bands At the bottom of bands, electrons have a similar dispersion relation to free electrons, but with altered mass","title":"Conclusions"},{"location":"3-1d/3-3-tightbinding/#exercises","text":"","title":"Exercises"},{"location":"3-1d/3-3-tightbinding/#preliminary-provocations","text":"Compare the expression of the effective mass with Newton's second law. Do you observe any similarities? Check the units of the effective mass. Is it what you expect? Calculate the effective mass of the free-electron dispersion relation. Is this what was expected? Under what condition is the effective mass the same for each electron?","title":"Preliminary provocations"},{"location":"3-1d/3-3-tightbinding/#exercise-1-the-next-nearest-neighbour-chain","text":"Let's expand our one-dimensional chain model by extending the range of interaction to include the next-nearest neighbours: \\langle \\phi_n | H | \\phi_{n+2}\\rangle \\equiv -t' \u2260 0. Write down the new Schr\u00f6dinger equation for this system. Solve the Schr\u00f6dinger equation to find the dispersion relation E(k) . Calculate the effective mass m^* . Sketch the effective mass as a function of k for the cases t=2t' , t=4t' and t=10t' . This is not necessary but makes the calculation much simpler, and the qualitative behaviour reproduces that in which we are interested \u21a9 Original, hey? \u21a9","title":"Exercise 1: The next-nearest neighbour chain"},{"location":"4-crystal/4-1-realspace/","text":"Crystal structure \u00b6 Learn the real secrets of crystals. Introduction \u00b6 Our journey to this point has seen our initial, somewhat crude models of solids receive enhancement in the form of microscopic models in one dimension, which gave rise to dispersion relations for both phonons and electrons, with many interesting predictions. Unfortunately, it is rare that the universe gives us one dimensional systems 1 and thus we must consider systems in higher dimensions, and we are going to look in detail at three-dimensional solids. Specifically, we are going to limit ourselves to ordered solids , but to do this meaningfully, we must develop the tools and nomenclature to properly describe these systems. Definition bashing There are to be many definitions to be introduced in this discussion. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Mathematics: Linear algebra, geometry and symmetry Text reference The material covered here is discussed in section(s) \\S 12 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: The Voronoi diagram generator can be accessed here Crystal classification \u00b6 Limiting ourselves to considering crystals may seem like a compromise, after all, not everything is a crystal. This is both true and false: crystals as we think of them (emeralds, sapphires, salt, etc.) are obviously not ubiquitous, but repeating motifs of atoms describe almost all systems at some length scale, and certainly for most systems of interest in solid-state physics, this is almost always true. Lattices and unit cells \u00b6 Crystals are highly ordered microscopic, where this microstructure can be reflected in the macrostructure. which are periodic multi-atomic structures. Definition: crystal A crystal is a periodic arrangement of particles (atoms, molecules, ...) To describe periodic structures, we need a framework, and this framework is provided by the concept of a lattice Definition: lattice i) A lattice is an infinite set of points defined by an integer sums of a set of linearly independent primitive lattice vectors Explicitly, in one dimension we can write \\mathbf{R}_{\\left[n_{1}\\right]}=n_{1} \\mathbf{a}_{1}, \\quad \\mathrm{for } \\:\\: n_{1}, \\in \\mathbb{Z}, in two dimensions we can write \\mathbf{R}_{\\left[n_{1} n_{2}\\right]}=n_{1} \\mathbf{a}_{1}+n_{2} \\mathbf{a}_{2}, \\quad \\mathrm{for } \\:\\: n_{1}, n_{2} \\in \\mathbb{Z}, and in three dimensions we can write \\mathbf{R}_{\\left[n_{1} n_{2} n_{3}\\right]}=n_{1} \\mathbf{a}_{1}+n_{2} \\mathbf{a}_{2}+n_{3} \\mathbf{a}_{3}, \\quad \\mathrm{for } \\:\\: n_{1}, n_{2}, n_{3} \\in \\mathbb{Z} where \\mathbf{a}_i are the primitive lattice vectors. It should be clear that for a n dimensional system, we require n linearly independent primitive lattice vectors to span the entire lattice, but also the choice of these vectors is not unique. The image below shows a simple two-dimensional square lattice, where each of the black dots are lattice points. There exist multiple equivalent definitions of a lattice, which can be useful to consider in different circumstances. Definition: lattice ii) A lattice is a set of points where the environment of each point is the same iii) A lattice is an infinite set of vectors where the addition of any two vectors returns a vector in the set We identify that definition (ii) is the more intuitive definition. For example, let's look a the well-known honeycomb structure: 4.1.1: Is the honeycomb structure a lattice? A vector connecting any two lattice points is called a lattice vector . Supposing that we choose two linearly independent lattice vectors \\mathbf{a}_1 and \\mathbf{a}_2 , these two lattice vectors span an area which is called a unit cell : Definition: unit cell A unit cell is a region of space such that when many identical units are stacked together it tiles (completely fills) all of space and reconstructs the full structure In the case of a 3D lattice, we need to choose three linearly independent lattice vectors and thus the unit cell will be a volume instead of an area. If the chosen unit cell only contains a single lattice point, this is called a primitive unit cell . The lattice vectors which construct the primitive unit cell are called primitive lattice vectors . Because the primitive unit cell is constructed out of a set of linearly independent primitive lattice vectors, the primitive unit cell can be repeated infinitely many times to map out the entire lattice! The unit cell, which was conveniently chosen in panel B , defines a primitive unit cell. At first glance it might seem that there are four lattice point inside the primitive unit cell instead of one; however, each point only occupies the lattice by one quarter. Therefore, there is exactly 4 \\times \\frac{1}{4} = 1 lattice point in the unit cell and is thus it is a primitive unit cell! As previously mentioned, the choice of the primitive lattice vectors is not unique. For example, we could have easily chosen another set of lattice vectors - as shown in panel C - which produce primitive unit cells. Both choices are primitive unit cells and thus make it possible to map out the entire lattice! 4.2.2: Looking the primitive cell in panel C , how much does each lattice point \"occupy\" the unit cell? Periodic structures. \u00b6 Equipped with the basic definitions of a lattice, (primitive) lattice vectors, and (primitive) unit cells, we can now apply our knowledge to an actual periodic structure. In the image below we show a periodic structure (panel A ). Again, it should be emphasised that there are several ways to assign lattice points to the periodic structure. For example, we could assign the lattice points to the stars themselves: this is a valid choice of a lattice for the periodic structure because each lattice point has the same environment. As the lattice points form triangles, this lattice is called a triangular lattice . The choice of a lattice also defines two linearly independent primitive lattice vectors \\mathbf{a}_1 and \\mathbf{a}_2 (panel B ): \\mathbf{a}_1 = \\hat{\\mathbf{x}} = \\left[ 1, 0\\right], \\quad \\mathbf{a}_2 = \\frac{1}{2}\\hat{\\mathbf{x}} + \\frac{\\sqrt{3}}{2} \\hat{\\mathbf{y}}= \\left[1/2, \\sqrt{3}/2\\right]. With these primitive lattice vector, the lattice is given by \\mathbf{R}_{\\left[n_{1}, n_{2}\\right]}=n_{1} \\left[ 1, 0 \\right]+n_{2} \\left[ 1/2, \\sqrt{3}/2 \\right], \\quad \\mathrm{for} \\:\\: n_{1}, n_{2} \\in \\mathbb{Z}. However, our description so far is insufficient to describe the periodic structure. Although we have mapped out the entire lattice, we still do not have any information about the repeated motif. It incorporate this information, we introduce the concept of a basis 2 : Definition: basis The description of objects with respect to the reference lattice point is known as a basis The reference lattice point is the chosen lattice point to which we apply the lattice vectors in order to reconstruct the lattice. In our case, we chose the reference point as [0, 0] . With respect to the reference point, the motif is located at [0, 0] . In this particular example, the repeated motif and the lattice can be coincident, but more generally this will not be the case. The location of all the patterned object in the lattice with respect to the reference lattice point is then given by: \\mathbf{R}_{\\left[n_{1}, n_{2}\\right]}^{\\mathrm{motif}} = n_{1} \\left[ 1, 0 \\right]+n_{2} \\left[ 1/2, \\sqrt{3}/2 \\right], \\quad \\mathrm{for} \\:\\: n_{1}, n_{2} \\in \\mathbb{Z}. Another way to define the basis is in terms of the fractional coordinates of the primitive lattice vectors. In other words, we want to express the basis as a linear combination of the primitive lattice vectors: (f_1, f_2, \\ldots, f_N) = \\sum_{i = 1}^{N}f_i \\mathbf{a}_i, where f_i is the fractional coordinate of \\mathbf{a}_i and N is the dimensionality of the system. In 2D this equation reduces to (f_1, f_2) = f_1 \\mathbf{a}_1 + f_2 \\mathbf{a}_2. In our case, the basis is hourglass (0,0) which is specifying the location of the hourglass in the unit cell as 0 \\mathbf{a}_1 +0 \\mathbf{a}_2 . Whilst the is slightly contrived, we shall see its application to physical systems soon enough, and there we would simply replace hourglass with the relevant atom, for example, \\mathrm{C}(f_1, f_2) for carbon in the unit cell. Similar to primitive lattice vectors, the choice of a lattice not unique. For example, shown in panel C , the same periodic structure is shown, but with the lattice translated by 1/2 \\mathbf{a}_1 . The choice still fulfils the definition of a lattice: the environment of each lattice point is the same, with the only difference being that the lattice is translated and thus we keep using the same primitive lattice vectors. 4.2.3: What is the basis for translated lattice as shown in panel C ? What are the positions of each of the hourglasses in the lattice? Conventional unit cell \u00b6 In the system we have been considering, it is clear that the primitive vectors are not orthogonal, and this is not ideal: we like orthogonal things! Definition: conventional unit cell A unit cell with an orthogonal set of lattice vectors An example of such a unit cell is shown is panel D . In contrast to primitive unit cells, conventional unit cells may contain multiple lattice points, as shown in the example: there is an additional lattice point at the centre, and thus 1 + 4 \\times \\frac{1}{4} = 2 lattice points in the conventional unit cell. Returning to our pattern, if we use the shown conventional unit cell, there is now a problem with the definition of the lattice vectors: no integer linear combination of lattice vectors is able to produce the lattice point in the centre of the unit cell. Therefore, in order to map out the entire lattice, we need to include an extra position the basis. Since there is one hourglass at the corner of the unit cell and one in the centre, the basis is: hourglass: (0,0),(1/2,1/2) . The corresponding locations of the stars with respect to the reference lattice point are then given by: \\begin{align} \\mathbf{R}_{\\left[n_{1}, n_{2}\\right]}^{\\mathrm{corner}} &= n_{1} \\mathbf{a}_1 +n_2 \\mathbf{a}_2 \\\\ \\mathbf{R}_{\\left[n_{1}, n_{2}\\right]}^{\\mathrm{centre}} &= \\frac{1}{2}\\left( \\mathbf{a}_1 + \\mathbf{a}_2 \\right) + n_{1} \\mathbf{a}_1 +n_2 \\mathbf{a}_2, \\end{align} for n_{1}, n_{2} \\in \\mathbb{Z} . Alternatively, one can think of the crystal structure being made up from two intersecting orthogonal lattices - one centred at [0,0] and another at [1/2,1/2] , each with basis (0,0) . 4.2.4: For what type of lattice is the conventional unit cell also primitive? Recipe for analysing periodic structures \u00b6 When encountering periodic systems in the wild, we can come up with a recipe for analysing them. One such recipe is as follows: Choose origin (can be an atom, but this is not necessary) Find other lattice points (identical atoms) Choose lattice vectors that translate between these lattice points: either primitive or not primitive (in which case you require an additional basis point) The lengths of lattice vectors and angle(s) between them fully define the crystal lattice Specify the basis Much like a good recipe, the definition of a lattice and a basis allows one to cook up the location of every atom in the periodic structure and thus the crystal structure. Example: Graphene \u00b6 Let's put our skills to the test, analysing a real structure. Graphene, besides being super cool, is made out of a single layer of carbon atoms arranged in a honeycomb shape, with a nearest-neighbour interatomic distance of a . The locations of atomic positions is shown in panel A below: Following our recipe above, the first task is to find suitable lattice points. We start by choosing a lattice point on the (0,0) coordinate, and looking for all identical atoms. We see that not all carbon atoms have the same environment with respect to our chosen lattice point, and hence, not all carbon atoms coincide with lattice points! Only those with the same environment are valid lattice points. Panel B shows the lattice, which we identify as a triangular lattice. Primitive unit cell \u00b6 Next, we find lattice vectors, and because lattice vectors are not unique, we are free to choose them. A natural choice for primitive lattice is shown in panel C and with a little geometry, we find that \\mathbf{a}_1 and \\mathbf{a}_2 are: \\mathbf{a}_1 = [\\sqrt{3}, 0] a, \\quad \\mathbf{a}_2 = [\\sqrt{3}/2, 3/2] a. As usual, with these lattice vectors we are able to map out the entire lattice, but we must specify the repeated motif, that is, the basis. Each primitive unit cell contains two carbon atoms: one is at the reference point (0,0) and the other is located at (\\sqrt{3},1)a . When writing the basis, we want to express the coordinates as fractional coordinates of the chosen lattice vectors, and for the atom at the reference point this is easy: \\mathrm{C}(0,0) . In order to find the fractional coordinates of the second carbon atom, it is convenient to first look at the y coordinate of the atom, which is a . Because only \\mathbf{a}_2 has a nonzero y component, we easily find the fractional coordinate of \\mathbf{a}_2 : it is simply \\frac{a}{3a/2} = 2/3 . To find the fractional coordinate of \\mathbf{a}_1 , we use the fractional coordinate of \\mathbf{a}_2 . Multiplying the x component of \\mathbf{a}_2 by 2/3 yields 1/\\sqrt{3} . We know that \\begin{align} \\sqrt{3} &= f_1 a_{1,x} + f_2 a_{2,x}\\\\ &= f_1 \\sqrt{3} + 1/\\sqrt{3}, \\end{align} Bringing 1/\\sqrt{3} to the other side and dividing both sides by \\sqrt{3} yields f_1 = 2/3 . Hence the basis of the second atom is \\mathrm{C}(2/3, 2/3) . Conventional unit cell \u00b6 To show why one likes conventional unit cells, in panel D we show the conventional unit cell with the lattice vectors: \\mathbf{a}_1 = [\\sqrt{3}, 0] a, \\quad \\mathbf{a}_2 = [0, 3] a. The price one pays for the convenience of orthogonal lattice vectors: the unit cell now contains four carbon atoms which need to be specified in the basis. Fortunately, the task is much more straightforward and the basis in fractional coordinates is: \\mathrm{C}(0,0), \\: \\mathrm{C}(0,1/3), \\: \\mathrm{C}(1/2, 1/2) and \\mathrm{C}(1/2, 5/6) . Wigner-Seitz unit cell \u00b6 Primitive unit cells are not unique; however, there exists a unique primitive cell which is the locus of points which are closer to a given lattice point than all other lattice points, and is known as the Wigner-Seitz cell . Much like our other unit cells, we can use a recipe to cook it up: From a given lattice point, find all neighbouring lattice points Draw lines between the reference lattice point and the neighbouring lattice points Draw perpendicular bisectors of each line Extend the perpendicular bisectors until they intersect This procedure will produce the Wigner-Seitz cell, and panel E in the graphene example shows the Wigner-Seitz cell for graphene. We note that the Wigner-Seitz cell only contains a single lattice point in the middle, but it does however contain other atoms, which must be specified in the basis. How many carbon atoms are inside the Wigner-Seitz cell of graphene? There are two methods to calculate this. We either translate the lattice and thus the Wigner-Seitz cell a bit and observe that there are two carbon atoms inside the cell. Another way to calculate the number of atoms inside the cell is by realizing that there is an atom at the lattice point itself and there is 1/3'rd of an atom at three corners of the cell. This results in 1+3\\times 1/3 = 2 atoms being inside the unit cell. For the moment, one can think of the Wigner-Seitz cell as a bit of an abstraction, but we shall see its importance as we continue on our journey. Three dimensional lattices (d++) \u00b6 Thus far we have looked at two-dimensional periodic structures, but as mentioned: most crystal structures are three dimensional. Fortunately, the tools used to describe lattices in any dimensional are applicable in other dimension. For the duration of this course, we shall be mainly concerned with cubic lattices (and closely-related friends), which have the delightful property that the lattice vectors \\mathbf{a_1} , \\mathbf{a_2} , and \\mathbf{a_3} are colinear with \\mathbf{\\hat{x}} , \\mathbf{\\hat{y}} , and \\mathbf{\\hat{z}} . In the case that the magnitude of each vector is identical (lattice constant a ), this is called a simple cubic lattice . Lattices: cubic and friends Cubic lattice All cell edge lengths equal Tetragonal Two cell edge lengths equal Orthorhombic All cell edge lengths unequal 4.2.5: How many lattice points are there in the unit cell for the simple cubic lattice? The only mineral which has a simple cubic lattice structure is a metallic allotrope of Polonium, but given the nature of polonium, there are no pretty pictures to show. Rather, shown below is the mineral pyrite (iron (II) disulphide), which at the macroscale has a very satisfying cubic structure: An incredible sample of pyrite, sourced from wikipedia and used under the creative commons licence CC BY-SA 3.0 To understand this, and other systems, we must consider additional classes of cubic lattices. The body-centred cubic lattice \u00b6 The image above (panel A ) shows a simple cubic lattice. Now, perhaps the simplest adjustment one can imagine making the simple cubic structure is the addition of a lattice point to the centre of the simple cubic cell, which is shown in panel B . This structure is known as the Body-Centred Cubic ( BCC ) lattice. In the BCC lattice, there are 8 lattice points on the corners of the cell and one in the centre and thus the conventional unit cell contains 8\\times 1/8+1 = 2 lattice points, and is therefore not primitive. What is the basis of the bcc lattice? (0,0,0) and (1/2, 1/2, 1/2) Iron at normal temperatures has a body-centred cubic structure The face-cantered cubic lattice \u00b6 Another way we can alter the simple cubic lattice is by adding a lattice point in every face of the simple cubic cell, which is shown in panel C . Such a structure is referred to as the Face-Centred Cubic ( FCC ) lattice. There is 1/2 of a lattice point at each face inside the lattice, in addition to the corner lattice points and thus there are a total of 8 \\times 1/8 + 6\\times 1/2 = 4 lattice points inside the unit cell, and thus it is not primitive. Materials which have a face-centred cubic lattice structure include diamond Filling factor \u00b6 Each of the three crystal structures above have a different configuration and number of atoms in the unit cell. This results in a different fraction of an unit cell being occupied by atoms. The filling factor , commonly called the atomic packing factor , measures the fraction of a volume of the unit cell that is occupied by atoms. It assumes that the atoms are solid spheres with a volume V_{\\mathrm{atom}} = \\frac{4 \\pi}{3} R^3 , where R is the radius of the sphere. For mono-atomic lattices, the filling factor F is defined as follows: F = \\frac{ N_{\\mathrm{atom}} V_{\\mathrm{atom}} }{V_{\\mathrm{cell}}}. Here N_{\\mathrm{atom}} is the number of atoms in the unit cell and V_{\\mathrm{cell}} is the volume of the unit cell. To calculate the filling factor we first need to find out what V_{\\mathrm{atom}} is. To do this, we need to \"blow up\" the atoms simultaneously until each atom touches its neighbour, and we use this \"blown up\" geometry to find an expression for R in terms of known quantities, such as the lattice constant. Filling factor of the FCC lattice The lattice is shown in panel A of the image above. We first need to increase the size of the atoms until they touch each other. The faces of the FCC are rotationally symmetric and we obtain that all four corner atoms touch the centre atom (panel B ). We identify that on the diagonal, the atoms touch each other, meaning that as the atoms have a radius R , the length of the diagonal is 4R . Given the sides of the unit cell are length a , the diagonal of the unit cell will be equal to \\sqrt{2}a . This implies that R = \\frac{a}{2\\sqrt{2}}. With the deduced radius of the atom, we can calculate V_{\\mathrm{atom}} : V_{\\mathrm{atom}} = \\frac{4\\pi}{3}R^3 = \\frac{4\\pi}{3} \\left( \\frac{a}{2\\sqrt{2}} \\right)^3 = \\frac{\\pi a^3}{12 \\sqrt{2}}. The only thing that is left for us to determine is the number of atoms in the unit cell, which we earlier noted to be 4. Therefore we calculate the filling factor to be \\begin{align} F &= \\frac{ N_{\\mathrm{atom}} V_{\\mathrm{atom}} }{V_{\\mathrm{cell}}}\\\\ &= \\frac{4 \\times \\frac{\\pi a^3}{12\\sqrt{2}}}{a^3}\\\\ &= \\frac{\\pi}{3\\sqrt{2}} \\approx 0.74 \\end{align} or put another way, approximately 74 \\% of the FCC unit cell is occupied by atoms. 4.2.5: Can you think of another lattice structure with a higher filling factor? Other lattice structures \u00b6 It should be noted that there exist three dimensional lattices with non-orthogonal axes (and indeed are common), and the method for their description as introduced here is identical, but just a bit messier. Perhaps surprisingly, there are only 14 distinct lattices from which all other lattices from which all other lattices can be constructed ! Conclusions \u00b6 Lattices can be used to describe periodic systems Several important concepts to describe crystal structure were introduced: lattice, lattice vectors, basis, primitive & conventional unit cells The crystal structures with orthogonal lattice vectors were introduced, in addition to their different centring types: simple cubic, FCC, and BCC Exercises \u00b6 Preliminary provocations \u00b6 State the definition of a primitive unit cell. What can be said about its volume? Draw the conventional unit cell of a FCC and the BCC. Write down the primitive lattice vectors and the basis of each lattice. Suppose you find the primitive unit cell of a diatomic crystal. How many basis vectors do you minimally need to describe the crystal? Can a diatomic crystal require more basis vectors? Calculate the filling factor of a simple cubic lattice. Sketch the (110),(1\\bar{1}0),(111) miller planes of a simple cubic lattice. Exercise 1: Diatomic crystal \u00b6 Consider the following two-dimensional diatomic crystal: Sketch the Wigner-Seitz unit cell and two other possible primitive unit cells of the crystal. If the distance between the filled cirles is a=0.28 nm, what is the area of the primitive unit cell? How would this area change if all the empty circles and the filled circles were identical? Write down one set of primitive lattice vectors and the basis for this crystal. What happens to the number of elements in the basis if all empty and filled circles were identical? Imagine expanding the lattice into the perpendicular direction z . We can define a new three-dimensional crystal by considering a periodic structure in the z direction, where the filled circles have been displaced by \\frac{a}{2} in both the x and y direction from the empty circles. The figure below shows the new arrangement of the atoms. What lattice do we obtain? Write down the basis of the three-dimensional crystal. If we consider all atoms to be the same, what lattice do we obtain? Compute the filling factor in the case where all atoms are the same. Exercise 2: Diamond lattice \u00b6 Consider a the diamond crystal structure structure. The following illustration shows the arrangement of the carbon atoms in a conventional unit cell. The side of the cube is a = 0.3567 nm. How is this crystal structure related to the fcc lattice? Write down one set of primitive lattice vectors and compute the volume of the corresponding primitive unit cell. How many atoms are in the primitive unit cell? Write down the basis. Determine the number of atoms in the conventional unit cell and compute its volume. What is the distance between nearest neighbouring atoms? Compute the filling factor. Although the do exist, and hopefully we shall discuss some later in the semester. \u21a9 Not to be confused with the basis from linear algebra! \u21a9","title":"Crystals"},{"location":"4-crystal/4-1-realspace/#crystal-structure","text":"Learn the real secrets of crystals.","title":"Crystal structure"},{"location":"4-crystal/4-1-realspace/#introduction","text":"Our journey to this point has seen our initial, somewhat crude models of solids receive enhancement in the form of microscopic models in one dimension, which gave rise to dispersion relations for both phonons and electrons, with many interesting predictions. Unfortunately, it is rare that the universe gives us one dimensional systems 1 and thus we must consider systems in higher dimensions, and we are going to look in detail at three-dimensional solids. Specifically, we are going to limit ourselves to ordered solids , but to do this meaningfully, we must develop the tools and nomenclature to properly describe these systems. Definition bashing There are to be many definitions to be introduced in this discussion. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Mathematics: Linear algebra, geometry and symmetry Text reference The material covered here is discussed in section(s) \\S 12 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: The Voronoi diagram generator can be accessed here","title":"Introduction"},{"location":"4-crystal/4-1-realspace/#crystal-classification","text":"Limiting ourselves to considering crystals may seem like a compromise, after all, not everything is a crystal. This is both true and false: crystals as we think of them (emeralds, sapphires, salt, etc.) are obviously not ubiquitous, but repeating motifs of atoms describe almost all systems at some length scale, and certainly for most systems of interest in solid-state physics, this is almost always true.","title":"Crystal classification"},{"location":"4-crystal/4-1-realspace/#lattices-and-unit-cells","text":"Crystals are highly ordered microscopic, where this microstructure can be reflected in the macrostructure. which are periodic multi-atomic structures. Definition: crystal A crystal is a periodic arrangement of particles (atoms, molecules, ...) To describe periodic structures, we need a framework, and this framework is provided by the concept of a lattice Definition: lattice i) A lattice is an infinite set of points defined by an integer sums of a set of linearly independent primitive lattice vectors Explicitly, in one dimension we can write \\mathbf{R}_{\\left[n_{1}\\right]}=n_{1} \\mathbf{a}_{1}, \\quad \\mathrm{for } \\:\\: n_{1}, \\in \\mathbb{Z}, in two dimensions we can write \\mathbf{R}_{\\left[n_{1} n_{2}\\right]}=n_{1} \\mathbf{a}_{1}+n_{2} \\mathbf{a}_{2}, \\quad \\mathrm{for } \\:\\: n_{1}, n_{2} \\in \\mathbb{Z}, and in three dimensions we can write \\mathbf{R}_{\\left[n_{1} n_{2} n_{3}\\right]}=n_{1} \\mathbf{a}_{1}+n_{2} \\mathbf{a}_{2}+n_{3} \\mathbf{a}_{3}, \\quad \\mathrm{for } \\:\\: n_{1}, n_{2}, n_{3} \\in \\mathbb{Z} where \\mathbf{a}_i are the primitive lattice vectors. It should be clear that for a n dimensional system, we require n linearly independent primitive lattice vectors to span the entire lattice, but also the choice of these vectors is not unique. The image below shows a simple two-dimensional square lattice, where each of the black dots are lattice points. There exist multiple equivalent definitions of a lattice, which can be useful to consider in different circumstances. Definition: lattice ii) A lattice is a set of points where the environment of each point is the same iii) A lattice is an infinite set of vectors where the addition of any two vectors returns a vector in the set We identify that definition (ii) is the more intuitive definition. For example, let's look a the well-known honeycomb structure: 4.1.1: Is the honeycomb structure a lattice? A vector connecting any two lattice points is called a lattice vector . Supposing that we choose two linearly independent lattice vectors \\mathbf{a}_1 and \\mathbf{a}_2 , these two lattice vectors span an area which is called a unit cell : Definition: unit cell A unit cell is a region of space such that when many identical units are stacked together it tiles (completely fills) all of space and reconstructs the full structure In the case of a 3D lattice, we need to choose three linearly independent lattice vectors and thus the unit cell will be a volume instead of an area. If the chosen unit cell only contains a single lattice point, this is called a primitive unit cell . The lattice vectors which construct the primitive unit cell are called primitive lattice vectors . Because the primitive unit cell is constructed out of a set of linearly independent primitive lattice vectors, the primitive unit cell can be repeated infinitely many times to map out the entire lattice! The unit cell, which was conveniently chosen in panel B , defines a primitive unit cell. At first glance it might seem that there are four lattice point inside the primitive unit cell instead of one; however, each point only occupies the lattice by one quarter. Therefore, there is exactly 4 \\times \\frac{1}{4} = 1 lattice point in the unit cell and is thus it is a primitive unit cell! As previously mentioned, the choice of the primitive lattice vectors is not unique. For example, we could have easily chosen another set of lattice vectors - as shown in panel C - which produce primitive unit cells. Both choices are primitive unit cells and thus make it possible to map out the entire lattice! 4.2.2: Looking the primitive cell in panel C , how much does each lattice point \"occupy\" the unit cell?","title":"Lattices and unit cells"},{"location":"4-crystal/4-1-realspace/#periodic-structures","text":"Equipped with the basic definitions of a lattice, (primitive) lattice vectors, and (primitive) unit cells, we can now apply our knowledge to an actual periodic structure. In the image below we show a periodic structure (panel A ). Again, it should be emphasised that there are several ways to assign lattice points to the periodic structure. For example, we could assign the lattice points to the stars themselves: this is a valid choice of a lattice for the periodic structure because each lattice point has the same environment. As the lattice points form triangles, this lattice is called a triangular lattice . The choice of a lattice also defines two linearly independent primitive lattice vectors \\mathbf{a}_1 and \\mathbf{a}_2 (panel B ): \\mathbf{a}_1 = \\hat{\\mathbf{x}} = \\left[ 1, 0\\right], \\quad \\mathbf{a}_2 = \\frac{1}{2}\\hat{\\mathbf{x}} + \\frac{\\sqrt{3}}{2} \\hat{\\mathbf{y}}= \\left[1/2, \\sqrt{3}/2\\right]. With these primitive lattice vector, the lattice is given by \\mathbf{R}_{\\left[n_{1}, n_{2}\\right]}=n_{1} \\left[ 1, 0 \\right]+n_{2} \\left[ 1/2, \\sqrt{3}/2 \\right], \\quad \\mathrm{for} \\:\\: n_{1}, n_{2} \\in \\mathbb{Z}. However, our description so far is insufficient to describe the periodic structure. Although we have mapped out the entire lattice, we still do not have any information about the repeated motif. It incorporate this information, we introduce the concept of a basis 2 : Definition: basis The description of objects with respect to the reference lattice point is known as a basis The reference lattice point is the chosen lattice point to which we apply the lattice vectors in order to reconstruct the lattice. In our case, we chose the reference point as [0, 0] . With respect to the reference point, the motif is located at [0, 0] . In this particular example, the repeated motif and the lattice can be coincident, but more generally this will not be the case. The location of all the patterned object in the lattice with respect to the reference lattice point is then given by: \\mathbf{R}_{\\left[n_{1}, n_{2}\\right]}^{\\mathrm{motif}} = n_{1} \\left[ 1, 0 \\right]+n_{2} \\left[ 1/2, \\sqrt{3}/2 \\right], \\quad \\mathrm{for} \\:\\: n_{1}, n_{2} \\in \\mathbb{Z}. Another way to define the basis is in terms of the fractional coordinates of the primitive lattice vectors. In other words, we want to express the basis as a linear combination of the primitive lattice vectors: (f_1, f_2, \\ldots, f_N) = \\sum_{i = 1}^{N}f_i \\mathbf{a}_i, where f_i is the fractional coordinate of \\mathbf{a}_i and N is the dimensionality of the system. In 2D this equation reduces to (f_1, f_2) = f_1 \\mathbf{a}_1 + f_2 \\mathbf{a}_2. In our case, the basis is hourglass (0,0) which is specifying the location of the hourglass in the unit cell as 0 \\mathbf{a}_1 +0 \\mathbf{a}_2 . Whilst the is slightly contrived, we shall see its application to physical systems soon enough, and there we would simply replace hourglass with the relevant atom, for example, \\mathrm{C}(f_1, f_2) for carbon in the unit cell. Similar to primitive lattice vectors, the choice of a lattice not unique. For example, shown in panel C , the same periodic structure is shown, but with the lattice translated by 1/2 \\mathbf{a}_1 . The choice still fulfils the definition of a lattice: the environment of each lattice point is the same, with the only difference being that the lattice is translated and thus we keep using the same primitive lattice vectors. 4.2.3: What is the basis for translated lattice as shown in panel C ? What are the positions of each of the hourglasses in the lattice?","title":"Periodic structures."},{"location":"4-crystal/4-1-realspace/#conventional-unit-cell","text":"In the system we have been considering, it is clear that the primitive vectors are not orthogonal, and this is not ideal: we like orthogonal things! Definition: conventional unit cell A unit cell with an orthogonal set of lattice vectors An example of such a unit cell is shown is panel D . In contrast to primitive unit cells, conventional unit cells may contain multiple lattice points, as shown in the example: there is an additional lattice point at the centre, and thus 1 + 4 \\times \\frac{1}{4} = 2 lattice points in the conventional unit cell. Returning to our pattern, if we use the shown conventional unit cell, there is now a problem with the definition of the lattice vectors: no integer linear combination of lattice vectors is able to produce the lattice point in the centre of the unit cell. Therefore, in order to map out the entire lattice, we need to include an extra position the basis. Since there is one hourglass at the corner of the unit cell and one in the centre, the basis is: hourglass: (0,0),(1/2,1/2) . The corresponding locations of the stars with respect to the reference lattice point are then given by: \\begin{align} \\mathbf{R}_{\\left[n_{1}, n_{2}\\right]}^{\\mathrm{corner}} &= n_{1} \\mathbf{a}_1 +n_2 \\mathbf{a}_2 \\\\ \\mathbf{R}_{\\left[n_{1}, n_{2}\\right]}^{\\mathrm{centre}} &= \\frac{1}{2}\\left( \\mathbf{a}_1 + \\mathbf{a}_2 \\right) + n_{1} \\mathbf{a}_1 +n_2 \\mathbf{a}_2, \\end{align} for n_{1}, n_{2} \\in \\mathbb{Z} . Alternatively, one can think of the crystal structure being made up from two intersecting orthogonal lattices - one centred at [0,0] and another at [1/2,1/2] , each with basis (0,0) . 4.2.4: For what type of lattice is the conventional unit cell also primitive?","title":"Conventional unit cell"},{"location":"4-crystal/4-1-realspace/#recipe-for-analysing-periodic-structures","text":"When encountering periodic systems in the wild, we can come up with a recipe for analysing them. One such recipe is as follows: Choose origin (can be an atom, but this is not necessary) Find other lattice points (identical atoms) Choose lattice vectors that translate between these lattice points: either primitive or not primitive (in which case you require an additional basis point) The lengths of lattice vectors and angle(s) between them fully define the crystal lattice Specify the basis Much like a good recipe, the definition of a lattice and a basis allows one to cook up the location of every atom in the periodic structure and thus the crystal structure.","title":"Recipe for analysing periodic structures"},{"location":"4-crystal/4-1-realspace/#example-graphene","text":"Let's put our skills to the test, analysing a real structure. Graphene, besides being super cool, is made out of a single layer of carbon atoms arranged in a honeycomb shape, with a nearest-neighbour interatomic distance of a . The locations of atomic positions is shown in panel A below: Following our recipe above, the first task is to find suitable lattice points. We start by choosing a lattice point on the (0,0) coordinate, and looking for all identical atoms. We see that not all carbon atoms have the same environment with respect to our chosen lattice point, and hence, not all carbon atoms coincide with lattice points! Only those with the same environment are valid lattice points. Panel B shows the lattice, which we identify as a triangular lattice.","title":"Example: Graphene"},{"location":"4-crystal/4-1-realspace/#primitive-unit-cell","text":"Next, we find lattice vectors, and because lattice vectors are not unique, we are free to choose them. A natural choice for primitive lattice is shown in panel C and with a little geometry, we find that \\mathbf{a}_1 and \\mathbf{a}_2 are: \\mathbf{a}_1 = [\\sqrt{3}, 0] a, \\quad \\mathbf{a}_2 = [\\sqrt{3}/2, 3/2] a. As usual, with these lattice vectors we are able to map out the entire lattice, but we must specify the repeated motif, that is, the basis. Each primitive unit cell contains two carbon atoms: one is at the reference point (0,0) and the other is located at (\\sqrt{3},1)a . When writing the basis, we want to express the coordinates as fractional coordinates of the chosen lattice vectors, and for the atom at the reference point this is easy: \\mathrm{C}(0,0) . In order to find the fractional coordinates of the second carbon atom, it is convenient to first look at the y coordinate of the atom, which is a . Because only \\mathbf{a}_2 has a nonzero y component, we easily find the fractional coordinate of \\mathbf{a}_2 : it is simply \\frac{a}{3a/2} = 2/3 . To find the fractional coordinate of \\mathbf{a}_1 , we use the fractional coordinate of \\mathbf{a}_2 . Multiplying the x component of \\mathbf{a}_2 by 2/3 yields 1/\\sqrt{3} . We know that \\begin{align} \\sqrt{3} &= f_1 a_{1,x} + f_2 a_{2,x}\\\\ &= f_1 \\sqrt{3} + 1/\\sqrt{3}, \\end{align} Bringing 1/\\sqrt{3} to the other side and dividing both sides by \\sqrt{3} yields f_1 = 2/3 . Hence the basis of the second atom is \\mathrm{C}(2/3, 2/3) .","title":"Primitive unit cell"},{"location":"4-crystal/4-1-realspace/#conventional-unit-cell_1","text":"To show why one likes conventional unit cells, in panel D we show the conventional unit cell with the lattice vectors: \\mathbf{a}_1 = [\\sqrt{3}, 0] a, \\quad \\mathbf{a}_2 = [0, 3] a. The price one pays for the convenience of orthogonal lattice vectors: the unit cell now contains four carbon atoms which need to be specified in the basis. Fortunately, the task is much more straightforward and the basis in fractional coordinates is: \\mathrm{C}(0,0), \\: \\mathrm{C}(0,1/3), \\: \\mathrm{C}(1/2, 1/2) and \\mathrm{C}(1/2, 5/6) .","title":"Conventional unit cell"},{"location":"4-crystal/4-1-realspace/#wigner-seitz-unit-cell","text":"Primitive unit cells are not unique; however, there exists a unique primitive cell which is the locus of points which are closer to a given lattice point than all other lattice points, and is known as the Wigner-Seitz cell . Much like our other unit cells, we can use a recipe to cook it up: From a given lattice point, find all neighbouring lattice points Draw lines between the reference lattice point and the neighbouring lattice points Draw perpendicular bisectors of each line Extend the perpendicular bisectors until they intersect This procedure will produce the Wigner-Seitz cell, and panel E in the graphene example shows the Wigner-Seitz cell for graphene. We note that the Wigner-Seitz cell only contains a single lattice point in the middle, but it does however contain other atoms, which must be specified in the basis. How many carbon atoms are inside the Wigner-Seitz cell of graphene? There are two methods to calculate this. We either translate the lattice and thus the Wigner-Seitz cell a bit and observe that there are two carbon atoms inside the cell. Another way to calculate the number of atoms inside the cell is by realizing that there is an atom at the lattice point itself and there is 1/3'rd of an atom at three corners of the cell. This results in 1+3\\times 1/3 = 2 atoms being inside the unit cell. For the moment, one can think of the Wigner-Seitz cell as a bit of an abstraction, but we shall see its importance as we continue on our journey.","title":"Wigner-Seitz unit cell"},{"location":"4-crystal/4-1-realspace/#three-dimensional-lattices-d","text":"Thus far we have looked at two-dimensional periodic structures, but as mentioned: most crystal structures are three dimensional. Fortunately, the tools used to describe lattices in any dimensional are applicable in other dimension. For the duration of this course, we shall be mainly concerned with cubic lattices (and closely-related friends), which have the delightful property that the lattice vectors \\mathbf{a_1} , \\mathbf{a_2} , and \\mathbf{a_3} are colinear with \\mathbf{\\hat{x}} , \\mathbf{\\hat{y}} , and \\mathbf{\\hat{z}} . In the case that the magnitude of each vector is identical (lattice constant a ), this is called a simple cubic lattice . Lattices: cubic and friends Cubic lattice All cell edge lengths equal Tetragonal Two cell edge lengths equal Orthorhombic All cell edge lengths unequal 4.2.5: How many lattice points are there in the unit cell for the simple cubic lattice? The only mineral which has a simple cubic lattice structure is a metallic allotrope of Polonium, but given the nature of polonium, there are no pretty pictures to show. Rather, shown below is the mineral pyrite (iron (II) disulphide), which at the macroscale has a very satisfying cubic structure: An incredible sample of pyrite, sourced from wikipedia and used under the creative commons licence CC BY-SA 3.0 To understand this, and other systems, we must consider additional classes of cubic lattices.","title":"Three dimensional lattices (d++)"},{"location":"4-crystal/4-1-realspace/#the-body-centred-cubic-lattice","text":"The image above (panel A ) shows a simple cubic lattice. Now, perhaps the simplest adjustment one can imagine making the simple cubic structure is the addition of a lattice point to the centre of the simple cubic cell, which is shown in panel B . This structure is known as the Body-Centred Cubic ( BCC ) lattice. In the BCC lattice, there are 8 lattice points on the corners of the cell and one in the centre and thus the conventional unit cell contains 8\\times 1/8+1 = 2 lattice points, and is therefore not primitive. What is the basis of the bcc lattice? (0,0,0) and (1/2, 1/2, 1/2) Iron at normal temperatures has a body-centred cubic structure","title":"The body-centred cubic lattice"},{"location":"4-crystal/4-1-realspace/#the-face-cantered-cubic-lattice","text":"Another way we can alter the simple cubic lattice is by adding a lattice point in every face of the simple cubic cell, which is shown in panel C . Such a structure is referred to as the Face-Centred Cubic ( FCC ) lattice. There is 1/2 of a lattice point at each face inside the lattice, in addition to the corner lattice points and thus there are a total of 8 \\times 1/8 + 6\\times 1/2 = 4 lattice points inside the unit cell, and thus it is not primitive. Materials which have a face-centred cubic lattice structure include diamond","title":"The face-cantered cubic lattice"},{"location":"4-crystal/4-1-realspace/#filling-factor","text":"Each of the three crystal structures above have a different configuration and number of atoms in the unit cell. This results in a different fraction of an unit cell being occupied by atoms. The filling factor , commonly called the atomic packing factor , measures the fraction of a volume of the unit cell that is occupied by atoms. It assumes that the atoms are solid spheres with a volume V_{\\mathrm{atom}} = \\frac{4 \\pi}{3} R^3 , where R is the radius of the sphere. For mono-atomic lattices, the filling factor F is defined as follows: F = \\frac{ N_{\\mathrm{atom}} V_{\\mathrm{atom}} }{V_{\\mathrm{cell}}}. Here N_{\\mathrm{atom}} is the number of atoms in the unit cell and V_{\\mathrm{cell}} is the volume of the unit cell. To calculate the filling factor we first need to find out what V_{\\mathrm{atom}} is. To do this, we need to \"blow up\" the atoms simultaneously until each atom touches its neighbour, and we use this \"blown up\" geometry to find an expression for R in terms of known quantities, such as the lattice constant. Filling factor of the FCC lattice The lattice is shown in panel A of the image above. We first need to increase the size of the atoms until they touch each other. The faces of the FCC are rotationally symmetric and we obtain that all four corner atoms touch the centre atom (panel B ). We identify that on the diagonal, the atoms touch each other, meaning that as the atoms have a radius R , the length of the diagonal is 4R . Given the sides of the unit cell are length a , the diagonal of the unit cell will be equal to \\sqrt{2}a . This implies that R = \\frac{a}{2\\sqrt{2}}. With the deduced radius of the atom, we can calculate V_{\\mathrm{atom}} : V_{\\mathrm{atom}} = \\frac{4\\pi}{3}R^3 = \\frac{4\\pi}{3} \\left( \\frac{a}{2\\sqrt{2}} \\right)^3 = \\frac{\\pi a^3}{12 \\sqrt{2}}. The only thing that is left for us to determine is the number of atoms in the unit cell, which we earlier noted to be 4. Therefore we calculate the filling factor to be \\begin{align} F &= \\frac{ N_{\\mathrm{atom}} V_{\\mathrm{atom}} }{V_{\\mathrm{cell}}}\\\\ &= \\frac{4 \\times \\frac{\\pi a^3}{12\\sqrt{2}}}{a^3}\\\\ &= \\frac{\\pi}{3\\sqrt{2}} \\approx 0.74 \\end{align} or put another way, approximately 74 \\% of the FCC unit cell is occupied by atoms. 4.2.5: Can you think of another lattice structure with a higher filling factor?","title":"Filling factor"},{"location":"4-crystal/4-1-realspace/#other-lattice-structures","text":"It should be noted that there exist three dimensional lattices with non-orthogonal axes (and indeed are common), and the method for their description as introduced here is identical, but just a bit messier. Perhaps surprisingly, there are only 14 distinct lattices from which all other lattices from which all other lattices can be constructed !","title":"Other lattice structures"},{"location":"4-crystal/4-1-realspace/#conclusions","text":"Lattices can be used to describe periodic systems Several important concepts to describe crystal structure were introduced: lattice, lattice vectors, basis, primitive & conventional unit cells The crystal structures with orthogonal lattice vectors were introduced, in addition to their different centring types: simple cubic, FCC, and BCC","title":"Conclusions"},{"location":"4-crystal/4-1-realspace/#exercises","text":"","title":"Exercises"},{"location":"4-crystal/4-1-realspace/#preliminary-provocations","text":"State the definition of a primitive unit cell. What can be said about its volume? Draw the conventional unit cell of a FCC and the BCC. Write down the primitive lattice vectors and the basis of each lattice. Suppose you find the primitive unit cell of a diatomic crystal. How many basis vectors do you minimally need to describe the crystal? Can a diatomic crystal require more basis vectors? Calculate the filling factor of a simple cubic lattice. Sketch the (110),(1\\bar{1}0),(111) miller planes of a simple cubic lattice.","title":"Preliminary provocations"},{"location":"4-crystal/4-1-realspace/#exercise-1-diatomic-crystal","text":"Consider the following two-dimensional diatomic crystal: Sketch the Wigner-Seitz unit cell and two other possible primitive unit cells of the crystal. If the distance between the filled cirles is a=0.28 nm, what is the area of the primitive unit cell? How would this area change if all the empty circles and the filled circles were identical? Write down one set of primitive lattice vectors and the basis for this crystal. What happens to the number of elements in the basis if all empty and filled circles were identical? Imagine expanding the lattice into the perpendicular direction z . We can define a new three-dimensional crystal by considering a periodic structure in the z direction, where the filled circles have been displaced by \\frac{a}{2} in both the x and y direction from the empty circles. The figure below shows the new arrangement of the atoms. What lattice do we obtain? Write down the basis of the three-dimensional crystal. If we consider all atoms to be the same, what lattice do we obtain? Compute the filling factor in the case where all atoms are the same.","title":"Exercise 1: Diatomic crystal"},{"location":"4-crystal/4-1-realspace/#exercise-2-diamond-lattice","text":"Consider a the diamond crystal structure structure. The following illustration shows the arrangement of the carbon atoms in a conventional unit cell. The side of the cube is a = 0.3567 nm. How is this crystal structure related to the fcc lattice? Write down one set of primitive lattice vectors and compute the volume of the corresponding primitive unit cell. How many atoms are in the primitive unit cell? Write down the basis. Determine the number of atoms in the conventional unit cell and compute its volume. What is the distance between nearest neighbouring atoms? Compute the filling factor. Although the do exist, and hopefully we shall discuss some later in the semester. \u21a9 Not to be confused with the basis from linear algebra! \u21a9","title":"Exercise 2: Diamond lattice"},{"location":"4-crystal/4-2-kspace/","text":"The reciprocal lattice \u00b6 Learn the reciprocal secrets of crystals 1 . Introduction \u00b6 Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Mathematics: vectors, the Fourier transform Text reference The material covered here is discussed in section(s) \\S 13.1, 13.2 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: In the last lecture, we learned crystallographic terminology in order to describe crystal structures. Now our goal is twofold, we will: - Study what consequences does a lattice have in reciprocal space (with the goal of considering dispersion relations) Reciprocal lattice motivation 1D case \u00b6 Previously, we discussed the reciprocal space of a simple 1D lattice. To obtain the dispersion relation we considered waves of the form e^{ikx_n} = e^{ikna}, \\quad n \\in \\mathbb{Z}, where x_n = na is the 1D lattice point. We then observed that these waves with wave vectors k and k+G , where G=2\\pi m/a with integer m , are exactly the same: e^{i(k+G)na} = e^{ikna+im2\\pi n} = e^{ikna}, where we used e^{iGx_n} = e^{i2\\pi mn} = 1. The set of points G=2\\pi m/a forms the reciprocal lattice . Let us now generalize the same idea to describe the reciprocal lattice in 3D. Extending to higher dimensions \u00b6 We start from a lattice in real space: \\mathbf{R}=n_1\\mathbf{a}_1+n_2\\mathbf{a}_2+n_3\\mathbf{a}_1, \\quad \\{n_1, n_2, n_3\\} \\in \\mathbb{Z}, where \\mathbf{a}_1 , \\mathbf{a}_2 and \\mathbf{a}_3 are the lattice vectors. The reciprocal lattice is also a lattice, but in the k -space: \\mathbf{G}=m_1\\mathbf{b}_1+m_2\\mathbf{b}_2+m_3\\mathbf{b_3}, \\quad \\{m_1, m_2, m_3\\} \\in \\mathbb{Z}. The vectors \\mathbf{b}_1 , \\mathbf{b}_2 and \\mathbf{b}_2 are the reciprocal lattice vectors . Let us now determine the reciprocal lattice vectors by requiring that waves that differ by a reciprocal lattice vector are indistinguishable. In other words, we require that e^{i\\mathbf{k}\\mathbf{R}} = e^{i(\\mathbf{k} + \\mathbf{G})\\mathbf{R}}, for any \\mathbf{R} in the lattice. Substituting the definitions of \\mathbf{R} and \\mathbf{G} we get \\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{R}} = \\mathrm{e}^{i\\sum_{\\{i,j\\}=1}^{3} n_i m_j \\mathbf{a}_i \\cdot \\mathbf{b}_j}=1. This relation holds only if \\mathbf{a_i}\\cdot\\mathbf{b_j}=2\\pi\\delta_{ij}. Indeed, after expanding the dot products in the exponent, we get \\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{R}} = \\mathrm{e}^{2\\pi i(n_1 m_1 + n_2 m_2 + n_3 m_3)}. Because n_i and m_j are both integers, the exponent evaluates to 1. The relation \\mathbf{a_i}\\cdot\\mathbf{b_j}=2\\pi\\delta_{ij} means that if we write the lattice vectors as rows of a matrix, the reciprocal lattice vectors are 2\\pi times the columns of the inverse of that matrix. 2D example: triangular lattice \u00b6 In order to gain extra intuition of the properties of the reciprocal lattice, let us study a specific example. In the previous lecture we studied the triangular lattice, which is shown in the figure below. The left panel show the real-space lattice with lattice vectors \\mathbf{a}_1 = a \\mathbf{\\hat{x}} and \\mathbf{a}_2 = a/2\\mathbf{\\hat{x}} + \\sqrt{3}a/2 \\mathbf{\\hat{y}} . While the right panel shows the corresponding reciprocal lattice and its reciprocal lattice vectors \\mathbf{b}_1 and \\mathbf{b}_2 . To find the reciprocal lattice vectors, we use the relation \\mathbf{a_i}\\cdot\\mathbf{b_j}=2\\pi\\delta_{ij}, which gives us the following equations: \\mathbf{a}_1\\cdot\\mathbf{b}_2=\\mathbf{a}_2\\cdot\\mathbf{b}_1=0, and \\mathbf{a}_1\\cdot\\mathbf{b}_1=\\mathbf{a}_2\\cdot\\mathbf{b}_2=2\\pi. We substitute \\mathbf{a_i}\\cdot\\mathbf{b_i} = \\lvert \\mathbf{a_i} \\rvert \\lvert \\mathbf{b_i} \\rvert \\cos(\\theta_i) : \\lvert \\mathbf{a}_1 \\rvert \\lvert \\mathbf{b}_1 \\rvert =\\frac{2\\pi}{\\cos(\\theta_1)} \\:\\: \\text{and} \\:\\: \\lvert \\mathbf{a}_2 \\rvert \\lvert \\mathbf{b}_2 \\rvert =\\frac{2\\pi}{\\cos(\\theta_2)}, where \\theta_i is the angle between the vectors \\mathbf{a}_i and \\mathbf{b}_i . To find the angles \\theta_1 and \\theta_2 , we use the orthogonality relations above and the fact that the angle between \\mathbf{a}_1 and \\mathbf{a}_2 is 60^\\circ . From this we conclude that \\theta_1 = \\theta_2 = 30^\\circ . Because \\lvert \\mathbf{a}_1 \\rvert = \\lvert \\mathbf{a}_2 \\rvert = a , we find \\lvert \\mathbf{b}_1 \\rvert = \\lvert \\mathbf{b}_2 \\rvert = \\frac{4\\pi}{a\\sqrt{3}}. Unsurprisingly, we find that the lengths of the reciprocal lattice vectors are equal and inversely proportional to the lattice constant a . With \\lvert \\mathbf{b}_2 \\rvert and \\mathbf{a}_1 \\perp \\mathbf{b}_2 , we easily find \\mathbf{b}_2 = \\frac{4\\pi}{a\\sqrt{3}} \\mathbf{\\hat{y}}. We follow the same procedure to find \\mathbf{b}_1 : \\mathbf{b}_1 = \\frac{4\\pi}{a\\sqrt{3}} \\left(\\frac{\\sqrt{3}}{2} \\mathbf{\\hat{x}} - \\frac{1}{2}\\mathbf{\\hat{y}} \\right). Is the choice of a set of reciprocal lattice vectors unique? If not, which other ones are possible? There are many equivalent ways to choose lattice vectors of the reciprocal lattice. In the example above we could as well use \\mathbf{b}_1 = \\frac{4\\pi}{a\\sqrt{3}} \\left(-\\frac{\\sqrt{3}}{2} \\mathbf{\\hat{x}} + \\frac{1}{2}\\mathbf{\\hat{y}} \\right) \\quad \\text{and} \\quad \\mathbf{b}_2 = -\\frac{4\\pi}{a\\sqrt{3}} \\mathbf{\\hat{y}}. There is however only one choice that satisfies the relations \\mathbf{a_i}\\cdot\\mathbf{b_j}=2\\pi\\delta_{ij} . 3D lattice example \u00b6 Let us now consider a more involved example of the 3D lattice. The explicit expression for the reciprocal lattice vectors in terms of their real space counterparts is: \\mathbf{b}_1=\\frac{2\\pi(\\mathbf{a}_2\\times\\mathbf{a}_3)}{ \\mathbf{a}_1\\cdot(\\mathbf{a}_2\\times\\mathbf{a_3})} \\mathbf{b}_2=\\frac{2\\pi(\\mathbf{a_3}\\times\\mathbf{a}_1)}{ \\mathbf{a}_1\\cdot(\\mathbf{a}_2\\times\\mathbf{a_3})} \\mathbf{b_3}=\\frac{2\\pi(\\mathbf{a}_1\\times\\mathbf{a}_2)}{ \\mathbf{a}_1\\cdot(\\mathbf{a}_2\\times\\mathbf{a_3})} Note that the denominator \\mathbf{a}_1\\cdot(\\mathbf{a}_2\\times\\mathbf{a_3}) is the volume V of the real-space unit cell spanned by the lattice vectors \\mathbf{a}_1 , \\mathbf{a}_2 and \\mathbf{a}_3 . The reciprocal lattice as a Fourier transform \u00b6 One can also think of the reciprocal lattice as a Fourier transform of the real-space lattice. For simplicity, we illustrate this for a 1D lattice (the same principles apply to a 3D lattice). We model the real-space lattice as a density function consisting of delta peaks: \\rho(x)=\\sum_{n} \\delta(x-na) We take the Fourier transform of this function to find: {\\mathcal F}_{k}\\left[\\rho(x)\\right]=\\int_{-\\infty}^\\infty \\mathrm{d}x\\ \\mathrm{e}^{ikx} \\rho(x)=\\sum_{n} \\int_{-\\infty}^\\infty \\mathrm{d}x\\ \\mathrm{e}^{ikx} \\delta(x-na)=\\sum_{n} \\mathrm{e}^{ikna} This sum is non-zero only if k=2\\pi m/a . If we recall the beginning of the lecture, then these points correspond to reciprocal lattice points G . Therefore, we rewrite this into the form {\\mathcal F}_{k}\\left[\\rho(x)\\right]=\\frac{2\\pi}{|a|}\\sum_{m} \\delta\\left(k-G\\right). Therefore, we see that the Fourier transform is non-zero only at reciprocal lattice points. In other words, Fourier transforming a real-space lattice yields a reciprocal lattice! The above result generalizes directly to three dimensions: {\\mathcal F}_\\mathbf{k}\\left[\\rho(\\mathbf{r})\\right]=\\int \\mathrm{d}\\mathbf{r}\\ \\mathrm{e}^{i\\mathbf{k}\\cdot\\mathbf{r}} \\rho(\\mathbf{r}) = \\sum_\\mathbf{G}\\delta(\\mathbf{k}-\\mathbf{G}). Periodicity of the reciprocal lattice \u00b6 In order to describe a reciprocal lattice, we need to define a primitive unit cell in reciprocal space. Previously, we learned that the choice of a primitive unit cell is not unique. However, a general convention in reciprocal space is to use the Wigner-Seitz cell which is called the 1st Brillouin zone . Because the Wigner-Seitz cell is primitive, the 1st Brillouin zone (1BZ) contains a set of unique \\mathbf{k} vectors. This means that all \\mathbf{k} vectors outside the 1st Brillouin zone are a copy of those inside the 1st Brillouin zone. For example, any \\mathbf{k'} outside the 1BZ is related to a wave vector inside 1BZ \\mathbf{k} by shifting it by reciprocal lattice vectors: \\mathbf{k'} = \\mathbf{k}+\\mathbf{G} We have already learned how to construct Wigner-Seitz cells, however here is a reminder of how a Brillouin zone looks like: Miller planes \u00b6 When fabricating crystals it is important to know both the orientation and the surface of the crystal. Different cuts of a crystal lead to different surfaces. In the chemical industry, this is especially significant because different surfaces lead to different chemical properties and thus is one of the foundations of research in catalysts. Therefore, we seek a way to describe different planes of a crystal within our developed framework. This leads us to a very important concept - Miller planes . To explain Miller planes, let's start off with a simple cubic lattice: Where |{\\bf a}_1|=|{\\bf a}_2|=|{\\bf a}_3|\\equiv a . We can cut multiple planes through the cubic lattice. Miller planes describe such planes with a set of indices. The plane designated by Miller indices (u,v,w) intersects lattice vector {\\bf a}_1 at \\frac{|{\\bf a}_1|}{u} , {\\bf a}_2 at \\frac{|{\\bf a}_2|}{v} and {\\bf a}_3 at \\frac{|{\\bf a}_3|}{w} . Miller index 0 means that the plane is parallel to that axis (intersection at \" \\frac{|{\\bf a}_3|}{0}\\rightarrow\\infty \"). A bar above a Miller index means intersection at a negative coordinate. If a crystal is symmetric under 90^\\circ rotations, then (100) , (010) and (001) are physically indistinguishable. Therefore, we use the notation \\{100\\} to indicate a whole family of these symmetry-related planes. In a cubic crystal, [100] (this is a vector) is perpendicular to (100) \\rightarrow proof in problem set. Why are these Miller planes usefull? It allows us to know the exact orientation of a crystal structure if the crystal structure is known. Conclusions \u00b6 We described how to construct a reciprocal lattice from a real-space lattice. Points in reciprocal space that differ by a reciprocal lattice vector are equivalent. Exercises \u00b6 Preliminary provocations \u00b6 Calculate \\mathbf{a}_1 \\cdot \\mathbf{b}_1 and \\mathbf{a}_2 \\cdot \\mathbf{b}_1 using the definitions of the reciprocal lattice vectors given in the lecture. Is the result what you expected? Exercise 1: Equivalence of direct and reciprocal lattice \u00b6 The volume of a primitive cell of a lattice with lattice vectors \\mathbf{a}_1, \\mathbf{a}_2, \\mathbf{a}_3 equals V = |\\mathbf{a}_1\\cdot(\\mathbf{a}_2\\times\\mathbf{a}_3)| . Find the volume of a primitive unit cell V^* = \\left| \\mathbf{b}_1 \\cdot (\\mathbf{b}_2 \\times \\mathbf{b}_3) \\right| of the corresponding reciprocal lattice. Derive the expressions for the lattice vectors \\mathbf{a}_i through the reciprocal lattice \\mathbf{b}_i . Hint Make use of the vector identity \\mathbf{A}\\times(\\mathbf{B}\\times\\mathbf{C}) = \\mathbf{B}(\\mathbf{A}\\cdot\\mathbf{C}) - \\mathbf{C}(\\mathbf{A}\\cdot\\mathbf{B}) Write down the primitive lattice vectors of the BCC lattice and calculate its reciprocal lattice vectors. Which type of lattice is the reciprocal lattice of a BCC crystal? Determine the shape of the 1st Brillouin zone. Exercise 2: Miller planes and reciprocal lattice vectors \u00b6 Consider a family of Miller planes (hkl) in a crystal. Prove that the reciprocal lattice vector \\mathbf{G} = h \\mathbf{b}_1 + k \\mathbf{b}_2 + l \\mathbf{b}_3 is perpendicular to the Miller plane (hkl) . Hint Choose two vectors that lie within the Miller plane and are not parallel to each other. Show that the distance between two adjacent Miller planes (hkl) of any lattice is d = 2\\pi/|\\mathbf{G}_\\textrm{min}| , where \\mathbf{G}_\\textrm{min} is the shortest reciprocal lattice vector perpendicular to these Miller planes. Find the family of Miller planes of the BCC lattice that has the highest density of lattice points. To solve this problem use that the density of lattice points per unit area on a Miller plane is \\rho = d/V . Here V is the volume of the primitive unit cell and d is the distance between adjacent planes given in 2.2. Exercise 3: Directions and Spacings of Miller planes \u00b6 Explain what is meant by the terms Miller planes and Miller indices. Consider a cubic crystal with one atom in the basis and a set of orthogonal primitive lattice vectors a\\hat{x} , a\\hat{y} and a\\hat{z} . Show that the direction [hkl] in this crystal is normal to the planes with Miller indices (hkl) . Show that this is not true in general. Consider for instance an orthorhombic crystal, for which the primitive lattice vectors are still orthogonal but have different lengths. Any set of Miller indices corresponds to a family of planes separated by a distance d . Show that the spacing d of the (hkl) set of planes in a cubic crystal with lattice parameter a is d = \\frac{a}{\\sqrt{h^2 + k^2 + l^2}} . Hint Recall that a family of lattice planes is an infinite set of equally separated parallel planes which taken all together contain all points of the lattice. Try computing the distance between the plane that contains the site (0,0,0) of the conventional unit cell and a plane defined by the (hkl) indices. This is only funny if you noticed the tagline of the previous section \u21a9","title":"Crystals in reciprocal space"},{"location":"4-crystal/4-2-kspace/#the-reciprocal-lattice","text":"Learn the reciprocal secrets of crystals 1 .","title":"The reciprocal lattice"},{"location":"4-crystal/4-2-kspace/#introduction","text":"Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Mathematics: vectors, the Fourier transform Text reference The material covered here is discussed in section(s) \\S 13.1, 13.2 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: In the last lecture, we learned crystallographic terminology in order to describe crystal structures. Now our goal is twofold, we will: - Study what consequences does a lattice have in reciprocal space (with the goal of considering dispersion relations)","title":"Introduction"},{"location":"4-crystal/4-2-kspace/#reciprocal-lattice-motivation-1d-case","text":"Previously, we discussed the reciprocal space of a simple 1D lattice. To obtain the dispersion relation we considered waves of the form e^{ikx_n} = e^{ikna}, \\quad n \\in \\mathbb{Z}, where x_n = na is the 1D lattice point. We then observed that these waves with wave vectors k and k+G , where G=2\\pi m/a with integer m , are exactly the same: e^{i(k+G)na} = e^{ikna+im2\\pi n} = e^{ikna}, where we used e^{iGx_n} = e^{i2\\pi mn} = 1. The set of points G=2\\pi m/a forms the reciprocal lattice . Let us now generalize the same idea to describe the reciprocal lattice in 3D.","title":"Reciprocal lattice motivation 1D case"},{"location":"4-crystal/4-2-kspace/#extending-to-higher-dimensions","text":"We start from a lattice in real space: \\mathbf{R}=n_1\\mathbf{a}_1+n_2\\mathbf{a}_2+n_3\\mathbf{a}_1, \\quad \\{n_1, n_2, n_3\\} \\in \\mathbb{Z}, where \\mathbf{a}_1 , \\mathbf{a}_2 and \\mathbf{a}_3 are the lattice vectors. The reciprocal lattice is also a lattice, but in the k -space: \\mathbf{G}=m_1\\mathbf{b}_1+m_2\\mathbf{b}_2+m_3\\mathbf{b_3}, \\quad \\{m_1, m_2, m_3\\} \\in \\mathbb{Z}. The vectors \\mathbf{b}_1 , \\mathbf{b}_2 and \\mathbf{b}_2 are the reciprocal lattice vectors . Let us now determine the reciprocal lattice vectors by requiring that waves that differ by a reciprocal lattice vector are indistinguishable. In other words, we require that e^{i\\mathbf{k}\\mathbf{R}} = e^{i(\\mathbf{k} + \\mathbf{G})\\mathbf{R}}, for any \\mathbf{R} in the lattice. Substituting the definitions of \\mathbf{R} and \\mathbf{G} we get \\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{R}} = \\mathrm{e}^{i\\sum_{\\{i,j\\}=1}^{3} n_i m_j \\mathbf{a}_i \\cdot \\mathbf{b}_j}=1. This relation holds only if \\mathbf{a_i}\\cdot\\mathbf{b_j}=2\\pi\\delta_{ij}. Indeed, after expanding the dot products in the exponent, we get \\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{R}} = \\mathrm{e}^{2\\pi i(n_1 m_1 + n_2 m_2 + n_3 m_3)}. Because n_i and m_j are both integers, the exponent evaluates to 1. The relation \\mathbf{a_i}\\cdot\\mathbf{b_j}=2\\pi\\delta_{ij} means that if we write the lattice vectors as rows of a matrix, the reciprocal lattice vectors are 2\\pi times the columns of the inverse of that matrix.","title":"Extending to higher dimensions"},{"location":"4-crystal/4-2-kspace/#2d-example-triangular-lattice","text":"In order to gain extra intuition of the properties of the reciprocal lattice, let us study a specific example. In the previous lecture we studied the triangular lattice, which is shown in the figure below. The left panel show the real-space lattice with lattice vectors \\mathbf{a}_1 = a \\mathbf{\\hat{x}} and \\mathbf{a}_2 = a/2\\mathbf{\\hat{x}} + \\sqrt{3}a/2 \\mathbf{\\hat{y}} . While the right panel shows the corresponding reciprocal lattice and its reciprocal lattice vectors \\mathbf{b}_1 and \\mathbf{b}_2 . To find the reciprocal lattice vectors, we use the relation \\mathbf{a_i}\\cdot\\mathbf{b_j}=2\\pi\\delta_{ij}, which gives us the following equations: \\mathbf{a}_1\\cdot\\mathbf{b}_2=\\mathbf{a}_2\\cdot\\mathbf{b}_1=0, and \\mathbf{a}_1\\cdot\\mathbf{b}_1=\\mathbf{a}_2\\cdot\\mathbf{b}_2=2\\pi. We substitute \\mathbf{a_i}\\cdot\\mathbf{b_i} = \\lvert \\mathbf{a_i} \\rvert \\lvert \\mathbf{b_i} \\rvert \\cos(\\theta_i) : \\lvert \\mathbf{a}_1 \\rvert \\lvert \\mathbf{b}_1 \\rvert =\\frac{2\\pi}{\\cos(\\theta_1)} \\:\\: \\text{and} \\:\\: \\lvert \\mathbf{a}_2 \\rvert \\lvert \\mathbf{b}_2 \\rvert =\\frac{2\\pi}{\\cos(\\theta_2)}, where \\theta_i is the angle between the vectors \\mathbf{a}_i and \\mathbf{b}_i . To find the angles \\theta_1 and \\theta_2 , we use the orthogonality relations above and the fact that the angle between \\mathbf{a}_1 and \\mathbf{a}_2 is 60^\\circ . From this we conclude that \\theta_1 = \\theta_2 = 30^\\circ . Because \\lvert \\mathbf{a}_1 \\rvert = \\lvert \\mathbf{a}_2 \\rvert = a , we find \\lvert \\mathbf{b}_1 \\rvert = \\lvert \\mathbf{b}_2 \\rvert = \\frac{4\\pi}{a\\sqrt{3}}. Unsurprisingly, we find that the lengths of the reciprocal lattice vectors are equal and inversely proportional to the lattice constant a . With \\lvert \\mathbf{b}_2 \\rvert and \\mathbf{a}_1 \\perp \\mathbf{b}_2 , we easily find \\mathbf{b}_2 = \\frac{4\\pi}{a\\sqrt{3}} \\mathbf{\\hat{y}}. We follow the same procedure to find \\mathbf{b}_1 : \\mathbf{b}_1 = \\frac{4\\pi}{a\\sqrt{3}} \\left(\\frac{\\sqrt{3}}{2} \\mathbf{\\hat{x}} - \\frac{1}{2}\\mathbf{\\hat{y}} \\right). Is the choice of a set of reciprocal lattice vectors unique? If not, which other ones are possible? There are many equivalent ways to choose lattice vectors of the reciprocal lattice. In the example above we could as well use \\mathbf{b}_1 = \\frac{4\\pi}{a\\sqrt{3}} \\left(-\\frac{\\sqrt{3}}{2} \\mathbf{\\hat{x}} + \\frac{1}{2}\\mathbf{\\hat{y}} \\right) \\quad \\text{and} \\quad \\mathbf{b}_2 = -\\frac{4\\pi}{a\\sqrt{3}} \\mathbf{\\hat{y}}. There is however only one choice that satisfies the relations \\mathbf{a_i}\\cdot\\mathbf{b_j}=2\\pi\\delta_{ij} .","title":"2D example: triangular lattice"},{"location":"4-crystal/4-2-kspace/#3d-lattice-example","text":"Let us now consider a more involved example of the 3D lattice. The explicit expression for the reciprocal lattice vectors in terms of their real space counterparts is: \\mathbf{b}_1=\\frac{2\\pi(\\mathbf{a}_2\\times\\mathbf{a}_3)}{ \\mathbf{a}_1\\cdot(\\mathbf{a}_2\\times\\mathbf{a_3})} \\mathbf{b}_2=\\frac{2\\pi(\\mathbf{a_3}\\times\\mathbf{a}_1)}{ \\mathbf{a}_1\\cdot(\\mathbf{a}_2\\times\\mathbf{a_3})} \\mathbf{b_3}=\\frac{2\\pi(\\mathbf{a}_1\\times\\mathbf{a}_2)}{ \\mathbf{a}_1\\cdot(\\mathbf{a}_2\\times\\mathbf{a_3})} Note that the denominator \\mathbf{a}_1\\cdot(\\mathbf{a}_2\\times\\mathbf{a_3}) is the volume V of the real-space unit cell spanned by the lattice vectors \\mathbf{a}_1 , \\mathbf{a}_2 and \\mathbf{a}_3 .","title":"3D lattice example"},{"location":"4-crystal/4-2-kspace/#the-reciprocal-lattice-as-a-fourier-transform","text":"One can also think of the reciprocal lattice as a Fourier transform of the real-space lattice. For simplicity, we illustrate this for a 1D lattice (the same principles apply to a 3D lattice). We model the real-space lattice as a density function consisting of delta peaks: \\rho(x)=\\sum_{n} \\delta(x-na) We take the Fourier transform of this function to find: {\\mathcal F}_{k}\\left[\\rho(x)\\right]=\\int_{-\\infty}^\\infty \\mathrm{d}x\\ \\mathrm{e}^{ikx} \\rho(x)=\\sum_{n} \\int_{-\\infty}^\\infty \\mathrm{d}x\\ \\mathrm{e}^{ikx} \\delta(x-na)=\\sum_{n} \\mathrm{e}^{ikna} This sum is non-zero only if k=2\\pi m/a . If we recall the beginning of the lecture, then these points correspond to reciprocal lattice points G . Therefore, we rewrite this into the form {\\mathcal F}_{k}\\left[\\rho(x)\\right]=\\frac{2\\pi}{|a|}\\sum_{m} \\delta\\left(k-G\\right). Therefore, we see that the Fourier transform is non-zero only at reciprocal lattice points. In other words, Fourier transforming a real-space lattice yields a reciprocal lattice! The above result generalizes directly to three dimensions: {\\mathcal F}_\\mathbf{k}\\left[\\rho(\\mathbf{r})\\right]=\\int \\mathrm{d}\\mathbf{r}\\ \\mathrm{e}^{i\\mathbf{k}\\cdot\\mathbf{r}} \\rho(\\mathbf{r}) = \\sum_\\mathbf{G}\\delta(\\mathbf{k}-\\mathbf{G}).","title":"The reciprocal lattice as a Fourier transform"},{"location":"4-crystal/4-2-kspace/#periodicity-of-the-reciprocal-lattice","text":"In order to describe a reciprocal lattice, we need to define a primitive unit cell in reciprocal space. Previously, we learned that the choice of a primitive unit cell is not unique. However, a general convention in reciprocal space is to use the Wigner-Seitz cell which is called the 1st Brillouin zone . Because the Wigner-Seitz cell is primitive, the 1st Brillouin zone (1BZ) contains a set of unique \\mathbf{k} vectors. This means that all \\mathbf{k} vectors outside the 1st Brillouin zone are a copy of those inside the 1st Brillouin zone. For example, any \\mathbf{k'} outside the 1BZ is related to a wave vector inside 1BZ \\mathbf{k} by shifting it by reciprocal lattice vectors: \\mathbf{k'} = \\mathbf{k}+\\mathbf{G} We have already learned how to construct Wigner-Seitz cells, however here is a reminder of how a Brillouin zone looks like:","title":"Periodicity of the reciprocal lattice"},{"location":"4-crystal/4-2-kspace/#miller-planes","text":"When fabricating crystals it is important to know both the orientation and the surface of the crystal. Different cuts of a crystal lead to different surfaces. In the chemical industry, this is especially significant because different surfaces lead to different chemical properties and thus is one of the foundations of research in catalysts. Therefore, we seek a way to describe different planes of a crystal within our developed framework. This leads us to a very important concept - Miller planes . To explain Miller planes, let's start off with a simple cubic lattice: Where |{\\bf a}_1|=|{\\bf a}_2|=|{\\bf a}_3|\\equiv a . We can cut multiple planes through the cubic lattice. Miller planes describe such planes with a set of indices. The plane designated by Miller indices (u,v,w) intersects lattice vector {\\bf a}_1 at \\frac{|{\\bf a}_1|}{u} , {\\bf a}_2 at \\frac{|{\\bf a}_2|}{v} and {\\bf a}_3 at \\frac{|{\\bf a}_3|}{w} . Miller index 0 means that the plane is parallel to that axis (intersection at \" \\frac{|{\\bf a}_3|}{0}\\rightarrow\\infty \"). A bar above a Miller index means intersection at a negative coordinate. If a crystal is symmetric under 90^\\circ rotations, then (100) , (010) and (001) are physically indistinguishable. Therefore, we use the notation \\{100\\} to indicate a whole family of these symmetry-related planes. In a cubic crystal, [100] (this is a vector) is perpendicular to (100) \\rightarrow proof in problem set. Why are these Miller planes usefull? It allows us to know the exact orientation of a crystal structure if the crystal structure is known.","title":"Miller planes"},{"location":"4-crystal/4-2-kspace/#conclusions","text":"We described how to construct a reciprocal lattice from a real-space lattice. Points in reciprocal space that differ by a reciprocal lattice vector are equivalent.","title":"Conclusions"},{"location":"4-crystal/4-2-kspace/#exercises","text":"","title":"Exercises"},{"location":"4-crystal/4-2-kspace/#preliminary-provocations","text":"Calculate \\mathbf{a}_1 \\cdot \\mathbf{b}_1 and \\mathbf{a}_2 \\cdot \\mathbf{b}_1 using the definitions of the reciprocal lattice vectors given in the lecture. Is the result what you expected?","title":"Preliminary provocations"},{"location":"4-crystal/4-2-kspace/#exercise-1-equivalence-of-direct-and-reciprocal-lattice","text":"The volume of a primitive cell of a lattice with lattice vectors \\mathbf{a}_1, \\mathbf{a}_2, \\mathbf{a}_3 equals V = |\\mathbf{a}_1\\cdot(\\mathbf{a}_2\\times\\mathbf{a}_3)| . Find the volume of a primitive unit cell V^* = \\left| \\mathbf{b}_1 \\cdot (\\mathbf{b}_2 \\times \\mathbf{b}_3) \\right| of the corresponding reciprocal lattice. Derive the expressions for the lattice vectors \\mathbf{a}_i through the reciprocal lattice \\mathbf{b}_i . Hint Make use of the vector identity \\mathbf{A}\\times(\\mathbf{B}\\times\\mathbf{C}) = \\mathbf{B}(\\mathbf{A}\\cdot\\mathbf{C}) - \\mathbf{C}(\\mathbf{A}\\cdot\\mathbf{B}) Write down the primitive lattice vectors of the BCC lattice and calculate its reciprocal lattice vectors. Which type of lattice is the reciprocal lattice of a BCC crystal? Determine the shape of the 1st Brillouin zone.","title":"Exercise 1: Equivalence of direct and reciprocal lattice"},{"location":"4-crystal/4-2-kspace/#exercise-2-miller-planes-and-reciprocal-lattice-vectors","text":"Consider a family of Miller planes (hkl) in a crystal. Prove that the reciprocal lattice vector \\mathbf{G} = h \\mathbf{b}_1 + k \\mathbf{b}_2 + l \\mathbf{b}_3 is perpendicular to the Miller plane (hkl) . Hint Choose two vectors that lie within the Miller plane and are not parallel to each other. Show that the distance between two adjacent Miller planes (hkl) of any lattice is d = 2\\pi/|\\mathbf{G}_\\textrm{min}| , where \\mathbf{G}_\\textrm{min} is the shortest reciprocal lattice vector perpendicular to these Miller planes. Find the family of Miller planes of the BCC lattice that has the highest density of lattice points. To solve this problem use that the density of lattice points per unit area on a Miller plane is \\rho = d/V . Here V is the volume of the primitive unit cell and d is the distance between adjacent planes given in 2.2.","title":"Exercise 2: Miller planes and reciprocal lattice vectors"},{"location":"4-crystal/4-2-kspace/#exercise-3-directions-and-spacings-of-miller-planes","text":"Explain what is meant by the terms Miller planes and Miller indices. Consider a cubic crystal with one atom in the basis and a set of orthogonal primitive lattice vectors a\\hat{x} , a\\hat{y} and a\\hat{z} . Show that the direction [hkl] in this crystal is normal to the planes with Miller indices (hkl) . Show that this is not true in general. Consider for instance an orthorhombic crystal, for which the primitive lattice vectors are still orthogonal but have different lengths. Any set of Miller indices corresponds to a family of planes separated by a distance d . Show that the spacing d of the (hkl) set of planes in a cubic crystal with lattice parameter a is d = \\frac{a}{\\sqrt{h^2 + k^2 + l^2}} . Hint Recall that a family of lattice planes is an infinite set of equally separated parallel planes which taken all together contain all points of the lattice. Try computing the distance between the plane that contains the site (0,0,0) of the conventional unit cell and a plane defined by the (hkl) indices. This is only funny if you noticed the tagline of the previous section \u21a9","title":"Exercise 3: Directions and Spacings of Miller planes"},{"location":"4-crystal/4-3-scattering/","text":"Scattering \u00b6 Introduction \u00b6 Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Wave mechanics: diffraction Mathematics: The Fourier transform Text reference The material covered here is discussed in section(s) \\S 14 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Reciprocal lattice: Laue conditions \u00b6 Reciprocal lattice manifests directly in the diffraction experiments. A diffraction experiment uses the crystal as a target and scatters high energy particles (X-rays, neutrons, or electrons) off of it. As a result of interference between mutiple waves, the scattered radiation reveals the reciprocal lattice of the crystal. In order to find the relationship between the incoming wave and the scattered one, let us consider a lattice of atoms separated by a lattice vector \\mathbf{R} . An incoming wave with wave vector \\mathbf{k} is incident upon the lattice. After scattering, the outgoing wave's wave vector is \\mathbf{k'} . We assume that the atomic scattering is elastic (does not cause an energy loss), such that |\\mathbf{k'}|=|\\mathbf{k}| . Below we present a simple sketch of two different atoms scattering an incoming wave. Observe that the bottom ray travels a larger distance compared to the upper ray. The difference in distance results in a relative phase shift between the rays \\Delta \\phi . With a bit of geometry, we find that the extra distance traveled by the lower ray relative to the upper one is x_{\\mathrm{extra}} = \\Delta x_1+\\Delta x_2 = \\cos(\\theta) \\lvert R \\rvert + \\cos(\\theta') \\lvert R \\rvert. As a result of the travel distance, the phase difference is: \\begin{align} \\Delta \\phi &= \\lvert\\mathbf{k} \\rvert(\\Delta x_1+\\Delta x_2)\\\\ &= \\lvert\\mathbf{k}\\rvert \\lvert\\mathbf{R}\\rvert(\\cos(\\theta)+\\cos(\\theta'))\\\\ &= \\mathbf{k'}\\cdot \\mathbf{R} - \\mathbf{k}\\cdot \\mathbf{R} = (\\mathbf{k'} - \\mathbf{k}) \\cdot \\mathbf{R}. \\end{align} However, that is only a phase difference between waves scattered off of two atoms. To find the outgoing wave's amplitude, we must sum over scattered waves from each and every atom in the lattice: A\\propto\\sum_\\mathbf{R}\\mathrm{e}^{i\\left(\\Delta \\phi-\\omega t\\right)} = \\sum_\\mathbf{R}\\mathrm{e}^{i\\left((\\mathbf{k'}-\\mathbf{k})\\cdot\\mathbf{R}-\\omega t\\right)}. The above sum is non-zero if and only if the scattered waves interfere constructively i.e. the phase difference equals 2\\pi n , where n is an integer. Furthermore, we know that real and reciprocal lattice vectors are related by \\mathbf{G} \\cdot \\mathbf{R} = 2 \\pi n . Therefore, we conclude that the difference between incoming and outgoing waves must be: \\mathbf{k'}-\\mathbf{k}=\\mathbf{G}. In other words, if the difference of the wavevector between the incoming and outgoing wave vectors co\u00efncides with a reciprocal lattice point, we expect constructive interference. This requirement is known as the Laue condition . As a result, the interference pattern produced in diffraction experiments is a direct measurement of the reciprocal lattice! Structure factor \u00b6 Above we assumed that the unit cell contains only a single atom. What if the basis contains more atoms though? In the figure below we show a simple lattice which contains multiple atoms in the unit cell. Note, the unit cell does not have to be primitive! Let \\mathbf{R} be the lattice and let \\mathbf{R}+\\mathbf{r}_j be the location of the atoms in the unit cell. The distance \\mathbf{r}_j is taken with respect to lattice point from which we construct the unit cell. Similar to before, we calculate the amplitude of the scattered wave. However, now there are multiple atoms in the unit cell and each of these atoms acquires a phase shift of its own. In order to keep track of the atoms, we define \\mathbf{r}_j to be the location of atom j in the unit cell. The distance \\mathbf{r}_j is defined with respect to the lattice point from which we construct the unit cell. In order to calculate the amplitude of the scattered wave, we must sum not only over all the lattice points but also over the atoms in a single unit cell: \\begin{align} A &\\propto \\sum_\\mathbf{R} \\sum_j f_j \\mathrm{e}^{i\\left(\\mathbf{G}\\cdot(\\mathbf{R}+\\mathbf{r}_j)-\\omega t\\right)}\\\\ &= \\sum_\\mathbf{R}\\mathrm{e}^{i\\left(\\mathbf{G}\\cdot\\mathbf{R}-\\omega t\\right)}\\sum_j f_j\\ \\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{r}_j} \\end{align} where f_j is the scattering amplitude off of a single atom, and it is called the form factor . The form factor mainly depends on the chemical element, nature of the scattered wave, and finer details like the electrical environment of the atom. The first part of the equation above is the familiar Laue condition, and it requires that the scattered wave satisfies the Laue condition. The second part gives the amplitude of the scattered wave, and it is called the structure factor : S(\\mathbf{G})=\\sum_j f_j\\ \\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{r}_j}. In diffraction experiments, the intensity of the scattered wave is I \\propto A^2 Therefore, the intensity of a scattered wave depends on the structure factor I \\propto S(\\mathbf{G})^2 . Because the structure factor depends on the form factors and the positions of the basis atoms, by studying the visibility of different diffraction peaks we may learn the locations of atoms within the unit cell. Non-primitive unit cell \u00b6 Laue conditions allow scattering as long as the scattering wave vector is a reciprocal lattice vector. However if we consider a non-primitive unit cell of the direct lattice, the reciprocal lattice contains more lattice points, seemingly leading to additional interference peaks. Computing the structure factor allows us to resolve this apparent contradiction. Calculate the structure factor in which there is a single atom the unit cell located at the lattice point. Do any diffraction peaks dissapear? \\mathbf{r}_1=(0,0,0)\\rightarrow S=f_1 . In this case, each reciprocal lattice point gives one interference peak, none of which are absent. As a demonstration of how it happens, let us compute the structure factor of the FCC lattice using the conventional unit cell in the real space. The basis of the conventional FCC unit cell contains four identical atoms. With respect to the reference lattice point, these attoms are located at \\begin{aligned} \\mathbf{r}_1&=(0,0,0)\\\\ \\mathbf{r}_2&=\\frac{1}{2}(\\mathbf{a}_1+\\mathbf{a}_2)\\\\ \\mathbf{r}_3&=\\frac{1}{2}(\\mathbf{a}_2+\\mathbf{a}_3)\\\\ \\mathbf{r}_4&=\\frac{1}{2}(\\mathbf{a}_3+\\mathbf{a}_1), \\end{aligned} with f_1=f_2=f_3=f_4\\equiv f . Let the reciprocal lattice be described by \\mathbf{G}=h\\mathbf{b}_1+k\\mathbf{b}_2+l\\mathbf{b}_3 , where h , k and l are integers. Using the basis described above and the reciprocal lattice, we calculate the structure factor: \\begin{aligned} S&=f\\left(\\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{r}_1}+\\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{r}_2}+\\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{r}_3}+\\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{r}_4}\\right)\\\\ &=f\\left(1+\\mathrm{e}^{i(h\\mathbf{b}_1\\cdot\\mathbf{a}_1+k\\mathbf{b}_2\\cdot\\mathbf{a}_2)/2}+\\mathrm{e}^{i(k\\mathbf{b}_2\\cdot\\mathbf{a}_2+l\\mathbf{b}_3\\cdot\\mathbf{a}_3)/2}+\\mathrm{e}^{i(h\\mathbf{b}_1\\cdot\\mathbf{a}_1+l\\mathbf{b}_3\\cdot\\mathbf{a}_3)/2}\\right)\\\\ &=f\\left(1+\\mathrm{e}^{i\\pi(h+k)}+\\mathrm{e}^{i\\pi(k+l)}+\\mathrm{e}^{i\\pi(h+l)}\\right). \\end{aligned} Because h , k and l are integers, all exponents are either +1 or -1 , and the interference is only present if S = \\begin{cases} 4f, \\: \\mathrm{if} \\: h, \\: k, \\: \\mathrm{and} \\: l \\: \\mathrm{are \\: all \\: even \\: or \\: odd,}\\\\ 0, \\: \\mathrm{in \\: all \\: other \\: cases}. \\end{cases} We now see that the reciprocal lattice points with nonzero amplitude exactly form the reciprocal lattice of the FCC lattice. Powder Diffraction \u00b6 The easiest way to do diffraction measurements is to take a crystal, shoot an X-ray beam through it and measure the direction of outgoing waves. However growing a single crystal may be hard because many materials are polycrystalline A simple alternative is to perform powder diffraction . By crushing the crystal into a powder, the small crystallites are now orientated in random directions. This improves the chances of fulfilling the Laue condition for a fixed direction incoming beam. The experiment is illustrated in the figure above. The result is that the diffracted beam exits the sample via concentric circles at discrete deflection angles 2 \\theta . In order to deduce the values of \\theta of a specific crystal, let us put the Laue condition into a more practical form. We first take the modulus squared of both sides: \\left|\\mathbf{G}\\right|^2 = \\left|\\mathbf{k'}-\\mathbf{k} \\right|^2 \\\\ G^2 = 2k^2-2\\mathbf{k'} \\cdot \\mathbf{k}, where we used |\\mathbf{k'}| = |\\mathbf{k}| . We then substitute the Laue condition \\mathbf{k'} = \\mathbf{k}+\\mathbf{G} : \\begin{align} \\lvert \\mathbf{G} \\rvert ^2 &= 2k^2-2 \\left(\\mathbf{k}+\\mathbf{G}\\right) \\cdot \\mathbf{k} \\\\ &= -2 \\mathbf{G} \\cdot \\mathbf{k}. \\end{align} Using \\mathbf{k} \\cdot \\mathbf{G} = \\lvert \\mathbf{k} \\rvert \\lvert \\mathbf{G} \\rvert \\cos(\\phi) , we obtain \\left| \\mathbf{G} \\right| = -2 \\lvert \\mathbf{k} \\rvert \\cos (\\phi). Note, \\phi is the angle between the vector \\mathbf{k} and \\mathbf{G} , which is not the same as the angle between the incoming and scattered waves \\theta . We are nearly there, but we are left with finding out the relation between \\phi and \\theta . Recall the concept of Miller planes. These are sets of planes identified by their Miller indices (h,k,l) which intersect the lattice vectors at \\mathbf{a}_1 / h , \\mathbf{a}_22 / k and \\mathbf{a}_3 / l . It turns out that Miller planes are normal to the reciprocal lattice vector \\mathbf{G} = h \\mathbf{b}_1 + k \\mathbf{b}_2 + l \\mathbf{b}_3 and the distance between subsequent Miller planes is d_{hkl} = 2 \\pi/\\lvert \\mathbf{G} \\rvert (you will derive this in today's exercise. Substituting the expression for \\lvert \\mathbf{G} \\rvert into the expression for the distance between Miller planes we get: d_{hkl} \\cos (\\phi) = -\\frac{\\pi}{\\lvert \\mathbf{k} \\rvert}. We know that \\lvert \\mathbf{k} \\rvert is related to the wavelength by \\lvert \\mathbf{k} \\rvert = 2\\pi/\\lambda . Therefore, we can write the equation above as 2 d_{hkl} \\cos (\\phi) = -\\lambda. Lastly, we express the equation in terms of the deflection angle through the relation \\phi = \\theta + \\pi/2 . With this, one can finally derive Bragg's Law : \\lambda = 2 d_{hkl} \\sin(\\theta) Bragg's law allows us to obtain atomic distances in the crystal d_{hkl} through powder diffraction experiments! Conclusions \u00b6 Exercises \u00b6 Preliminary provocations \u00b6 Why is the amplitude of a scattered wave zero if \\mathbf{k'}-\\mathbf{k} \\neq \\mathbf{G} ? Suppose we have a unit cell with a single atom in it. Can any intensity peaks dissapear as a result of the structure factor? Can increasing the unit cell in real space introduce new diffraction peaks due to reciprocal lattice having more points? Exercise 3: X-ray scattering in 2D \u00b6 Consider a two-dimensional crystal with a rectangular lattice and lattice vectors \\mathbf{a}_1 = (0.468, 0) nm and \\mathbf{a}_2 = (0, 0.342) nm (so that \\mathbf{a}_1 points along x -axis and \\mathbf{a}_2 points along y -axis). Sketch the reciprocal lattice of this crystal. Consider an X-ray diffraction experiment performed on this crystal using monochromatic X-rays with wavelength 0.166 nm. By assuming elastic scattering, find the magnitude of the wave vectors of the incident and reflected X-ray beams. On the reciprocal lattice sketched in 3.1, draw the \"scattering triangle\" corresponding to the diffraction from (210) planes. To do that use the Laue condition \\Delta \\mathbf{k} = \\mathbf{G} for the constructive interference of diffracted beams. Exercise 4: Structure factors and powder diffraction \u00b6 Compute the structure factor \\mathbf{S} of the BCC lattice. Which diffraction peaks are missing? How does this structure factor change if the atoms in the center of the conventional unit cell have a different form factor from the atoms at the corner of the conventional unit cell? A student carried out X-ray powder diffraction on Chromium (Cr) which is known to have a BCC structure. The first five diffraction peaks are given below. Furthermore, the student took the liberty of assigning Miller indices to the peaks. Were the peaks assigned correctly? Fix any mistakes and explain your reasoning. Calculate the lattice constant, a , of the chromium bcc unit cell. Note that X-ray diffraction was carried out using Cu K- \\alpha ( 1.5406 \\unicode{xC5} ) radiation.","title":"Scattering"},{"location":"4-crystal/4-3-scattering/#scattering","text":"","title":"Scattering"},{"location":"4-crystal/4-3-scattering/#introduction","text":"Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Wave mechanics: diffraction Mathematics: The Fourier transform Text reference The material covered here is discussed in section(s) \\S 14 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"4-crystal/4-3-scattering/#reciprocal-lattice-laue-conditions","text":"Reciprocal lattice manifests directly in the diffraction experiments. A diffraction experiment uses the crystal as a target and scatters high energy particles (X-rays, neutrons, or electrons) off of it. As a result of interference between mutiple waves, the scattered radiation reveals the reciprocal lattice of the crystal. In order to find the relationship between the incoming wave and the scattered one, let us consider a lattice of atoms separated by a lattice vector \\mathbf{R} . An incoming wave with wave vector \\mathbf{k} is incident upon the lattice. After scattering, the outgoing wave's wave vector is \\mathbf{k'} . We assume that the atomic scattering is elastic (does not cause an energy loss), such that |\\mathbf{k'}|=|\\mathbf{k}| . Below we present a simple sketch of two different atoms scattering an incoming wave. Observe that the bottom ray travels a larger distance compared to the upper ray. The difference in distance results in a relative phase shift between the rays \\Delta \\phi . With a bit of geometry, we find that the extra distance traveled by the lower ray relative to the upper one is x_{\\mathrm{extra}} = \\Delta x_1+\\Delta x_2 = \\cos(\\theta) \\lvert R \\rvert + \\cos(\\theta') \\lvert R \\rvert. As a result of the travel distance, the phase difference is: \\begin{align} \\Delta \\phi &= \\lvert\\mathbf{k} \\rvert(\\Delta x_1+\\Delta x_2)\\\\ &= \\lvert\\mathbf{k}\\rvert \\lvert\\mathbf{R}\\rvert(\\cos(\\theta)+\\cos(\\theta'))\\\\ &= \\mathbf{k'}\\cdot \\mathbf{R} - \\mathbf{k}\\cdot \\mathbf{R} = (\\mathbf{k'} - \\mathbf{k}) \\cdot \\mathbf{R}. \\end{align} However, that is only a phase difference between waves scattered off of two atoms. To find the outgoing wave's amplitude, we must sum over scattered waves from each and every atom in the lattice: A\\propto\\sum_\\mathbf{R}\\mathrm{e}^{i\\left(\\Delta \\phi-\\omega t\\right)} = \\sum_\\mathbf{R}\\mathrm{e}^{i\\left((\\mathbf{k'}-\\mathbf{k})\\cdot\\mathbf{R}-\\omega t\\right)}. The above sum is non-zero if and only if the scattered waves interfere constructively i.e. the phase difference equals 2\\pi n , where n is an integer. Furthermore, we know that real and reciprocal lattice vectors are related by \\mathbf{G} \\cdot \\mathbf{R} = 2 \\pi n . Therefore, we conclude that the difference between incoming and outgoing waves must be: \\mathbf{k'}-\\mathbf{k}=\\mathbf{G}. In other words, if the difference of the wavevector between the incoming and outgoing wave vectors co\u00efncides with a reciprocal lattice point, we expect constructive interference. This requirement is known as the Laue condition . As a result, the interference pattern produced in diffraction experiments is a direct measurement of the reciprocal lattice!","title":"Reciprocal lattice: Laue conditions"},{"location":"4-crystal/4-3-scattering/#structure-factor","text":"Above we assumed that the unit cell contains only a single atom. What if the basis contains more atoms though? In the figure below we show a simple lattice which contains multiple atoms in the unit cell. Note, the unit cell does not have to be primitive! Let \\mathbf{R} be the lattice and let \\mathbf{R}+\\mathbf{r}_j be the location of the atoms in the unit cell. The distance \\mathbf{r}_j is taken with respect to lattice point from which we construct the unit cell. Similar to before, we calculate the amplitude of the scattered wave. However, now there are multiple atoms in the unit cell and each of these atoms acquires a phase shift of its own. In order to keep track of the atoms, we define \\mathbf{r}_j to be the location of atom j in the unit cell. The distance \\mathbf{r}_j is defined with respect to the lattice point from which we construct the unit cell. In order to calculate the amplitude of the scattered wave, we must sum not only over all the lattice points but also over the atoms in a single unit cell: \\begin{align} A &\\propto \\sum_\\mathbf{R} \\sum_j f_j \\mathrm{e}^{i\\left(\\mathbf{G}\\cdot(\\mathbf{R}+\\mathbf{r}_j)-\\omega t\\right)}\\\\ &= \\sum_\\mathbf{R}\\mathrm{e}^{i\\left(\\mathbf{G}\\cdot\\mathbf{R}-\\omega t\\right)}\\sum_j f_j\\ \\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{r}_j} \\end{align} where f_j is the scattering amplitude off of a single atom, and it is called the form factor . The form factor mainly depends on the chemical element, nature of the scattered wave, and finer details like the electrical environment of the atom. The first part of the equation above is the familiar Laue condition, and it requires that the scattered wave satisfies the Laue condition. The second part gives the amplitude of the scattered wave, and it is called the structure factor : S(\\mathbf{G})=\\sum_j f_j\\ \\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{r}_j}. In diffraction experiments, the intensity of the scattered wave is I \\propto A^2 Therefore, the intensity of a scattered wave depends on the structure factor I \\propto S(\\mathbf{G})^2 . Because the structure factor depends on the form factors and the positions of the basis atoms, by studying the visibility of different diffraction peaks we may learn the locations of atoms within the unit cell.","title":"Structure factor"},{"location":"4-crystal/4-3-scattering/#non-primitive-unit-cell","text":"Laue conditions allow scattering as long as the scattering wave vector is a reciprocal lattice vector. However if we consider a non-primitive unit cell of the direct lattice, the reciprocal lattice contains more lattice points, seemingly leading to additional interference peaks. Computing the structure factor allows us to resolve this apparent contradiction. Calculate the structure factor in which there is a single atom the unit cell located at the lattice point. Do any diffraction peaks dissapear? \\mathbf{r}_1=(0,0,0)\\rightarrow S=f_1 . In this case, each reciprocal lattice point gives one interference peak, none of which are absent. As a demonstration of how it happens, let us compute the structure factor of the FCC lattice using the conventional unit cell in the real space. The basis of the conventional FCC unit cell contains four identical atoms. With respect to the reference lattice point, these attoms are located at \\begin{aligned} \\mathbf{r}_1&=(0,0,0)\\\\ \\mathbf{r}_2&=\\frac{1}{2}(\\mathbf{a}_1+\\mathbf{a}_2)\\\\ \\mathbf{r}_3&=\\frac{1}{2}(\\mathbf{a}_2+\\mathbf{a}_3)\\\\ \\mathbf{r}_4&=\\frac{1}{2}(\\mathbf{a}_3+\\mathbf{a}_1), \\end{aligned} with f_1=f_2=f_3=f_4\\equiv f . Let the reciprocal lattice be described by \\mathbf{G}=h\\mathbf{b}_1+k\\mathbf{b}_2+l\\mathbf{b}_3 , where h , k and l are integers. Using the basis described above and the reciprocal lattice, we calculate the structure factor: \\begin{aligned} S&=f\\left(\\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{r}_1}+\\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{r}_2}+\\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{r}_3}+\\mathrm{e}^{i\\mathbf{G}\\cdot\\mathbf{r}_4}\\right)\\\\ &=f\\left(1+\\mathrm{e}^{i(h\\mathbf{b}_1\\cdot\\mathbf{a}_1+k\\mathbf{b}_2\\cdot\\mathbf{a}_2)/2}+\\mathrm{e}^{i(k\\mathbf{b}_2\\cdot\\mathbf{a}_2+l\\mathbf{b}_3\\cdot\\mathbf{a}_3)/2}+\\mathrm{e}^{i(h\\mathbf{b}_1\\cdot\\mathbf{a}_1+l\\mathbf{b}_3\\cdot\\mathbf{a}_3)/2}\\right)\\\\ &=f\\left(1+\\mathrm{e}^{i\\pi(h+k)}+\\mathrm{e}^{i\\pi(k+l)}+\\mathrm{e}^{i\\pi(h+l)}\\right). \\end{aligned} Because h , k and l are integers, all exponents are either +1 or -1 , and the interference is only present if S = \\begin{cases} 4f, \\: \\mathrm{if} \\: h, \\: k, \\: \\mathrm{and} \\: l \\: \\mathrm{are \\: all \\: even \\: or \\: odd,}\\\\ 0, \\: \\mathrm{in \\: all \\: other \\: cases}. \\end{cases} We now see that the reciprocal lattice points with nonzero amplitude exactly form the reciprocal lattice of the FCC lattice.","title":"Non-primitive unit cell"},{"location":"4-crystal/4-3-scattering/#powder-diffraction","text":"The easiest way to do diffraction measurements is to take a crystal, shoot an X-ray beam through it and measure the direction of outgoing waves. However growing a single crystal may be hard because many materials are polycrystalline A simple alternative is to perform powder diffraction . By crushing the crystal into a powder, the small crystallites are now orientated in random directions. This improves the chances of fulfilling the Laue condition for a fixed direction incoming beam. The experiment is illustrated in the figure above. The result is that the diffracted beam exits the sample via concentric circles at discrete deflection angles 2 \\theta . In order to deduce the values of \\theta of a specific crystal, let us put the Laue condition into a more practical form. We first take the modulus squared of both sides: \\left|\\mathbf{G}\\right|^2 = \\left|\\mathbf{k'}-\\mathbf{k} \\right|^2 \\\\ G^2 = 2k^2-2\\mathbf{k'} \\cdot \\mathbf{k}, where we used |\\mathbf{k'}| = |\\mathbf{k}| . We then substitute the Laue condition \\mathbf{k'} = \\mathbf{k}+\\mathbf{G} : \\begin{align} \\lvert \\mathbf{G} \\rvert ^2 &= 2k^2-2 \\left(\\mathbf{k}+\\mathbf{G}\\right) \\cdot \\mathbf{k} \\\\ &= -2 \\mathbf{G} \\cdot \\mathbf{k}. \\end{align} Using \\mathbf{k} \\cdot \\mathbf{G} = \\lvert \\mathbf{k} \\rvert \\lvert \\mathbf{G} \\rvert \\cos(\\phi) , we obtain \\left| \\mathbf{G} \\right| = -2 \\lvert \\mathbf{k} \\rvert \\cos (\\phi). Note, \\phi is the angle between the vector \\mathbf{k} and \\mathbf{G} , which is not the same as the angle between the incoming and scattered waves \\theta . We are nearly there, but we are left with finding out the relation between \\phi and \\theta . Recall the concept of Miller planes. These are sets of planes identified by their Miller indices (h,k,l) which intersect the lattice vectors at \\mathbf{a}_1 / h , \\mathbf{a}_22 / k and \\mathbf{a}_3 / l . It turns out that Miller planes are normal to the reciprocal lattice vector \\mathbf{G} = h \\mathbf{b}_1 + k \\mathbf{b}_2 + l \\mathbf{b}_3 and the distance between subsequent Miller planes is d_{hkl} = 2 \\pi/\\lvert \\mathbf{G} \\rvert (you will derive this in today's exercise. Substituting the expression for \\lvert \\mathbf{G} \\rvert into the expression for the distance between Miller planes we get: d_{hkl} \\cos (\\phi) = -\\frac{\\pi}{\\lvert \\mathbf{k} \\rvert}. We know that \\lvert \\mathbf{k} \\rvert is related to the wavelength by \\lvert \\mathbf{k} \\rvert = 2\\pi/\\lambda . Therefore, we can write the equation above as 2 d_{hkl} \\cos (\\phi) = -\\lambda. Lastly, we express the equation in terms of the deflection angle through the relation \\phi = \\theta + \\pi/2 . With this, one can finally derive Bragg's Law : \\lambda = 2 d_{hkl} \\sin(\\theta) Bragg's law allows us to obtain atomic distances in the crystal d_{hkl} through powder diffraction experiments!","title":"Powder Diffraction"},{"location":"4-crystal/4-3-scattering/#conclusions","text":"","title":"Conclusions"},{"location":"4-crystal/4-3-scattering/#exercises","text":"","title":"Exercises"},{"location":"4-crystal/4-3-scattering/#preliminary-provocations","text":"Why is the amplitude of a scattered wave zero if \\mathbf{k'}-\\mathbf{k} \\neq \\mathbf{G} ? Suppose we have a unit cell with a single atom in it. Can any intensity peaks dissapear as a result of the structure factor? Can increasing the unit cell in real space introduce new diffraction peaks due to reciprocal lattice having more points?","title":"Preliminary provocations"},{"location":"4-crystal/4-3-scattering/#exercise-3-x-ray-scattering-in-2d","text":"Consider a two-dimensional crystal with a rectangular lattice and lattice vectors \\mathbf{a}_1 = (0.468, 0) nm and \\mathbf{a}_2 = (0, 0.342) nm (so that \\mathbf{a}_1 points along x -axis and \\mathbf{a}_2 points along y -axis). Sketch the reciprocal lattice of this crystal. Consider an X-ray diffraction experiment performed on this crystal using monochromatic X-rays with wavelength 0.166 nm. By assuming elastic scattering, find the magnitude of the wave vectors of the incident and reflected X-ray beams. On the reciprocal lattice sketched in 3.1, draw the \"scattering triangle\" corresponding to the diffraction from (210) planes. To do that use the Laue condition \\Delta \\mathbf{k} = \\mathbf{G} for the constructive interference of diffracted beams.","title":"Exercise 3: X-ray scattering in 2D"},{"location":"4-crystal/4-3-scattering/#exercise-4-structure-factors-and-powder-diffraction","text":"Compute the structure factor \\mathbf{S} of the BCC lattice. Which diffraction peaks are missing? How does this structure factor change if the atoms in the center of the conventional unit cell have a different form factor from the atoms at the corner of the conventional unit cell? A student carried out X-ray powder diffraction on Chromium (Cr) which is known to have a BCC structure. The first five diffraction peaks are given below. Furthermore, the student took the liberty of assigning Miller indices to the peaks. Were the peaks assigned correctly? Fix any mistakes and explain your reasoning. Calculate the lattice constant, a , of the chromium bcc unit cell. Note that X-ray diffraction was carried out using Cu K- \\alpha ( 1.5406 \\unicode{xC5} ) radiation.","title":"Exercise 4: Structure factors and powder diffraction"},{"location":"5-solids/5-1-Bloch/","text":"The nearly free electron model \u00b6 Introduction \u00b6 Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Quantum: Perturbation theory, including degenerate perturbation theory Text reference The material covered here is discussed in section(s) \\S 15 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Let's summarize what we learned about electrons so far: Free electrons are described by plane waves with a quadratic dispersion and form a Fermi sea ( Electrons in metals II ) Electrons on isolated atoms live in discrete orbitals ( Chemistry ) When orbitals hybridise we get LCAO or tight-binding band structures ( The tight binding model ) In this section, we will analyse how electrons behave in solids using the nearly-free electron model . This model considers electrons as plane waves (as in the free electron model) that are weakly perturbed by the periodic potential associated with the atoms in a solid. This approach is opposite to that of the tight-binding model, where our starting point was that the electrons are strongly bound to the individual atoms and we included hopping to other atoms as a small effect. Perhaps surprisingly, we will find that the nearly-free electron model gives very similar results to the tight binding model: it also leads to the formation of energy bands, and these bands are separated by band gaps - regions in the band structure where there are no allowed energy states. Nearly free electron model \u00b6 In the free electron model, the dispersion is E = \\hbar^2 |\\mathbf{k}|^2/2m . The corresponding eigenfunctions |\\mathbf{k}\\rangle are plane waves with a real-space representation \\psi(\\mathbf{r}) \\propto e^{i\\mathbf{k}\\cdot \\mathbf{r}} . We note that in the free electron model, there is only one band the band structure is not periodic in k -space i.e., the Brillouin zone is infinite in k -space Within the nearly free electron model we start from the dispersion relation of free electrons and analyse the effect of introducing a weak lattice potential. The logic is very similar to getting optical and acoustic phonon branches by changing atom masses (and thereby reducing the size of the Brillouin zone). The lattice potential results in a band structure that is periodic in k -space, with a period given by the period of the reciprocal lattice: In this figure, the orange curves represent the nearly-free electron dispersion, which differs from the free-electron dispersion (blue curves) because of the interaction with the lattice. We see that band gaps open where two copies of the free-electron dispersion cross. A key goal of this lecture is to understand how the weak interaction with the lattice leads to this modified band structure. Analysing the avoided crossings \u00b6 To analyse what happens near the crossings, we first neglect the lattice potential and consider the free-electron dispersion near the crossing at k=\\pi/a in 1D. Near this crossing, we see that two copies of the dispersion come together (one copy centred at k=0 , the other at k=2\\pi/a ). We call the corresponding plane-wave eigenfunctions |k\\rangle and |k'\\rangle =|k-2\\pi/a\\rangle . We now express the wavefunction near this crossing as a linear superposition |\\psi\\rangle = \\alpha |k\\rangle + \\beta |k'\\rangle . Note that this wave function is very similar to that used in the LCAO model, except there we used linear combinations of the orbitals |1\\rangle and |2\\rangle instead of the plane waves |k\\rangle and |k'\\rangle . We express the Hamiltonian near the crossing as a matrix, using | k \\rangle and | k' \\rangle as the basis states. The matrix elements are given by \\langle k |H|k\\rangle = E_0 + v \\hbar \\delta k and \\langle k' |H|k'\\rangle = E_0 - v \\hbar \\delta k , where \\delta k = k-\\pi/a is the distance from the centre of the crossing and we approximated the dispersion near the crossing by a linear term. In matrix form, this yields H\\begin{pmatrix}\\alpha \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} E_0 + v \\hbar \\delta k & 0 \\\\ 0 & E_0 - v \\hbar \\delta k\\end{pmatrix} \\begin{pmatrix}\\alpha \\\\ \\beta \\end{pmatrix}. Note that this Hamiltonian is diagonal, so the eigenenergies are on the diagonal and the eigenfunctions are simply the |k\\rangle and |k'\\rangle basis states. calculate E_0 and the velocity v The edge of the Brilloin zone has k = \\pi/a . Substituting this in the free electron dispersion E = \\hbar^2 k^2/2m we get E_0 = \\hbar^2 \\pi^2/2 m a^2 , and v=\\hbar k/m=\\hbar \\pi/ma . As we will see below, the lattice potential V(x) can couple the states |k\\rangle and |k'\\rangle . The coupling between these states is given by the matrix element W=\\langle k | V | k'\\rangle . Including this coupling into the Hamiltonian: H\\begin{pmatrix}\\alpha \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} E_0 + v \\hbar \\delta k & W \\\\ W^* & E_0 - v \\hbar \\delta k\\end{pmatrix} \\begin{pmatrix}\\alpha \\\\ \\beta \\end{pmatrix}, (This is where we applied perturbation theory, and this is very similar to the LCAO Hamiltonian where the coupling was given by -t=\\langle 1 | H | 2 \\rangle ) . Dispersion near the avoided level crossing \u00b6 To find the dispersion near k=\\pi/a , we need to diagonalize this 2x2 matrix Hamiltonian. The solutions for the eigenvalues are E(\\delta k) = E_0 \\pm \\sqrt{v^2\\hbar^2\\delta k^2 + |W|^2} (Check out section 15.1.1 of the book for the details of this calculation). This equation describes the avoided crossing. We observe that the gap that has opened at \\delta k=0 is equal to 2W . Calculating the magnitude of the gaps \u00b6 We will now show that W=\\langle k | V |k' \\rangle represents a Fourier component of the lattice potential. To see this, we express the lattice potential (which is periodic with V(x)=V(x+a) ) as a Fourier series V(x) = \\sum_{n=-\\infty}^{\\infty} V_n e^{2\\pi i n x/a} and recall that such a series has Fourier components V_n given by V_n = \\frac{1}{a}\\int_0^a e^{- i n 2\\pi x /a} V(x) dx Calculating W , we find W = \\langle k | V | k' \\rangle = \\frac{1}{a}\\int_0^{a} e^{-i k x} V(x) e^{i k'x} dx = \\frac{1}{a}\\int_0^a e^{-i 2\\pi x /a} V(x) dx = V_1 where we have used that k-k' =2\\pi/a because we are analysing the first crossing. We see that the first component of the Fourier-series representation of V(x) determines the gap near the first crossing. Crossings between the higher bands \u00b6 Everything we did can also be applied to the higher-energy crossings seen in the figure above. We note that all crossings occur between parabola's that are shifted by integer multiples of reciprocal lattice vectors n 2\\pi/a . The first crossing corresponds to n=1 , and we found that the magnitude of the gap is given by V_1 . Similarly, V_2 determines the gap between the second and third bands, V_3 for the crossing between third and fourth, etc. The key conclusion is that the lattice potential couples plane-wave states that differ by integer multiples of reciprocal lattice vectors. This coupling alters the band structure most strongly where the free-electron eigenenergies cross, opening up gaps of which the magnitudes are determined by the Fourier components of the lattice potential. Suppose the lattice potential is V(x)=A\\cos(2\\pi/ax) . At what locations in the dispersion does V(x) lead to the formation of gaps? Hint: The Fourier series of V(x) is V(x)=A(e^{i2\\pi/ax}+e^{-i2\\pi/ax})/2 , so the only non-zero Fourier components are V_1=V_{-1} = A/2 . General description of a band structure in a crystal - Bloch theorem \u00b6 The different models considered thus far can be organized as a function of the strength of the lattice potential V(x) : We have seen that in the nearly-free electron model, the electrons behave as plane waves that are only slightly perturbed by the lattice potential. How is it possible that an electron that can scatter off all the atoms in a solid can even remotely look like a plane wave? The answer lies in that the periodic potential of the atoms can only scatter an electron between momentum states |\\mathbf{k}\\rangle and |\\mathbf{k'}\\rangle if these momenta differ by a reciprocal lattice vector . This condition is very similar to the Laue condition of X-ray scattering. In this lecture we have explicitly analysed it in the context of the nearly-free electron model. The condition is known as the conservation of crystal momentum and is central to Bloch's theorem, which provides a general framework for computing band structures in crystals. Bloch theorem: All Hamiltonian eigenstates in a crystal have the form \\psi^\\alpha(\\mathbf{r}) = u^\\alpha(\\mathbf{r})e^{i\\mathbf{k}\\cdot \\mathbf{r}} with u^\\alpha(\\mathbf{r}) having the same periodicity as the lattice potential V(\\mathbf{r}) , and index \\alpha labeling electron bands with energies E^\\alpha(\\mathbf{k}) . In other words: any electron wave function in a crystal is a product of a periodic part that describes electron motion within a unit cell and a plane wave. In both the tight-binding and the nearly-free electron models, the wave functions we considered are consistent with Bloch's theorem. Does our nearly-free electron wavefunction |\\psi\\rangle = \\alpha|k\\rangle + \\beta|k'\\rangle satisfy the Bloch theorem? What is u(x) in this case? The wave function has a form \\psi(x) = \\alpha \\exp[ikx] + \\beta \\exp[i(k - 2\\pi/a)x] (here k = \\pi/a + \\delta k ). Choosing u(x) = \\alpha + \\beta \\exp(2\\pi i x/a) we see that \\psi(x) = u(x) \\exp(ikx) . Extra remarks \u00b6 The wave function u^\\alpha(\\mathbf{r})e^{i\\mathbf{k} \\cdot \\mathbf{r}} is called a Bloch wave . The u^\\alpha(\\mathbf{r}) part is some unknown function. To calculate it we need to solve the Schr\u00f6dinger equation. It is hard in general, but there are two limits when U is \"weak\" and U is \"large\" that provide us with most intuition. If we change \\mathbf{k} by a reciprocal lattice vector \\mathbf{k} \\rightarrow \\mathbf{k} + h\\mathbf{b}_1 + k\\mathbf{b}_2 + l\\mathbf{b}_3 , and we change u^\\alpha(\\mathbf{r}) \\rightarrow u^\\alpha(\\mathbf{r})\\exp\\left[i(-h\\mathbf{b}_1 - k\\mathbf{b}_2 - l\\mathbf{b}_3)\\cdot \\mathbf{r}\\right] (also periodic!), we obtain the same wave function. Therefore energies of all bands E^\\alpha(\\mathbf{k}) are periodic in reciprocal space with the periodicity of the reciprocal lattice. An alternative way for expressing the Bloch wave is obtained by formulating u^\\alpha(r) as a Fourier series: u^\\alpha(r) = \\sum_\\mathbf{G} u^{\\alpha}_\\mathbf{G}e^{i\\mathbf{G}\\cdot\\mathbf{r}} where u^{\\alpha}_\\mathbf{G} are the Fourier coefficients. Substituting this into our expression for the Bloch wave, we get \\psi^\\alpha(r) = \\sum_\\mathbf{G} u^{\\alpha}_\\mathbf{G} e^{i(\\mathbf{k}+\\mathbf{G})\\cdot\\mathbf{r}} which shows that each Bloch wave can be written as a sum over plane waves that differ by a reciprocal lattice vector. Does the tight-binding wavefunction |\\psi\\rangle = \\sum_n e^{ikna}(\\phi_0|n,1\\rangle+\\psi_0|n,2\\rangle) satisfy the Bloch theorem? What part of |\\psi\\rangle describes u(x) in this case? Try to describe in words how this Bloch wave is built up. Repeated vs reduced vs extended Brillouin zone \u00b6 There are several common ways to plot the same dispersion relation (no difference in physical information). Repeated BZ (all possible Bloch bands): Contains redundant information May be easier to count/follow the bands Reduced BZ (all bands within 1st BZ): No redundant information Hard to relate to original dispersion Extended BZ (n-th band within n-th BZ): No redundant information Easy to relate to free electron model Contains discontinuities Exercises \u00b6 Exercise 1: Bloch's theorem \u00b6 Suppose we have a crystal with lattice vectors \\mathbf{a}_ 1 , \\mathbf{a}_ 2 , and \\mathbf{a}_ 3 . What can be said about the symmetry of the Hamiltonian \\hat{H} of this crystal? Now define the translation operator \\hat{T}_{\\alpha,\\beta,\\gamma} so that \\hat{T}_{\\alpha,\\beta,\\gamma} \\psi(\\mathbf{r}) = \\psi(\\mathbf{r} - \\alpha \\mathbf{a}_1 - \\beta \\mathbf{a}_2 - \\gamma \\mathbf{a}_3), where \\alpha , \\beta , \\gamma are integers. Show that \\hat{T}_{\\alpha,\\beta,\\gamma} and \\hat{H} commute. Show that the Bloch wavefunctions defined in the lecture notes are eigenfunctions of \\hat{T}_{\\alpha,\\beta,\\gamma} . What are the corresponding eigenvalues? What does this say about the eigenfunctions of \\hat{H} . By applying \\hat{H} to the Bloch wavefunction, show that the Schr\u00f6dinger equation can be rewritten as \\left[ \\frac{\\mathbf{\\hat{p}}^2}{2m} + \\frac{\\hbar}{m} \\mathbf{k} \\cdot \\mathbf{\\hat{p}} + \\frac{\\hbar^2 \\mathbf{k}^2}{2m} + V(\\mathbf{r}) \\right] u_{n,\\mathbf{k}}(\\mathbf{r}) = E_{n,\\mathbf{k}} u_{n,\\mathbf{k}}(\\mathbf{r}), where \\mathbf{\\hat{p}} =-i\\hbar\\nabla . What is u_{n,\\mathbf{k}}(\\mathbf{r}) in case of free electrons? Is your answer consistent with the equation above? Exercise 2: the central equation in 1D \u00b6 Let's consider a 1D crystal with a period a . Let k_0 be any wave number of an electron in the first Brillouin zone. What k_n are equivalent to k_0 in this crystal? First, we assume that the electrons with these k_n are free. In that case, what are the wavefunctions \\phi_n(x) and energies E_n of these electrons? Make a sketch of the dispersion relation using a repeated Brillouin zone representation. Indicate some k_n and E_n as well as the first Brillouin zone in your sketch. We will now introduce a weak periodic potential V(x) = V(x+na) in our system. This causes coupling between eigenstates \\left| \\phi_n\\right> in the free electron case. In order to find the right eigenstates of the system with that potential, we need an 'LCAO-like' trial eigenstate given by \\left|\\psi\\right> = \\sum_{n=-\\infty}^{\\infty}C_n \\left|\\phi_n\\right> Using the trial eigenstate above and the Schr\u00f6dinger equation, show that E C_m = \\varepsilon_m C_m+\\sum_{n=-\\infty}^{\\infty} V_{n}C_{m-n}, where V_n are the Fourier components of the potential defined above . Find an expression for \\varepsilon_m . NB: This equation is also known as the central equation (in 1D). Hint Apply \\left<\\phi_m\\right| to the Schr\u00f6dinger equation. To evaluate \\left<\\phi_m\\right| \\hat{H} \\left| \\phi_n\\right> , it may be helpful to separate the kinetic energy and potential energy of the Hamiltonian. Why is the dispersion relation only affected near k=0 and at the edge of the Brillouin zone (see also figures above )? Hint To answer this question, only consider consider two free electron wavefunctions in the Hamiltonian and ignore all the others. Between what two of free electron wavefunctions does the coupling give significant contribution to the energy levels of the free electron wavefunctions? Exercise 3: the tight binding model vs. the nearly free electron model \u00b6 Consider a 1D crystal with a periodic potential given by delta peaks: V(x) = -\\lambda \\sum_{n=-\\infty}^{\\infty} \\delta(x+na), where \\lambda>0 . In this exercise, we will find the band structure of this crystal in two ways: By means of the nearly free electron model explained in this lecture. By means of the tight binding model explained in lecture 7 . We first find the band structure using the nearly free electron model. To this end, we consider the effect of the potential on the free electron wavefunctions given by \\psi_1(x) \\propto e^{ikx} and \\psi_2(x) \\propto e^{i[k-2\\pi/a]x} on the interval k=[0,\\pi/a] . Derive a dispersion relation of the lower band using the Sch\u00f6dinger equation and the trial eigenstate \\Psi(x) = \\alpha \\psi_1(x) + \\beta \\psi_2(x). Hint Using the Schr\u00f6dinger equation and the trial eigenstate, first derive a 2\u00d72 eigenvalue problem given by E \\begin{pmatrix}\\alpha \\\\ \\beta\\end{pmatrix} = \\begin{pmatrix}\\varepsilon_0(k)+V_0 & V_1^* \\\\ V_1 & \\varepsilon_0(k - 2\\pi/a) + V_0\\end{pmatrix} \\begin{pmatrix}\\alpha \\\\ \\beta\\end{pmatrix}. What are \\varepsilon_0(k) , V_0 and V_1 ? Make a sketch of the lower band. We now use the tight binding model, where we know that the dispersion relation can be described by E = \\varepsilon_0 - 2 t \\cos (ka). Find an expression for \\varepsilon_0=\\left<n\\right| \\hat{H} \\left|n\\right> and -t=\\left<n-1\\right| \\hat{H} \\left| n \\right> , where |n\\rangle is the wavefunction of a single \\delta -peak well at site n . You may make use of the results obtained in exercise 2 of lecture 5 or look up the wavefunction . Compare the bands obtained in exercise 1 and 2: what are the minima and bandwidths (difference between maximum and minimum) of those bands? For what a and \\lambda is the nearly free electron model more accurate? And for what a and \\lambda is the tight binding model more accurate?","title":"The nearly free electron model"},{"location":"5-solids/5-1-Bloch/#the-nearly-free-electron-model","text":"","title":"The nearly free electron model"},{"location":"5-solids/5-1-Bloch/#introduction","text":"Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Quantum: Perturbation theory, including degenerate perturbation theory Text reference The material covered here is discussed in section(s) \\S 15 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Let's summarize what we learned about electrons so far: Free electrons are described by plane waves with a quadratic dispersion and form a Fermi sea ( Electrons in metals II ) Electrons on isolated atoms live in discrete orbitals ( Chemistry ) When orbitals hybridise we get LCAO or tight-binding band structures ( The tight binding model ) In this section, we will analyse how electrons behave in solids using the nearly-free electron model . This model considers electrons as plane waves (as in the free electron model) that are weakly perturbed by the periodic potential associated with the atoms in a solid. This approach is opposite to that of the tight-binding model, where our starting point was that the electrons are strongly bound to the individual atoms and we included hopping to other atoms as a small effect. Perhaps surprisingly, we will find that the nearly-free electron model gives very similar results to the tight binding model: it also leads to the formation of energy bands, and these bands are separated by band gaps - regions in the band structure where there are no allowed energy states.","title":"Introduction"},{"location":"5-solids/5-1-Bloch/#nearly-free-electron-model","text":"In the free electron model, the dispersion is E = \\hbar^2 |\\mathbf{k}|^2/2m . The corresponding eigenfunctions |\\mathbf{k}\\rangle are plane waves with a real-space representation \\psi(\\mathbf{r}) \\propto e^{i\\mathbf{k}\\cdot \\mathbf{r}} . We note that in the free electron model, there is only one band the band structure is not periodic in k -space i.e., the Brillouin zone is infinite in k -space Within the nearly free electron model we start from the dispersion relation of free electrons and analyse the effect of introducing a weak lattice potential. The logic is very similar to getting optical and acoustic phonon branches by changing atom masses (and thereby reducing the size of the Brillouin zone). The lattice potential results in a band structure that is periodic in k -space, with a period given by the period of the reciprocal lattice: In this figure, the orange curves represent the nearly-free electron dispersion, which differs from the free-electron dispersion (blue curves) because of the interaction with the lattice. We see that band gaps open where two copies of the free-electron dispersion cross. A key goal of this lecture is to understand how the weak interaction with the lattice leads to this modified band structure.","title":"Nearly free electron model"},{"location":"5-solids/5-1-Bloch/#analysing-the-avoided-crossings","text":"To analyse what happens near the crossings, we first neglect the lattice potential and consider the free-electron dispersion near the crossing at k=\\pi/a in 1D. Near this crossing, we see that two copies of the dispersion come together (one copy centred at k=0 , the other at k=2\\pi/a ). We call the corresponding plane-wave eigenfunctions |k\\rangle and |k'\\rangle =|k-2\\pi/a\\rangle . We now express the wavefunction near this crossing as a linear superposition |\\psi\\rangle = \\alpha |k\\rangle + \\beta |k'\\rangle . Note that this wave function is very similar to that used in the LCAO model, except there we used linear combinations of the orbitals |1\\rangle and |2\\rangle instead of the plane waves |k\\rangle and |k'\\rangle . We express the Hamiltonian near the crossing as a matrix, using | k \\rangle and | k' \\rangle as the basis states. The matrix elements are given by \\langle k |H|k\\rangle = E_0 + v \\hbar \\delta k and \\langle k' |H|k'\\rangle = E_0 - v \\hbar \\delta k , where \\delta k = k-\\pi/a is the distance from the centre of the crossing and we approximated the dispersion near the crossing by a linear term. In matrix form, this yields H\\begin{pmatrix}\\alpha \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} E_0 + v \\hbar \\delta k & 0 \\\\ 0 & E_0 - v \\hbar \\delta k\\end{pmatrix} \\begin{pmatrix}\\alpha \\\\ \\beta \\end{pmatrix}. Note that this Hamiltonian is diagonal, so the eigenenergies are on the diagonal and the eigenfunctions are simply the |k\\rangle and |k'\\rangle basis states. calculate E_0 and the velocity v The edge of the Brilloin zone has k = \\pi/a . Substituting this in the free electron dispersion E = \\hbar^2 k^2/2m we get E_0 = \\hbar^2 \\pi^2/2 m a^2 , and v=\\hbar k/m=\\hbar \\pi/ma . As we will see below, the lattice potential V(x) can couple the states |k\\rangle and |k'\\rangle . The coupling between these states is given by the matrix element W=\\langle k | V | k'\\rangle . Including this coupling into the Hamiltonian: H\\begin{pmatrix}\\alpha \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} E_0 + v \\hbar \\delta k & W \\\\ W^* & E_0 - v \\hbar \\delta k\\end{pmatrix} \\begin{pmatrix}\\alpha \\\\ \\beta \\end{pmatrix}, (This is where we applied perturbation theory, and this is very similar to the LCAO Hamiltonian where the coupling was given by -t=\\langle 1 | H | 2 \\rangle ) .","title":"Analysing the avoided crossings"},{"location":"5-solids/5-1-Bloch/#dispersion-near-the-avoided-level-crossing","text":"To find the dispersion near k=\\pi/a , we need to diagonalize this 2x2 matrix Hamiltonian. The solutions for the eigenvalues are E(\\delta k) = E_0 \\pm \\sqrt{v^2\\hbar^2\\delta k^2 + |W|^2} (Check out section 15.1.1 of the book for the details of this calculation). This equation describes the avoided crossing. We observe that the gap that has opened at \\delta k=0 is equal to 2W .","title":"Dispersion near the avoided level crossing"},{"location":"5-solids/5-1-Bloch/#calculating-the-magnitude-of-the-gaps","text":"We will now show that W=\\langle k | V |k' \\rangle represents a Fourier component of the lattice potential. To see this, we express the lattice potential (which is periodic with V(x)=V(x+a) ) as a Fourier series V(x) = \\sum_{n=-\\infty}^{\\infty} V_n e^{2\\pi i n x/a} and recall that such a series has Fourier components V_n given by V_n = \\frac{1}{a}\\int_0^a e^{- i n 2\\pi x /a} V(x) dx Calculating W , we find W = \\langle k | V | k' \\rangle = \\frac{1}{a}\\int_0^{a} e^{-i k x} V(x) e^{i k'x} dx = \\frac{1}{a}\\int_0^a e^{-i 2\\pi x /a} V(x) dx = V_1 where we have used that k-k' =2\\pi/a because we are analysing the first crossing. We see that the first component of the Fourier-series representation of V(x) determines the gap near the first crossing.","title":"Calculating the magnitude of the gaps"},{"location":"5-solids/5-1-Bloch/#crossings-between-the-higher-bands","text":"Everything we did can also be applied to the higher-energy crossings seen in the figure above. We note that all crossings occur between parabola's that are shifted by integer multiples of reciprocal lattice vectors n 2\\pi/a . The first crossing corresponds to n=1 , and we found that the magnitude of the gap is given by V_1 . Similarly, V_2 determines the gap between the second and third bands, V_3 for the crossing between third and fourth, etc. The key conclusion is that the lattice potential couples plane-wave states that differ by integer multiples of reciprocal lattice vectors. This coupling alters the band structure most strongly where the free-electron eigenenergies cross, opening up gaps of which the magnitudes are determined by the Fourier components of the lattice potential. Suppose the lattice potential is V(x)=A\\cos(2\\pi/ax) . At what locations in the dispersion does V(x) lead to the formation of gaps? Hint: The Fourier series of V(x) is V(x)=A(e^{i2\\pi/ax}+e^{-i2\\pi/ax})/2 , so the only non-zero Fourier components are V_1=V_{-1} = A/2 .","title":"Crossings between the higher bands"},{"location":"5-solids/5-1-Bloch/#general-description-of-a-band-structure-in-a-crystal-bloch-theorem","text":"The different models considered thus far can be organized as a function of the strength of the lattice potential V(x) : We have seen that in the nearly-free electron model, the electrons behave as plane waves that are only slightly perturbed by the lattice potential. How is it possible that an electron that can scatter off all the atoms in a solid can even remotely look like a plane wave? The answer lies in that the periodic potential of the atoms can only scatter an electron between momentum states |\\mathbf{k}\\rangle and |\\mathbf{k'}\\rangle if these momenta differ by a reciprocal lattice vector . This condition is very similar to the Laue condition of X-ray scattering. In this lecture we have explicitly analysed it in the context of the nearly-free electron model. The condition is known as the conservation of crystal momentum and is central to Bloch's theorem, which provides a general framework for computing band structures in crystals. Bloch theorem: All Hamiltonian eigenstates in a crystal have the form \\psi^\\alpha(\\mathbf{r}) = u^\\alpha(\\mathbf{r})e^{i\\mathbf{k}\\cdot \\mathbf{r}} with u^\\alpha(\\mathbf{r}) having the same periodicity as the lattice potential V(\\mathbf{r}) , and index \\alpha labeling electron bands with energies E^\\alpha(\\mathbf{k}) . In other words: any electron wave function in a crystal is a product of a periodic part that describes electron motion within a unit cell and a plane wave. In both the tight-binding and the nearly-free electron models, the wave functions we considered are consistent with Bloch's theorem. Does our nearly-free electron wavefunction |\\psi\\rangle = \\alpha|k\\rangle + \\beta|k'\\rangle satisfy the Bloch theorem? What is u(x) in this case? The wave function has a form \\psi(x) = \\alpha \\exp[ikx] + \\beta \\exp[i(k - 2\\pi/a)x] (here k = \\pi/a + \\delta k ). Choosing u(x) = \\alpha + \\beta \\exp(2\\pi i x/a) we see that \\psi(x) = u(x) \\exp(ikx) .","title":"General description of a band structure in a crystal - Bloch theorem"},{"location":"5-solids/5-1-Bloch/#extra-remarks","text":"The wave function u^\\alpha(\\mathbf{r})e^{i\\mathbf{k} \\cdot \\mathbf{r}} is called a Bloch wave . The u^\\alpha(\\mathbf{r}) part is some unknown function. To calculate it we need to solve the Schr\u00f6dinger equation. It is hard in general, but there are two limits when U is \"weak\" and U is \"large\" that provide us with most intuition. If we change \\mathbf{k} by a reciprocal lattice vector \\mathbf{k} \\rightarrow \\mathbf{k} + h\\mathbf{b}_1 + k\\mathbf{b}_2 + l\\mathbf{b}_3 , and we change u^\\alpha(\\mathbf{r}) \\rightarrow u^\\alpha(\\mathbf{r})\\exp\\left[i(-h\\mathbf{b}_1 - k\\mathbf{b}_2 - l\\mathbf{b}_3)\\cdot \\mathbf{r}\\right] (also periodic!), we obtain the same wave function. Therefore energies of all bands E^\\alpha(\\mathbf{k}) are periodic in reciprocal space with the periodicity of the reciprocal lattice. An alternative way for expressing the Bloch wave is obtained by formulating u^\\alpha(r) as a Fourier series: u^\\alpha(r) = \\sum_\\mathbf{G} u^{\\alpha}_\\mathbf{G}e^{i\\mathbf{G}\\cdot\\mathbf{r}} where u^{\\alpha}_\\mathbf{G} are the Fourier coefficients. Substituting this into our expression for the Bloch wave, we get \\psi^\\alpha(r) = \\sum_\\mathbf{G} u^{\\alpha}_\\mathbf{G} e^{i(\\mathbf{k}+\\mathbf{G})\\cdot\\mathbf{r}} which shows that each Bloch wave can be written as a sum over plane waves that differ by a reciprocal lattice vector. Does the tight-binding wavefunction |\\psi\\rangle = \\sum_n e^{ikna}(\\phi_0|n,1\\rangle+\\psi_0|n,2\\rangle) satisfy the Bloch theorem? What part of |\\psi\\rangle describes u(x) in this case? Try to describe in words how this Bloch wave is built up.","title":"Extra remarks"},{"location":"5-solids/5-1-Bloch/#repeated-vs-reduced-vs-extended-brillouin-zone","text":"There are several common ways to plot the same dispersion relation (no difference in physical information). Repeated BZ (all possible Bloch bands): Contains redundant information May be easier to count/follow the bands Reduced BZ (all bands within 1st BZ): No redundant information Hard to relate to original dispersion Extended BZ (n-th band within n-th BZ): No redundant information Easy to relate to free electron model Contains discontinuities","title":"Repeated vs reduced vs extended Brillouin zone"},{"location":"5-solids/5-1-Bloch/#exercises","text":"","title":"Exercises"},{"location":"5-solids/5-1-Bloch/#exercise-1-blochs-theorem","text":"Suppose we have a crystal with lattice vectors \\mathbf{a}_ 1 , \\mathbf{a}_ 2 , and \\mathbf{a}_ 3 . What can be said about the symmetry of the Hamiltonian \\hat{H} of this crystal? Now define the translation operator \\hat{T}_{\\alpha,\\beta,\\gamma} so that \\hat{T}_{\\alpha,\\beta,\\gamma} \\psi(\\mathbf{r}) = \\psi(\\mathbf{r} - \\alpha \\mathbf{a}_1 - \\beta \\mathbf{a}_2 - \\gamma \\mathbf{a}_3), where \\alpha , \\beta , \\gamma are integers. Show that \\hat{T}_{\\alpha,\\beta,\\gamma} and \\hat{H} commute. Show that the Bloch wavefunctions defined in the lecture notes are eigenfunctions of \\hat{T}_{\\alpha,\\beta,\\gamma} . What are the corresponding eigenvalues? What does this say about the eigenfunctions of \\hat{H} . By applying \\hat{H} to the Bloch wavefunction, show that the Schr\u00f6dinger equation can be rewritten as \\left[ \\frac{\\mathbf{\\hat{p}}^2}{2m} + \\frac{\\hbar}{m} \\mathbf{k} \\cdot \\mathbf{\\hat{p}} + \\frac{\\hbar^2 \\mathbf{k}^2}{2m} + V(\\mathbf{r}) \\right] u_{n,\\mathbf{k}}(\\mathbf{r}) = E_{n,\\mathbf{k}} u_{n,\\mathbf{k}}(\\mathbf{r}), where \\mathbf{\\hat{p}} =-i\\hbar\\nabla . What is u_{n,\\mathbf{k}}(\\mathbf{r}) in case of free electrons? Is your answer consistent with the equation above?","title":"Exercise 1: Bloch's theorem"},{"location":"5-solids/5-1-Bloch/#exercise-2-the-central-equation-in-1d","text":"Let's consider a 1D crystal with a period a . Let k_0 be any wave number of an electron in the first Brillouin zone. What k_n are equivalent to k_0 in this crystal? First, we assume that the electrons with these k_n are free. In that case, what are the wavefunctions \\phi_n(x) and energies E_n of these electrons? Make a sketch of the dispersion relation using a repeated Brillouin zone representation. Indicate some k_n and E_n as well as the first Brillouin zone in your sketch. We will now introduce a weak periodic potential V(x) = V(x+na) in our system. This causes coupling between eigenstates \\left| \\phi_n\\right> in the free electron case. In order to find the right eigenstates of the system with that potential, we need an 'LCAO-like' trial eigenstate given by \\left|\\psi\\right> = \\sum_{n=-\\infty}^{\\infty}C_n \\left|\\phi_n\\right> Using the trial eigenstate above and the Schr\u00f6dinger equation, show that E C_m = \\varepsilon_m C_m+\\sum_{n=-\\infty}^{\\infty} V_{n}C_{m-n}, where V_n are the Fourier components of the potential defined above . Find an expression for \\varepsilon_m . NB: This equation is also known as the central equation (in 1D). Hint Apply \\left<\\phi_m\\right| to the Schr\u00f6dinger equation. To evaluate \\left<\\phi_m\\right| \\hat{H} \\left| \\phi_n\\right> , it may be helpful to separate the kinetic energy and potential energy of the Hamiltonian. Why is the dispersion relation only affected near k=0 and at the edge of the Brillouin zone (see also figures above )? Hint To answer this question, only consider consider two free electron wavefunctions in the Hamiltonian and ignore all the others. Between what two of free electron wavefunctions does the coupling give significant contribution to the energy levels of the free electron wavefunctions?","title":"Exercise 2: the central equation in 1D"},{"location":"5-solids/5-1-Bloch/#exercise-3-the-tight-binding-model-vs-the-nearly-free-electron-model","text":"Consider a 1D crystal with a periodic potential given by delta peaks: V(x) = -\\lambda \\sum_{n=-\\infty}^{\\infty} \\delta(x+na), where \\lambda>0 . In this exercise, we will find the band structure of this crystal in two ways: By means of the nearly free electron model explained in this lecture. By means of the tight binding model explained in lecture 7 . We first find the band structure using the nearly free electron model. To this end, we consider the effect of the potential on the free electron wavefunctions given by \\psi_1(x) \\propto e^{ikx} and \\psi_2(x) \\propto e^{i[k-2\\pi/a]x} on the interval k=[0,\\pi/a] . Derive a dispersion relation of the lower band using the Sch\u00f6dinger equation and the trial eigenstate \\Psi(x) = \\alpha \\psi_1(x) + \\beta \\psi_2(x). Hint Using the Schr\u00f6dinger equation and the trial eigenstate, first derive a 2\u00d72 eigenvalue problem given by E \\begin{pmatrix}\\alpha \\\\ \\beta\\end{pmatrix} = \\begin{pmatrix}\\varepsilon_0(k)+V_0 & V_1^* \\\\ V_1 & \\varepsilon_0(k - 2\\pi/a) + V_0\\end{pmatrix} \\begin{pmatrix}\\alpha \\\\ \\beta\\end{pmatrix}. What are \\varepsilon_0(k) , V_0 and V_1 ? Make a sketch of the lower band. We now use the tight binding model, where we know that the dispersion relation can be described by E = \\varepsilon_0 - 2 t \\cos (ka). Find an expression for \\varepsilon_0=\\left<n\\right| \\hat{H} \\left|n\\right> and -t=\\left<n-1\\right| \\hat{H} \\left| n \\right> , where |n\\rangle is the wavefunction of a single \\delta -peak well at site n . You may make use of the results obtained in exercise 2 of lecture 5 or look up the wavefunction . Compare the bands obtained in exercise 1 and 2: what are the minima and bandwidths (difference between maximum and minimum) of those bands? For what a and \\lambda is the nearly free electron model more accurate? And for what a and \\lambda is the tight binding model more accurate?","title":"Exercise 3: the tight binding model vs. the nearly free electron model"},{"location":"5-solids/5-2-Bands/","text":"Band structure and material properties \u00b6 Introduction \u00b6 Expected competencies It is assumed that you have familiarity with the following concepts/techniques: * Text reference The material covered here is discussed in section(s) \\S 16 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Band structure \u00b6 How are material properties related to the band structure? For a material to be a conductor, there should be available electron states at the Fermi level. Otherwise all the states are occupied, and all the currents cancel out. A band structure of a 1D material may look similar to this: We see several energy bands that may be separated by a band gap or overlapping. When the Fermi level lies in the band gap, the material is called a semiconductor (or dielectric or insulator). When the Fermi level lies within a band, it is a conductor (metal). A simple requirement for insulators \u00b6 In an insulator every single band is either completely filled or completely empty. What determines if an energy band if fully occupied or only partly? To answer this we need to know the number of available states within an energy band, and the number of electrons in the system. We can find the number of states in a band by integrating the density of states g(E) , but this is hard. Fortunately, we can easily see how many states there are in an energy band by counting the number of k -states in the first Brillouin zone. For a single band: N_{states} = 2 \\frac{L^3}{(2\\pi)^3} \\int_{BZ} dk_x dk_y dk_z = 2 L^3 / a^3 Here, L^3/a^3 is the number of unit cells in the system, so we see that a single band has room for 2 electrons per unit cell (the factor 2 comes from the spin). We come to the important rule: Any material with an odd number of electrons per unit cell is a metal. If the material has an even number of electrons per unit cell it may be a semiconductor, but only if the bands are not overlapping (see the figure above). For example: Si, Ge, Sn all have 4 valence electrons. Si (silicon, band gap 1.14 eV) and Ge (germanium, band gap 0.67 eV) are semiconductors, Sn (tin) is a metal. Interesting feature: the heaviest material is a metal, why? Fermi surface using a nearly free electron model \u00b6 Sequence of steps (same procedure as in 1D, but harder because of the need to imagine a 2D dispersion relation): Compute k_f using the free electron model (remember this is our starting point). Plot the free electron model Fermi surface and the Brillouin zones. Apply the perturbation where the Fermi surface crosses the Brillouin zone (due to avoided level crossings). The resulting band structure looks like this (in the extended Brillouin zone scheme): Observe that the top of the first band is above the bottom of the lowest band. Therefore if V is sufficiently weak, the material can be conducting even with 2 electrons per unit cell! A larger V makes the Fermi surface more distorted and eventually makes the material insulating. Let's compare the almost parabolic dispersion of the nearly free electron model with a tight-binding model in 2D. We now have a dispersion relation E = E_0 - 2t(\\cos k_x a + \\cos k_y a) , which looks like this: Light absorption \u00b6 Photons of external light can be reflected, transmitted, or absorbed by the material. Absorption, in turn requires energy transfer from the photon to electrons. In a filled band there are no available states where energy could be transferred (that's why insulators may be transparent). When transition between two bands becomes possible due to photons having high energy, the absorption increases in a step-like fashion, see the sketch below for germanium. Here E'_G\\approx 0.9eV and E_G\\approx 0.8 eV . The two visible steps are due to the special band structure of Ge: The band structure has two band gaps: direct , the band gap at k=0 , E'_G and indirect gap E_G at any k . In Ge E_G < E'_G , and therefore it is an indirect band gap semiconductor . Silicon also has an indirect band gap. Direct band gap materials are for example GaAs and InAs. Photons carry very little momentum and a very high energy since E = c \\hbar k and c is large. Therefore to excite electrons at E_G , despite a lower photon energy is sufficient, there is not enough momentum. Then an extra phonon is required. Phonons may have a very large momentum at room temperature, and a very low energy since atomic mass is much higher than electron mass. A joint absorbtion of a photon and a phonon collision may excite an electron across an indirect band gap, however this process is much less efficient, and therefore materials with an indirect bandgap are much worse for optics applications (light emitting diodes, light sensors, etc). Summary \u00b6 Each band can host two electrons per unit cell, therefore a material with an odd number of electrons is a metal; that with an even number of electrons may be an insulator. Light absorption is a tool to measure the band gap, and it distinguishes direct from indirect band gaps. Exercises \u00b6 Exercise 1: 3D Fermi surfaces \u00b6 Using the periodic table of the Fermi surfaces (or the static images at https://www.phys.ufl.edu/fermisurface/ if 3D does not work for you), answer the following questions: Find 4 elements that are well described by the nearly-free electron model and 4 that are poorly described by it. Is the Fermi surface of lithium or potassium better described by the free electron model? What about nearly-free electron model? Why? Do you expect a crystal with a simple cubic lattice and monovalent atoms to be conducting? What Fermi surface shape would you expect the NaCl crystal to have? Explain your answer using both the atomic valences and the optical properties of this crystal. Exercise 2: Tight-binding in 2D \u00b6 Consider a rectangular lattice with lattice constants a_x and a_y . Suppose the hopping parameters in the two corresponding directions to be -t_1 and -t_2 . Consider a single orbital per atom and only nearest-neighbour interactions. Write down a 2D tight-binding Schr\u00f6dinger equation (expand to 2D the results of 1D). Formulate the Bloch ansatz for the wave function. Calculate the dispersion relation of this model. What Fermi surface shape would this model have if the atoms are monovalent? What Fermi surface shape would it have if the number of electrons per atom is much smaller than 1? Exercise 3: Nearly-free Electron model in 2D \u00b6 (based on exercise 15.4 of the book) Suppose we have a square lattice with lattice constant a , with a periodic potential given by V(x,y)=2V_{10}(\\cos(2\\pi x/a)+\\cos(2\\pi y/a))+4V_{11}\\cos(2 \\pi x/a)\\cos(2 \\pi y/a) . Use the Nearly-free electron model to find the energy of state \\mathbf{q}=(\\pi/a, 0) . Hint This is analogous to the 1D case: the states that interact have k -vectors (\\pi/a,0) and (-\\pi/a,0) ; ( \\psi_{+}\\sim e^{i\\pi x /a} ; \\psi_{-}\\sim e^{-i\\pi x /a} ). Let's now study the more complicated case of state \\mathbf{q}=(\\pi/a,\\pi/a) . How many k -points have the same energy? Which ones? Write down the nearly free electron model Hamiltonian near this point. Find its eigenvalues.","title":"Band structure"},{"location":"5-solids/5-2-Bands/#band-structure-and-material-properties","text":"","title":"Band structure and material properties"},{"location":"5-solids/5-2-Bands/#introduction","text":"Expected competencies It is assumed that you have familiarity with the following concepts/techniques: * Text reference The material covered here is discussed in section(s) \\S 16 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"5-solids/5-2-Bands/#band-structure","text":"How are material properties related to the band structure? For a material to be a conductor, there should be available electron states at the Fermi level. Otherwise all the states are occupied, and all the currents cancel out. A band structure of a 1D material may look similar to this: We see several energy bands that may be separated by a band gap or overlapping. When the Fermi level lies in the band gap, the material is called a semiconductor (or dielectric or insulator). When the Fermi level lies within a band, it is a conductor (metal).","title":"Band structure"},{"location":"5-solids/5-2-Bands/#a-simple-requirement-for-insulators","text":"In an insulator every single band is either completely filled or completely empty. What determines if an energy band if fully occupied or only partly? To answer this we need to know the number of available states within an energy band, and the number of electrons in the system. We can find the number of states in a band by integrating the density of states g(E) , but this is hard. Fortunately, we can easily see how many states there are in an energy band by counting the number of k -states in the first Brillouin zone. For a single band: N_{states} = 2 \\frac{L^3}{(2\\pi)^3} \\int_{BZ} dk_x dk_y dk_z = 2 L^3 / a^3 Here, L^3/a^3 is the number of unit cells in the system, so we see that a single band has room for 2 electrons per unit cell (the factor 2 comes from the spin). We come to the important rule: Any material with an odd number of electrons per unit cell is a metal. If the material has an even number of electrons per unit cell it may be a semiconductor, but only if the bands are not overlapping (see the figure above). For example: Si, Ge, Sn all have 4 valence electrons. Si (silicon, band gap 1.14 eV) and Ge (germanium, band gap 0.67 eV) are semiconductors, Sn (tin) is a metal. Interesting feature: the heaviest material is a metal, why?","title":"A simple requirement for insulators"},{"location":"5-solids/5-2-Bands/#fermi-surface-using-a-nearly-free-electron-model","text":"Sequence of steps (same procedure as in 1D, but harder because of the need to imagine a 2D dispersion relation): Compute k_f using the free electron model (remember this is our starting point). Plot the free electron model Fermi surface and the Brillouin zones. Apply the perturbation where the Fermi surface crosses the Brillouin zone (due to avoided level crossings). The resulting band structure looks like this (in the extended Brillouin zone scheme): Observe that the top of the first band is above the bottom of the lowest band. Therefore if V is sufficiently weak, the material can be conducting even with 2 electrons per unit cell! A larger V makes the Fermi surface more distorted and eventually makes the material insulating. Let's compare the almost parabolic dispersion of the nearly free electron model with a tight-binding model in 2D. We now have a dispersion relation E = E_0 - 2t(\\cos k_x a + \\cos k_y a) , which looks like this:","title":"Fermi surface using a nearly free electron model"},{"location":"5-solids/5-2-Bands/#light-absorption","text":"Photons of external light can be reflected, transmitted, or absorbed by the material. Absorption, in turn requires energy transfer from the photon to electrons. In a filled band there are no available states where energy could be transferred (that's why insulators may be transparent). When transition between two bands becomes possible due to photons having high energy, the absorption increases in a step-like fashion, see the sketch below for germanium. Here E'_G\\approx 0.9eV and E_G\\approx 0.8 eV . The two visible steps are due to the special band structure of Ge: The band structure has two band gaps: direct , the band gap at k=0 , E'_G and indirect gap E_G at any k . In Ge E_G < E'_G , and therefore it is an indirect band gap semiconductor . Silicon also has an indirect band gap. Direct band gap materials are for example GaAs and InAs. Photons carry very little momentum and a very high energy since E = c \\hbar k and c is large. Therefore to excite electrons at E_G , despite a lower photon energy is sufficient, there is not enough momentum. Then an extra phonon is required. Phonons may have a very large momentum at room temperature, and a very low energy since atomic mass is much higher than electron mass. A joint absorbtion of a photon and a phonon collision may excite an electron across an indirect band gap, however this process is much less efficient, and therefore materials with an indirect bandgap are much worse for optics applications (light emitting diodes, light sensors, etc).","title":"Light absorption"},{"location":"5-solids/5-2-Bands/#summary","text":"Each band can host two electrons per unit cell, therefore a material with an odd number of electrons is a metal; that with an even number of electrons may be an insulator. Light absorption is a tool to measure the band gap, and it distinguishes direct from indirect band gaps.","title":"Summary"},{"location":"5-solids/5-2-Bands/#exercises","text":"","title":"Exercises"},{"location":"5-solids/5-2-Bands/#exercise-1-3d-fermi-surfaces","text":"Using the periodic table of the Fermi surfaces (or the static images at https://www.phys.ufl.edu/fermisurface/ if 3D does not work for you), answer the following questions: Find 4 elements that are well described by the nearly-free electron model and 4 that are poorly described by it. Is the Fermi surface of lithium or potassium better described by the free electron model? What about nearly-free electron model? Why? Do you expect a crystal with a simple cubic lattice and monovalent atoms to be conducting? What Fermi surface shape would you expect the NaCl crystal to have? Explain your answer using both the atomic valences and the optical properties of this crystal.","title":"Exercise 1: 3D Fermi surfaces"},{"location":"5-solids/5-2-Bands/#exercise-2-tight-binding-in-2d","text":"Consider a rectangular lattice with lattice constants a_x and a_y . Suppose the hopping parameters in the two corresponding directions to be -t_1 and -t_2 . Consider a single orbital per atom and only nearest-neighbour interactions. Write down a 2D tight-binding Schr\u00f6dinger equation (expand to 2D the results of 1D). Formulate the Bloch ansatz for the wave function. Calculate the dispersion relation of this model. What Fermi surface shape would this model have if the atoms are monovalent? What Fermi surface shape would it have if the number of electrons per atom is much smaller than 1?","title":"Exercise 2: Tight-binding in 2D"},{"location":"5-solids/5-2-Bands/#exercise-3-nearly-free-electron-model-in-2d","text":"(based on exercise 15.4 of the book) Suppose we have a square lattice with lattice constant a , with a periodic potential given by V(x,y)=2V_{10}(\\cos(2\\pi x/a)+\\cos(2\\pi y/a))+4V_{11}\\cos(2 \\pi x/a)\\cos(2 \\pi y/a) . Use the Nearly-free electron model to find the energy of state \\mathbf{q}=(\\pi/a, 0) . Hint This is analogous to the 1D case: the states that interact have k -vectors (\\pi/a,0) and (-\\pi/a,0) ; ( \\psi_{+}\\sim e^{i\\pi x /a} ; \\psi_{-}\\sim e^{-i\\pi x /a} ). Let's now study the more complicated case of state \\mathbf{q}=(\\pi/a,\\pi/a) . How many k -points have the same energy? Which ones? Write down the nearly free electron model Hamiltonian near this point. Find its eigenvalues.","title":"Exercise 3: Nearly-free Electron model in 2D"},{"location":"6-semiconductors/6-1-semiconductors/","text":"Semiconductor physics \u00b6 Introduction \u00b6 Expected competencies It is assumed that you have familiarity with the following concepts/techniques: * Text reference The material covered here is discussed in section(s) \\S 17 and 18 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Up until this point, we focused on calculating and understanding the band structures. However, the dispersion of a band is only part of the story. An empty band is not going to lead to any interesting physical properties no matter how sophisticated it is. Therefore, it is also important how bands are filled by the particles. By carefully controlling the distribution of particles in the bands, we are able to engineer material properties that we require. Without a doubt, the greatest example is semiconductors \u2014the bedrock of modern electronics. In this lecture, we shall grasp the basics of semiconductors by learning how to treat bands at different levels of filling. Review of band structure properties \u00b6 Before proceeding further, let us remind ourselves of important band structure properties. Group velocity v=\\hbar^{-1}\\partial E(k)/\\partial k . Descibes how quickly electrons move within the lattice. Effective mass m^* = \\hbar^2\\left(d^2 E(k)/dk^2\\right)^{-1} . Tells us how hard it is to accelerate the particles and is related to the curvature of the band. Density of states g(E) = \\sum_{\\textrm{FS}} (dn/dk) \\times (dk/dE) . The amount of states per infinitesimal interval of energy at given energy. The quantity is vital in order to calculate any bulk property of the material such as conductivity, heat capacity, etc. In order to check that everything makes sense, we apply the concepts to the free electron model: H = \\frac{\\hbar^2 k^2}{2m} The velocity is \\hbar^{-1}\\partial E(k)/\\partial k = \\hbar k / m \\equiv p/m . The effective mass is m^* = \\hbar^2\\left(d^2 E(k)/dk^2\\right)^{-1} = m . So in this simplest case the definitions match the usual expressions. Filled vs empty bands \u00b6 We distinguish three different band filling types: filled, empty and partially filled. Despite being two opposite extreme extreme cases, filled and empty bands are very similar. For example, both filled and empty bands carry no electric current: \\begin{align} j = 2e \\frac{1}{2\\pi} \\int_{-\\pi/a}^{\\pi/a} v(k) dk = 2e \\frac{1}{2\\pi \\hbar} \\int_{-\\pi/a}^{\\pi/a} \\frac{dE}{dk} \\times dk = \\\\ 2e \\frac{1}{2\\pi \\hbar} [E(\\pi/a) - E(-\\pi/a)] = 0. \\end{align} On the other hand, a filled band has an equal number of electrons going forwards and backwards which thus cancel and lead to zero current. Similar results apply to many other physical quantities such as heat capacity and magnetization. Therefore, filled and empty bands do not affect most physical properties and can be disregarded. As a result, rather than to consider hundreds of bands that a material contains, we neglect most of them and just focus on the handful of partially filled bands around Fermi level. From electrons to holes \u00b6 Because completely filled or completely empty bands have simple properties, we may search for a convenient way to describe a band that only has a few electrons missing or extra. While keeping track of a few electrons has no tricks, even a few electrons missing from a band seem to require considering all the other electrons in a band. A more efficient approach to describing a nearly filled band is motivated by the following analogy. Let us say we have 100 boxes: 99 are occupied and 1 is empty. To keep track which box is occupied/empty, we could write down the numbers of all 99 occupied boxes. If, on the other hand, we only keep track which single box is empty, we solve the problem with a lot less book-keeping. The same approach applies to band filling. Instead of describing a lot of electrons that are present in an almost filled band, we focus on those that are absent. The absence of an electron is called a hole : a state of a completely filled band with one particle missing. In this schematic we can either say that 8\u00d72 electron states are occupied (the system has 8\u00d72 electrons counting spin), or 10\u00d72 hole states are occupied. Electron and hole pictures correspond to two different, but equivalent ways of describing the occupation of a band. Naturally, dealing with electrons is more convenient whenever a band is almost empty and with holes when a band is almost full. Properties of holes \u00b6 Let us compare the properties of an electron with energy E and a hole obtained by removing that electron. Since removing an electron reduces the total energy of the system, the hole's energy is opposite to that of an electron E_h = -E . The probability for an electron state to be occupied in equilibrium is given by f(E) : f(E) = \\frac{1}{e^{(E-E_F)/kT} + 1}. Since a hole is a missing electron, the probability for a hole state to be occupied is f_h(E) = 1 - f(E) = 1 - \\frac{1}{e^{(E-E_F)/kT} + 1} = \\frac{1}{e^{(-E+E_F)/kT} + 1}, which is the Fermi distribution of particles with energy E_h= -E and E_{F,h} = -E_F . The momentum p_h of a hole should give the correct total momentum of a partially filled band if one sums momenta of all holes. Therefore p_h = -\\hbar k , where k is the wave vector of the electron. Similarly, the total charge should be the same regardless of whether we count electrons or holes, so holes have a positive charge +e (electrons having -e ). On the other hand, hole's velocity is the same as that of an electron: \\frac{dE_h}{dp_h} = \\frac{-dE}{-d\\hbar k} = \\frac{dE}{dp}. Finally, we derive the hole effective mass from the equations of motion: m_h \\frac{d v}{d t} = +e (E + v\\times B). Comparing with m_e \\frac{d v}{d t} = -e (E + v\\times B), we get m_h = -m_e (we could also obtain this by differentiating the hole's velocity). Semiconductors: materials with two bands. \u00b6 Semiconductors are materials with all bands either nearly occupied or almost empty. Unlike in insulators, however, the band gap in semiconductors is sufficiently small, for it to be possible to create a few electrons in the lowest unoccupied band or the highest filled band. Because in the unoccupied band the charge carriers (particles carrying electric current) are electrons, it is called conduction band , while in the almost occupied valence band the charge carriers are holes. Holes in semiconductors When introducting holes, we discussed holes obtained by removing any electron. From this point on, similar to most other discussions of semiconductors in the literature, we will only speak of holes in valence band and electrons in conduction band. The occupation of the two bands is dictated by the Fermi distribution. Furthermore, the Fermi level of a semiconductor lies between the conduction and the valence bands, and the band gap E_G \\gg k_B T in most materials. As a result, only the bottom of the conduction band has electrons and the top of the valence band has holes. Therefore we can approximate the dispersion relation of both bands as parabolic, like shown below Or in other words \\begin{align} E_e &= E_c + \\frac{\\hbar^2k^2}{2m_e},\\\\ E_h &= E_{v,h} + \\frac{\\hbar^2k^2}{2m_h} = -E_{v} + \\frac{\\hbar^2k^2}{2m_h}. \\end{align} Here E_c is the energy of an electron at the bottom of the conduction band and E_v is the energy of an electron at the top of the valence band. Observe that because we are describing particles in the valence band as holes, m_h > 0 and E_h > -E_v . The corresponding density of states of the two types of particles is \\begin{align} g(E) &= (2m_e)^{3/2}\\sqrt{E-E_c}/2\\pi^2\\hbar^3,\\\\ g_h(E_h) &= (2m_h)^{3/2}\\sqrt{E_h+E_v}/2\\pi^2\\hbar^3. \\end{align} A photon gives a single electron enough energy to move from the valence band to the conduction band. How many particles does this process create? Two: one electron and one hole. Semiconductor density of states and Fermi level \u00b6 Intrinsic semiconductor \u00b6 Our next task is to figure out how many electrons and holes there are, and for that we need to find where the Fermi level E_F is located. Let us plot the density of states, the Fermi distribution function, and the density of particles at each energy in the same plot: We know that by itself, the semiconductor should have no charge, and therefore the total numbers of electrons and holes must be equal. Since increasing the Fermi level increases the number of electrons and reduces the number of holes, we will use the charge neutrality condition to determine where the Fermi level is situated. The key algorithm of describing the state of a semiconductor: Compute the density of states of all types of particles. Calculate the number of electrons in the conduction band and holes in the valence band, assuming a certain value of E_F Write down the charge balance condition: the difference between electrons and holes should equal the total charge of the semiconductor. Apply approximations to simplify the equations (this is important!). Find E_F and concentrations of electrons and holes Applying the first two steps of the algorithm: n_h = \\int_{-E_v}^\\infty f_h(E_h) g_h(E_h) dE_h = \\int_{-E_v}^\\infty\\frac{(2m_h)^{3/2}}{2\\pi^2\\hbar^3}\\sqrt{E_h+E_v}\\frac{1}{e^{(E_h+E_F)/kT}+1}dE_h n_e = \\int_{E_c}^\\infty f(E)g_e(E)dE = \\int_{E_c}^\\infty\\frac{(2m_e)^{3/2}}{2\\pi^2\\hbar^3} \\sqrt{E-E_c}\\frac{1}{e^{(E-E_F)/kT}+1}dE. Note that whenever calculating the hole dependent quantities, we replace all the relevant physical quantities with their hole equivalents. Since the hole energy is opposite E_h = -E , we replace the Fermi energy E_F \\to -E_F and the bottom of the valance band by E_v \\to -E_v in the integration limits. In the third step, we need to solve the equation under charge balance n_e = n_h . The equation is not a pleasant one and cannot be solved analytically unless an approximation is made. Therefore, the fourth step assumes that the Fermi level is far from both bands E_F-E_v \\gg kT and E_c - E_F \\gg kT . As a result, the Fermi-Dirac distribution is approximately similar to Boltzmann distribution: f(E)_{e/h} \\approx \\exp\\left[-(E_{e/h}\\pm E_F)/kT\\right]. Now we can move to the last step and calculate n_e and n_h : n_h \\approx \\frac{(2m_h)^{3/2}}{2\\pi^2\\hbar^3}e^{-E_F/kT} \\int_{-E_v}^\\infty\\sqrt{E_h+E_v}e^{-E_h/kT}dE_h = N_V e^{(E_v-E_F)/kT}, where we used \\int_0^\\infty \\sqrt{x}e^{-x}dx=\\sqrt{\\pi}/2 and we defined N_V = 2\\left(\\frac{2\\pi m_h kT}{h^2}\\right)^{3/2}. We see that holes are exponentially activated into the valence band. how large is N_V at room temperature? (hard question) If kT \\sim 1\\textrm{eV} (the typical energy size of a band), then electrons in the whole band may be excited and N_V \\sim 1 per unit cell. On the other hand, N_V \\sim T^{3/2} Therefore N_V \\sim (kT/1 \\textrm{eV})^{3/2}\\sim 1\\% . Similarly for electrons: n_e = N_C e^{-(E_c - E_F)/kT},\\quad N_C = 2\\left(\\frac{2\\pi m_e kT}{h^2}\\right)^{3/2}. Combining everything together: n_h \\approx N_V e^{(E_v-E_F)/kT} = N_C e^{-(E_c-E_F)/kT} \\approx n_e. Solving for E_F : E_F = \\frac{E_c + E_v}{2} - \\frac{3}{4}kT\\ln(m_e/m_h). An extra observation: regardless of where E_F is located, n_e n_h = N_C N_V e^{-E_g/kT} \\equiv n_i^2 , where E_g=E_c-E_v is the band gap of the semiconductor. n_i is the intrinsic carrier concentration , and for a pristine semiconductor n_e = n_h = n_i . The equation n_e n_h = n_i^2 is the law of mass action . The name is borrowed from chemistry, and describes the equilibrium concentration of two reagents in a reaction A+B \\leftrightarrow AB . Here electrons and hole constantly split and recombine. Conduction \u00b6 Earlier, we deduced that empty and filled bands provide no current. We finish the analysis by considering partially filled bands of an intrinsic (pristine) semiconductor. To calculate the current, we utilize the Drude model and sum the electron and hole contributions: j = -n_e e v_e + n_h e v_h -m_e v_e /\\tau_e = -eE;\\quad -m_h v_h /\\tau_h = eE. We see that despite opposite velocity signs for electrons and holes, they carry electric current in the same direction: \\sigma \\equiv \\frac{j}{E} = \\left(\\frac{n_e e^2 \\tau_e}{m_e}+\\frac{n_h e^2 \\tau_h}{m_h}\\right) = n_e e \\mu_e + n_h e \\mu_h. We know that for intrinsic semiconductors, the hole/electron densities are n_e = n_h = n_i \\propto e^{-E_G/kT} . Therefore, it is possible to measure the band gap of an intrinsic semiconductor by looking at the temperature dependant conductivity E_G \\approx d \\ln \\sigma / d [kT]^{-1} . Additional information can be obtained using Hall effect. However Hall effect is much more complex in semiconductors since only the current in the direction perpendicular to the applied electric field must vanish. This, however only means that the electron current is opposite of the hole current in that direction, not that the electrons and holes move parallel to the applied current. Exercises \u00b6 Exercise 1: Energy, mass, velocity and cyclotron motion of electrons and holes \u00b6 Consider the top of the valence band of a semiconductor (see above ). Does an electron near the top of the valence band have a positive or a negative effective mass? Does the electron's energy increase or decrease as k increases from 0? Does the electron have a positive or negative group velocity for k>0 ? Answer the same last 3 questions for a hole in the valence band. We now consider an electron in a 2D semiconductor near the bottom of the conduction band described by an energy dispersion E=E_{G}+\\frac{\\hbar^2}{2m^*}(k_x^2+k_y^2) . The electron's velocity is given by \\mathbf{v}=\\nabla_\\mathbf{k} E/\\hbar = \\frac{1}{\\hbar}(\\frac{\\partial E}{\\partial k_x}\\mathbf{\\hat{x}} + \\frac{\\partial E}{\\partial k_y}\\mathbf{\\hat{y}}) . Suppose we turn on a magnetic field B in the z -direction. Write down the equation of motion for this electron (neglecting collisions). What is the shape of the motion of the electron? What is the characteristic 'cyclotron' frequency of this motion? What is the direction of the Lorentz force with respect to \\nabla_\\mathbf{k} E ? Suppose we now consider a hole near the bottom of the conduction band and turn on a magnetic field B in the z -direction. Is the direction of the circular motion (i.e., the chirality) of the hole the same as that of the electron? Would the chirality change if we instead consider a hole (or electron) near the top of the valence band? Exercise 2: holes in Drude and tight binding model \u00b6 Recall from the Drude model that electrons give rise to a negative Hall coefficient. Explain why the Hall coefficient is positive if holes are the charge carriers in a material. What would be the Hall coefficient if both carriers with equal concentration are present? Assume that both electrons and holes can move freely and have the same scattering time. Recall that the dispersion relation of a 1D single orbital tight binding chain is given by E(k)=\\varepsilon + 2t \\cos(ka), where a is the lattice constant and \\varepsilon and t are tight binding parameters. What is the group velocity and effective mass of this band for holes compared to that of electrons? Give an integral expression of the hole concentration in this band given the chemical potential \\mu and temperature T . Show that the sum of the electron and hole concentration in this band is constant as a function of the temperature. Exercise 3: a 1D semiconductor \u00b6 Suppose we have a 1D semiconductor with a conduction band described by E_{cb} = E_G - 2 t_{cb} [\\cos(ka)-1], and a valence band described by E_{vb} = 2 t_{vb} [\\cos(ka)-1]. Furthermore, the chemical potential is set at 0 < \\mu < E_G . Derive an expression for the group velocity and effective mass for electrons in the conduction bands and holes in the valence band. Assume that the Fermi level is far away from both bands. That is, |E - \\mu| \\gg k_B T . In that case, it is acceptable to approximate the bands for low k . Why is it acceptable? Write down an approximate expression of these bands. Write down an expression for the density of states per unit length for both bands using the approximated expressions. Compare with the actual density of states per unit length. Calculate the electron density in the conduction band and the hole density in the valence band. What would the chemical potential \\mu be in case of an intrinsic semiconductor?","title":"Semiconductors 101"},{"location":"6-semiconductors/6-1-semiconductors/#semiconductor-physics","text":"","title":"Semiconductor physics"},{"location":"6-semiconductors/6-1-semiconductors/#introduction","text":"Expected competencies It is assumed that you have familiarity with the following concepts/techniques: * Text reference The material covered here is discussed in section(s) \\S 17 and 18 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Up until this point, we focused on calculating and understanding the band structures. However, the dispersion of a band is only part of the story. An empty band is not going to lead to any interesting physical properties no matter how sophisticated it is. Therefore, it is also important how bands are filled by the particles. By carefully controlling the distribution of particles in the bands, we are able to engineer material properties that we require. Without a doubt, the greatest example is semiconductors \u2014the bedrock of modern electronics. In this lecture, we shall grasp the basics of semiconductors by learning how to treat bands at different levels of filling.","title":"Introduction"},{"location":"6-semiconductors/6-1-semiconductors/#review-of-band-structure-properties","text":"Before proceeding further, let us remind ourselves of important band structure properties. Group velocity v=\\hbar^{-1}\\partial E(k)/\\partial k . Descibes how quickly electrons move within the lattice. Effective mass m^* = \\hbar^2\\left(d^2 E(k)/dk^2\\right)^{-1} . Tells us how hard it is to accelerate the particles and is related to the curvature of the band. Density of states g(E) = \\sum_{\\textrm{FS}} (dn/dk) \\times (dk/dE) . The amount of states per infinitesimal interval of energy at given energy. The quantity is vital in order to calculate any bulk property of the material such as conductivity, heat capacity, etc. In order to check that everything makes sense, we apply the concepts to the free electron model: H = \\frac{\\hbar^2 k^2}{2m} The velocity is \\hbar^{-1}\\partial E(k)/\\partial k = \\hbar k / m \\equiv p/m . The effective mass is m^* = \\hbar^2\\left(d^2 E(k)/dk^2\\right)^{-1} = m . So in this simplest case the definitions match the usual expressions.","title":"Review of band structure properties"},{"location":"6-semiconductors/6-1-semiconductors/#filled-vs-empty-bands","text":"We distinguish three different band filling types: filled, empty and partially filled. Despite being two opposite extreme extreme cases, filled and empty bands are very similar. For example, both filled and empty bands carry no electric current: \\begin{align} j = 2e \\frac{1}{2\\pi} \\int_{-\\pi/a}^{\\pi/a} v(k) dk = 2e \\frac{1}{2\\pi \\hbar} \\int_{-\\pi/a}^{\\pi/a} \\frac{dE}{dk} \\times dk = \\\\ 2e \\frac{1}{2\\pi \\hbar} [E(\\pi/a) - E(-\\pi/a)] = 0. \\end{align} On the other hand, a filled band has an equal number of electrons going forwards and backwards which thus cancel and lead to zero current. Similar results apply to many other physical quantities such as heat capacity and magnetization. Therefore, filled and empty bands do not affect most physical properties and can be disregarded. As a result, rather than to consider hundreds of bands that a material contains, we neglect most of them and just focus on the handful of partially filled bands around Fermi level.","title":"Filled vs empty bands"},{"location":"6-semiconductors/6-1-semiconductors/#from-electrons-to-holes","text":"Because completely filled or completely empty bands have simple properties, we may search for a convenient way to describe a band that only has a few electrons missing or extra. While keeping track of a few electrons has no tricks, even a few electrons missing from a band seem to require considering all the other electrons in a band. A more efficient approach to describing a nearly filled band is motivated by the following analogy. Let us say we have 100 boxes: 99 are occupied and 1 is empty. To keep track which box is occupied/empty, we could write down the numbers of all 99 occupied boxes. If, on the other hand, we only keep track which single box is empty, we solve the problem with a lot less book-keeping. The same approach applies to band filling. Instead of describing a lot of electrons that are present in an almost filled band, we focus on those that are absent. The absence of an electron is called a hole : a state of a completely filled band with one particle missing. In this schematic we can either say that 8\u00d72 electron states are occupied (the system has 8\u00d72 electrons counting spin), or 10\u00d72 hole states are occupied. Electron and hole pictures correspond to two different, but equivalent ways of describing the occupation of a band. Naturally, dealing with electrons is more convenient whenever a band is almost empty and with holes when a band is almost full.","title":"From electrons to holes"},{"location":"6-semiconductors/6-1-semiconductors/#properties-of-holes","text":"Let us compare the properties of an electron with energy E and a hole obtained by removing that electron. Since removing an electron reduces the total energy of the system, the hole's energy is opposite to that of an electron E_h = -E . The probability for an electron state to be occupied in equilibrium is given by f(E) : f(E) = \\frac{1}{e^{(E-E_F)/kT} + 1}. Since a hole is a missing electron, the probability for a hole state to be occupied is f_h(E) = 1 - f(E) = 1 - \\frac{1}{e^{(E-E_F)/kT} + 1} = \\frac{1}{e^{(-E+E_F)/kT} + 1}, which is the Fermi distribution of particles with energy E_h= -E and E_{F,h} = -E_F . The momentum p_h of a hole should give the correct total momentum of a partially filled band if one sums momenta of all holes. Therefore p_h = -\\hbar k , where k is the wave vector of the electron. Similarly, the total charge should be the same regardless of whether we count electrons or holes, so holes have a positive charge +e (electrons having -e ). On the other hand, hole's velocity is the same as that of an electron: \\frac{dE_h}{dp_h} = \\frac{-dE}{-d\\hbar k} = \\frac{dE}{dp}. Finally, we derive the hole effective mass from the equations of motion: m_h \\frac{d v}{d t} = +e (E + v\\times B). Comparing with m_e \\frac{d v}{d t} = -e (E + v\\times B), we get m_h = -m_e (we could also obtain this by differentiating the hole's velocity).","title":"Properties of holes"},{"location":"6-semiconductors/6-1-semiconductors/#semiconductors-materials-with-two-bands","text":"Semiconductors are materials with all bands either nearly occupied or almost empty. Unlike in insulators, however, the band gap in semiconductors is sufficiently small, for it to be possible to create a few electrons in the lowest unoccupied band or the highest filled band. Because in the unoccupied band the charge carriers (particles carrying electric current) are electrons, it is called conduction band , while in the almost occupied valence band the charge carriers are holes. Holes in semiconductors When introducting holes, we discussed holes obtained by removing any electron. From this point on, similar to most other discussions of semiconductors in the literature, we will only speak of holes in valence band and electrons in conduction band. The occupation of the two bands is dictated by the Fermi distribution. Furthermore, the Fermi level of a semiconductor lies between the conduction and the valence bands, and the band gap E_G \\gg k_B T in most materials. As a result, only the bottom of the conduction band has electrons and the top of the valence band has holes. Therefore we can approximate the dispersion relation of both bands as parabolic, like shown below Or in other words \\begin{align} E_e &= E_c + \\frac{\\hbar^2k^2}{2m_e},\\\\ E_h &= E_{v,h} + \\frac{\\hbar^2k^2}{2m_h} = -E_{v} + \\frac{\\hbar^2k^2}{2m_h}. \\end{align} Here E_c is the energy of an electron at the bottom of the conduction band and E_v is the energy of an electron at the top of the valence band. Observe that because we are describing particles in the valence band as holes, m_h > 0 and E_h > -E_v . The corresponding density of states of the two types of particles is \\begin{align} g(E) &= (2m_e)^{3/2}\\sqrt{E-E_c}/2\\pi^2\\hbar^3,\\\\ g_h(E_h) &= (2m_h)^{3/2}\\sqrt{E_h+E_v}/2\\pi^2\\hbar^3. \\end{align} A photon gives a single electron enough energy to move from the valence band to the conduction band. How many particles does this process create? Two: one electron and one hole.","title":"Semiconductors: materials with two bands."},{"location":"6-semiconductors/6-1-semiconductors/#semiconductor-density-of-states-and-fermi-level","text":"","title":"Semiconductor density of states and Fermi level"},{"location":"6-semiconductors/6-1-semiconductors/#intrinsic-semiconductor","text":"Our next task is to figure out how many electrons and holes there are, and for that we need to find where the Fermi level E_F is located. Let us plot the density of states, the Fermi distribution function, and the density of particles at each energy in the same plot: We know that by itself, the semiconductor should have no charge, and therefore the total numbers of electrons and holes must be equal. Since increasing the Fermi level increases the number of electrons and reduces the number of holes, we will use the charge neutrality condition to determine where the Fermi level is situated. The key algorithm of describing the state of a semiconductor: Compute the density of states of all types of particles. Calculate the number of electrons in the conduction band and holes in the valence band, assuming a certain value of E_F Write down the charge balance condition: the difference between electrons and holes should equal the total charge of the semiconductor. Apply approximations to simplify the equations (this is important!). Find E_F and concentrations of electrons and holes Applying the first two steps of the algorithm: n_h = \\int_{-E_v}^\\infty f_h(E_h) g_h(E_h) dE_h = \\int_{-E_v}^\\infty\\frac{(2m_h)^{3/2}}{2\\pi^2\\hbar^3}\\sqrt{E_h+E_v}\\frac{1}{e^{(E_h+E_F)/kT}+1}dE_h n_e = \\int_{E_c}^\\infty f(E)g_e(E)dE = \\int_{E_c}^\\infty\\frac{(2m_e)^{3/2}}{2\\pi^2\\hbar^3} \\sqrt{E-E_c}\\frac{1}{e^{(E-E_F)/kT}+1}dE. Note that whenever calculating the hole dependent quantities, we replace all the relevant physical quantities with their hole equivalents. Since the hole energy is opposite E_h = -E , we replace the Fermi energy E_F \\to -E_F and the bottom of the valance band by E_v \\to -E_v in the integration limits. In the third step, we need to solve the equation under charge balance n_e = n_h . The equation is not a pleasant one and cannot be solved analytically unless an approximation is made. Therefore, the fourth step assumes that the Fermi level is far from both bands E_F-E_v \\gg kT and E_c - E_F \\gg kT . As a result, the Fermi-Dirac distribution is approximately similar to Boltzmann distribution: f(E)_{e/h} \\approx \\exp\\left[-(E_{e/h}\\pm E_F)/kT\\right]. Now we can move to the last step and calculate n_e and n_h : n_h \\approx \\frac{(2m_h)^{3/2}}{2\\pi^2\\hbar^3}e^{-E_F/kT} \\int_{-E_v}^\\infty\\sqrt{E_h+E_v}e^{-E_h/kT}dE_h = N_V e^{(E_v-E_F)/kT}, where we used \\int_0^\\infty \\sqrt{x}e^{-x}dx=\\sqrt{\\pi}/2 and we defined N_V = 2\\left(\\frac{2\\pi m_h kT}{h^2}\\right)^{3/2}. We see that holes are exponentially activated into the valence band. how large is N_V at room temperature? (hard question) If kT \\sim 1\\textrm{eV} (the typical energy size of a band), then electrons in the whole band may be excited and N_V \\sim 1 per unit cell. On the other hand, N_V \\sim T^{3/2} Therefore N_V \\sim (kT/1 \\textrm{eV})^{3/2}\\sim 1\\% . Similarly for electrons: n_e = N_C e^{-(E_c - E_F)/kT},\\quad N_C = 2\\left(\\frac{2\\pi m_e kT}{h^2}\\right)^{3/2}. Combining everything together: n_h \\approx N_V e^{(E_v-E_F)/kT} = N_C e^{-(E_c-E_F)/kT} \\approx n_e. Solving for E_F : E_F = \\frac{E_c + E_v}{2} - \\frac{3}{4}kT\\ln(m_e/m_h). An extra observation: regardless of where E_F is located, n_e n_h = N_C N_V e^{-E_g/kT} \\equiv n_i^2 , where E_g=E_c-E_v is the band gap of the semiconductor. n_i is the intrinsic carrier concentration , and for a pristine semiconductor n_e = n_h = n_i . The equation n_e n_h = n_i^2 is the law of mass action . The name is borrowed from chemistry, and describes the equilibrium concentration of two reagents in a reaction A+B \\leftrightarrow AB . Here electrons and hole constantly split and recombine.","title":"Intrinsic semiconductor"},{"location":"6-semiconductors/6-1-semiconductors/#conduction","text":"Earlier, we deduced that empty and filled bands provide no current. We finish the analysis by considering partially filled bands of an intrinsic (pristine) semiconductor. To calculate the current, we utilize the Drude model and sum the electron and hole contributions: j = -n_e e v_e + n_h e v_h -m_e v_e /\\tau_e = -eE;\\quad -m_h v_h /\\tau_h = eE. We see that despite opposite velocity signs for electrons and holes, they carry electric current in the same direction: \\sigma \\equiv \\frac{j}{E} = \\left(\\frac{n_e e^2 \\tau_e}{m_e}+\\frac{n_h e^2 \\tau_h}{m_h}\\right) = n_e e \\mu_e + n_h e \\mu_h. We know that for intrinsic semiconductors, the hole/electron densities are n_e = n_h = n_i \\propto e^{-E_G/kT} . Therefore, it is possible to measure the band gap of an intrinsic semiconductor by looking at the temperature dependant conductivity E_G \\approx d \\ln \\sigma / d [kT]^{-1} . Additional information can be obtained using Hall effect. However Hall effect is much more complex in semiconductors since only the current in the direction perpendicular to the applied electric field must vanish. This, however only means that the electron current is opposite of the hole current in that direction, not that the electrons and holes move parallel to the applied current.","title":"Conduction"},{"location":"6-semiconductors/6-1-semiconductors/#exercises","text":"","title":"Exercises"},{"location":"6-semiconductors/6-1-semiconductors/#exercise-1-energy-mass-velocity-and-cyclotron-motion-of-electrons-and-holes","text":"Consider the top of the valence band of a semiconductor (see above ). Does an electron near the top of the valence band have a positive or a negative effective mass? Does the electron's energy increase or decrease as k increases from 0? Does the electron have a positive or negative group velocity for k>0 ? Answer the same last 3 questions for a hole in the valence band. We now consider an electron in a 2D semiconductor near the bottom of the conduction band described by an energy dispersion E=E_{G}+\\frac{\\hbar^2}{2m^*}(k_x^2+k_y^2) . The electron's velocity is given by \\mathbf{v}=\\nabla_\\mathbf{k} E/\\hbar = \\frac{1}{\\hbar}(\\frac{\\partial E}{\\partial k_x}\\mathbf{\\hat{x}} + \\frac{\\partial E}{\\partial k_y}\\mathbf{\\hat{y}}) . Suppose we turn on a magnetic field B in the z -direction. Write down the equation of motion for this electron (neglecting collisions). What is the shape of the motion of the electron? What is the characteristic 'cyclotron' frequency of this motion? What is the direction of the Lorentz force with respect to \\nabla_\\mathbf{k} E ? Suppose we now consider a hole near the bottom of the conduction band and turn on a magnetic field B in the z -direction. Is the direction of the circular motion (i.e., the chirality) of the hole the same as that of the electron? Would the chirality change if we instead consider a hole (or electron) near the top of the valence band?","title":"Exercise 1: Energy, mass, velocity and cyclotron motion of electrons and holes"},{"location":"6-semiconductors/6-1-semiconductors/#exercise-2-holes-in-drude-and-tight-binding-model","text":"Recall from the Drude model that electrons give rise to a negative Hall coefficient. Explain why the Hall coefficient is positive if holes are the charge carriers in a material. What would be the Hall coefficient if both carriers with equal concentration are present? Assume that both electrons and holes can move freely and have the same scattering time. Recall that the dispersion relation of a 1D single orbital tight binding chain is given by E(k)=\\varepsilon + 2t \\cos(ka), where a is the lattice constant and \\varepsilon and t are tight binding parameters. What is the group velocity and effective mass of this band for holes compared to that of electrons? Give an integral expression of the hole concentration in this band given the chemical potential \\mu and temperature T . Show that the sum of the electron and hole concentration in this band is constant as a function of the temperature.","title":"Exercise 2: holes in Drude and tight binding model"},{"location":"6-semiconductors/6-1-semiconductors/#exercise-3-a-1d-semiconductor","text":"Suppose we have a 1D semiconductor with a conduction band described by E_{cb} = E_G - 2 t_{cb} [\\cos(ka)-1], and a valence band described by E_{vb} = 2 t_{vb} [\\cos(ka)-1]. Furthermore, the chemical potential is set at 0 < \\mu < E_G . Derive an expression for the group velocity and effective mass for electrons in the conduction bands and holes in the valence band. Assume that the Fermi level is far away from both bands. That is, |E - \\mu| \\gg k_B T . In that case, it is acceptable to approximate the bands for low k . Why is it acceptable? Write down an approximate expression of these bands. Write down an expression for the density of states per unit length for both bands using the approximated expressions. Compare with the actual density of states per unit length. Calculate the electron density in the conduction band and the hole density in the valence band. What would the chemical potential \\mu be in case of an intrinsic semiconductor?","title":"Exercise 3: a 1D semiconductor"},{"location":"6-semiconductors/6-2-devices/","text":"Devices \u00b6 Introduction \u00b6 Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Text reference The material covered here is discussed in section(s) \\S 18 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: In the previous section, we learned how to deal with partially filled bands. The concept of electrons/holes established the foundations needed to understand semiconductors. We saw that the filling in semiconductors can be controlled by tuning the temperature. However, Fermi level control through temperature is still far too constrained and leads to equal electron and hole densities n_e = n_h . The full utility of semiconductors is achieved through another Fermi level control method - doping . In today's lecture, we will take a look at how doping allows the fine control of Fermi level and the practical applications that come with it. Adding an impurity to semiconductor \u00b6 In order to understand doping, we need to remember some basic chemistry. Most semiconductors are made up of group IV elements (Si, Ge) or binary compounds between group III-V elements (GaAs). In both cases, there are 4 valance electrons per atom. If we want to increase the average number of electrons per atom, we can add a group V element that has an extra valance electron. We therefore refer to group V elements as donor impurities. However, the extra donor electron is bound to the impurity because group V elements also have an extra proton. In order to estimate the binding strength, we treat the lattice as a background and only consider the system of an electron bound to a proton. We immediately recognize this system as a Hydrogen model with energy levels E_n = - \\frac{m_e e^4}{8\\pi^2\\hbar^3\\varepsilon^2_0n^2} = -R_E /n^2= -\\frac{13.6\\text{eV}}{n^2}. The spatial extent of the bound state is given by the Bohr radius: r_B = 4 \\pi \\varepsilon_0 \\hbar^2/m_{\\mathrm{e}} e^2. However, we have to remember that the above equations are written in the free space background. In our case, the extra electron moves in the semiconductor's conduction band and not free space. Therefore, there are a couple of differences from the Hydrogen model. One difference is that the electron's mass is conduction band's effective mass. Another difference is that the interactions between the electron and proton are screened by the lattice. As a result, we need to introduce the following substitutions: m_e \\to m_e^* , \\epsilon_0 \\to \\epsilon\\epsilon_0 . We thus estimate the energy of the bound state created by the impurity: E = -\\frac{m_e^*}{m_e \\varepsilon^2} R_E = -0.01 \\text{eV (in Ge)}, with Bohr radius r_B = 4 nm (vs r_B = 0.5 \u00c5 in Hydrogen). The electron is very weakly bound to the impurity! At room temperature (0.026 eV), the donor electron is easily thermally excited into the conduction band. On the other hand, we can add a group III element to reduce the average number of electrons in the system. Group III elements lacks 1 electron and 1 proton and are therefore known as acceptors . We treat the absence of an electron as a hole and the lacking proton as an effective negative charge. As a result, we once again end up with a Hydrogen model, except this time the charges are flipped (hole circles around a negative center). That allows us to use the previous results and to conclude that an acceptor creates a weakly bound state above the valance band. Density of states with donors and acceptors \u00b6 In order to model multiple donor/acceptor states, we assume that they are all degenerate at the binding energy. Therefore, we model the density of states of donors/acceptors as a Dirac delta function: g_D(E) = N_D \\delta(E- E_D), \\quad g_A(E) = N_A \\delta(E-E_A), where N_D and N_A are donor and acceptor concentrations respectively. The binding energies of the donor and acceptor are defined as E_A and E_D . How good is this Dirac delta approximation? That depends on the concentrations. If we keep on adding impurities, then at some point the weakly bound states will begin to overlap. The overlap will create an effective tight-binding model that leads to a formation of an \"impurity\" band which breaks our approximation. We must therefore prevent the overlap of impurity bound states. From the previous section, we know that the extent of the bound state is roughly 4 nm and thus the distance between impurity atoms cannot exceed that. As a result, the impurity concentration is bounded to N_D \\lesssim (1/4\\textrm{nm})^3 . Number of carriers \u00b6 Symbol Meaning n_e Concentration of electrons in the conduction band n_h Concentration of holes in the valance band n_D Concentration of electrons in the donor bound state n_A Concentration of holes in the acceptor bound state N_D Concentration of donor impurities N_A Concentration of acceptor impurities We now have the necessary tools to determine how the Fermi level changes with doping. The algorithm to determine the Fermi level of a semiconductor was outlined in the previous lecture and we continue to use it here. The process is the same up until the third step - charge conservation. The semiconductor now contains impurities that become charged through ionization. For example, if the donor impurity bound state loses an electron - it becomes positively charged. We determine the electron/hole occupation of the donor/acceptor states by applying Fermi-Dirac statistics to their simple Dirac delta density of states: n_D = N_D \\frac{1}{e^{(E_D-E_F)/kT} + 1}, n_A = N_A \\frac{1}{e^{(E_F-E_A)/kT} + 1}. Here we refer to n_D ( n_A ) as the electron(hole) concentration inside donor(acceptor) bound state. With this, the charge balance equation reads: n_e - n_h + n_D - n_A = N_D - N_A. The equation is not an easy one to solve: all of the terms on the lhs depend non-trivially on E_F . In order to solve it, we require several approximations: Firstly, we assume that the Fermi level is far from both bands E_F\u2212E_v \\gg kT and E_c\u2212E_F \\gg kT . The approximation allows us to use the law of mass action from the previous lecture: n_e n_h = N_C N_V e^{-E_g/kT} \\equiv n_i^2. Secondly, we determined that electrons/holes are weakly bound to the impurities. Therefore, at ambient temperatures, we assume that all the impurities are fully ionized and therefore n_D = n_A = 0 . The approximations allow us to simplify the charge balance equation: n_e - n_i^2/n_e = N_D - N_A, which is just the quadratic equation for n_e . When |N_D-N_A| \\gg n_i the semiconductor is extrinsic , so that if N_D > N_A ( n -doped semiconductor), n_e \\approx N_D -N_A and n_h = n_i^2/(N_D-N_A) . If N_D < N_A ( p -doped semiconductor), n_h \\approx N_A -N_D and n_e = n_i^2/(N_A-N_D) . We can now easily find the Fermi level. From the first approximation, we know that the simplified relation between n_{e/h} and E_F is: n_e \\approx N_C e^{-(E_c - E_F)/kT}, n_h \\approx N_V e^{(E_v-E_F)/kT}. We express the lhs with the quadratic equation solution and solve for Fermi level: E_F = E_c - kT\\ln[N_C/(N_D-N_A)], \\textrm{ for } N_D > N_A and E_F = kT\\ln[N_V/(N_A-N_D)], \\textrm{ for } N_A > N_D When is a semiconductor intrinsic, and when it is extrinsic? By definition the semiconductor is intrinsic when |N_D-N_A| \\ll n_i , so kT \\gtrsim E_G/\\log[N_C N_V/(N_D-N_A)^2] . Temperature dependence of the carrier density and Fermi level \u00b6 It is instructive to consider how E_F , n_e and n_h depend on carrier concentrations. In this case, we consider an n-doped semiconductor, however, the same logic applies to p-doped semiconductors. There are several relevant temperature limits: Intrinsic limit . If the temperature is sufficiently large, then n_i \\gg |N_D-N_A| and therefore n_e = n_h = n_i . Additionally, if holes are heavier than electrons, then E_F has an upturn in this limit. Extrinsic limit . If we decrease the temperature, we decrease the number of intrinsic carriers to the point where most of the charge carriers come form the fully ionized donors. As a result, the number of carriers stays approximately constant in this temperature range. Freeze-out limit . Once the temperature is sufficiently low kT \\ll E_G - E_D , we expect the electrons to \"freeze away\" from the conduction band to the donor band. The charge carriers still come from the donors, however, not all donors are ionized now. Zero temperature . There are no charge carriers in neither conduction nor valance bands. The highest energy electrons are in the donor band and therefore E_F should match the donor band. Exercise check that you can reproduce all the relevant limits in a calculation. Combining semiconductors: pn -junction \u00b6 What happens if we bring two differently doped semiconductors together (one of p -type, one of n -type)? Band diagram \u00b6 Previously we dealt with homogeneous materials, now the position coordinate (let's call it x ) starts playing a role. We represent the properties of inhomogeneous materials using the band diagram . The main idea is to plot the dependence of various energies ( E_F , bottom of conduction band E_C , top of the valence band E_V ) as a function of position. Let us build up the band diagram step by step: The main difference between n -type and p -type semiconductors is the location of the Fermi level E_F (see \"n and p\" tab above). The Fermi level of an n -type semiconductor is close to the donor states. On the other hand, the p -type semiconductor has its Fermi level near the acceptor states. At equilibrium (no external fields), we do not expect to see any currents in the system and therefore the Fermi level E_F must be constant across the system (see \"Equilibrium\" tab). To achieve a homogenous Fermi level, we could bring up in energy the p -type region or bring down the n -type region until the Fermi levels are aligned. However, a question arises: what happens at the junction? We can understand the junction with a simple picture. In physics, most of the time we expect things to change continuously . Therefore, we expect that the valance E_V and conduction E_C bands connect continuously in the middle region as shown in the \"Band Bending\" tab. On the contrary, if the bands were to be discontinuous, then an electric field must develop at a single point in the middle region to shift the bands in energy. However, we do not expect such point-like electric fields to develop because electrons can move freely in semiconductors. On a more microscopic level, the electrons at the junction in the n -type semiconductor will move into the p -type semiconductor to recombine with the holes. After the recombination, the n and p -type semiconductors lose an electron and a hole respectively. As a result, a positive ionized donor dopant is not screened anymore and the n -type semiconductor obtains a positive overall charge. Similarly, p -type region obtains a negative charge. Therefore, an electric field develops across the junction. As the recombination process continues, a larger charge density \\rho develops and thus the electric field grows until it is large enough to prevent the electrons/holes from crossing the junction. Inside the region, energy deviates by \\delta \\varphi \\gg kT from the bulk value and thus the density of electrons/holes drops exponentially fast. Therefore, we refer to the region as the depletion region . The charge density \\rho distribution inside of a depletion region is shown below: The typical values of w_n+w_p are \\sim 1 \\mu \\textrm{m} at N_A,\\,N_D \\sim 10^{16} \\textrm{cm}^{-3} , and \\sim 0.1 \\mu \\textrm{m} at N_A,\\,N_D \\sim 10^{18} \\textrm{cm}^{-3} , so it may be much larger than the distance between the dopant atoms. pn -junction diode \u00b6 What happens if we apply voltage to a junction? Because the conductivity of the p -region and n -region is much larger than that of the depletion region, most of the voltage difference will appear in the depletion region: The number of majority carriers moving across the junction is proportional to their concentration. Increasing the voltage bias \"pushes\" carriers up in energy, it depends exponentially on the voltage. We therefore get the Shockley diode equation: I = I_0 \\left(\\exp(eV/kT) -1\\right) Solar cell \u00b6 Light absorbed in the pn -junction creates electron-hole pairs. The eletric field then moves electrons to the n -doped region, holes to the p -doped one, and therefore generates a voltage. Semiconducting laser \u00b6 A heavily doped pn -junction so that the Fermi level is in the conduction/valence band produces an extremely high rate electron-hole recombination with an extremely high rate, and makes the pn -junction function like a laser. MOSFET and quantum well \u00b6 See the book for details. Summary \u00b6 Density of states in a doped semiconductor: Charge balance determines the number of electrons and holes as well as the position of the Fermi level. If dopant concentration is low, then n_e = n_h = n_i \\equiv \\sqrt{N_C N_V}e^{-E_G/2kT} . If dopant concentration is high, then in n -doped semiconductor n_e = N_D - N_A and n_h = n_i^2/n_e (or vice versa in p -doped one). Temperature switches between intrinsic and extrinsic regimes, and controls the carrier density Conductance combines the contributions of electrons and holes, and allows to determine E_G . A pn -junction has a depletion layer in its middle with the potential in a pn -junction having the following shape (where the transition region is made out of two parabolas): Exercises \u00b6 Exercise 1: Crossover between extrinsic and intrinsic regimes \u00b6 In the lecture we have identified the intrinsic and extrinsic regimes. Let us now work out what happens when the semiconductor is at the border between these two regimes, and the dopant concentration |N_D - N_A| is comparable to the intrinsic one n_i . Write down the law of mass action and the charge balance condition for a doped semiconductor. Solve this system of equations for n_e and n_h only assuming E_G \\gg k_B T . Verify that your solution reproduces intrinsic regime when |N_D - N_A| \u226a n_i and the extrinsic regime when |N_D - N_A| \u226b n_i Exercise 2: Donor ionization \u00b6 Let us examine when the full donor ionization is a good assumption. For that we consider a doped semiconductor in the extrinsic regime. Assume that all dopants are ionized, determine the position of the Fermi level. Write down the concentration of dopants that are not ionized. Determine at what donor concentration one cannot assume anymore that all donors are ionized in germanium at room temperature. Exercise 3: Performance of a diode \u00b6 Consider a pn-junction diode as follows source By Raffamaiden CC BY-SA 3.0 ), Link The current flowing through a diode as a function of applied bias voltage is given by the Shockley diode equation: I(V) = I_s(T)\\left(e^{\\frac{eV}{kT}}-1\\right) where I_s(T) is the saturation current. What is the significance of adding dopant atoms to an intrinsic semiconductor? Can two intrinsic semiconductors joined together make a diode? Discuss which processes carry current in a diode under reverse bias. Based on this, estimate how the saturation current I_s depends on temperature. Exercise 4: Quantum well heterojunction in detail \u00b6 Consider a a quantum well formed from a layer of GaAs of thickness L , surrounded by layers of Al _{x} Ga _{1\u2212x} As. source Vectorised by User:Sushant savla from the work by Gianderiu - Quantum well.svg , CC-BY-SA 3.0 . Assume that the band gap of the Al _{x} Ga _{1\u2212x} As is substantially larger than that of GaAs. The electron effective mass in GaAs is 0.068 m_{e} , the hole effective mass is 0.45 m_{e} with m_{e} the mass of the electron. Sketch the band diagram of this quantum well. Write down the Schr\u00f6dinger's equation for electrons and holes Find the energies of electron and holes in the quantum well as a function of k_x, k_y . Calculate the density of states of electron and holes in this quantum well. If we want to design a quantum well with a bandgap 0.1 eV larger than that of bulk GaAs , what thickness L do we need? Why is this structure more useful for making a laser than a normal pn-junction? What would be the advantage of doping the Al _{x} Ga _{1\u2212x} As compared to the GaAs in this quantum well?","title":"Doping and devices"},{"location":"6-semiconductors/6-2-devices/#devices","text":"","title":"Devices"},{"location":"6-semiconductors/6-2-devices/#introduction","text":"Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Text reference The material covered here is discussed in section(s) \\S 18 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: In the previous section, we learned how to deal with partially filled bands. The concept of electrons/holes established the foundations needed to understand semiconductors. We saw that the filling in semiconductors can be controlled by tuning the temperature. However, Fermi level control through temperature is still far too constrained and leads to equal electron and hole densities n_e = n_h . The full utility of semiconductors is achieved through another Fermi level control method - doping . In today's lecture, we will take a look at how doping allows the fine control of Fermi level and the practical applications that come with it.","title":"Introduction"},{"location":"6-semiconductors/6-2-devices/#adding-an-impurity-to-semiconductor","text":"In order to understand doping, we need to remember some basic chemistry. Most semiconductors are made up of group IV elements (Si, Ge) or binary compounds between group III-V elements (GaAs). In both cases, there are 4 valance electrons per atom. If we want to increase the average number of electrons per atom, we can add a group V element that has an extra valance electron. We therefore refer to group V elements as donor impurities. However, the extra donor electron is bound to the impurity because group V elements also have an extra proton. In order to estimate the binding strength, we treat the lattice as a background and only consider the system of an electron bound to a proton. We immediately recognize this system as a Hydrogen model with energy levels E_n = - \\frac{m_e e^4}{8\\pi^2\\hbar^3\\varepsilon^2_0n^2} = -R_E /n^2= -\\frac{13.6\\text{eV}}{n^2}. The spatial extent of the bound state is given by the Bohr radius: r_B = 4 \\pi \\varepsilon_0 \\hbar^2/m_{\\mathrm{e}} e^2. However, we have to remember that the above equations are written in the free space background. In our case, the extra electron moves in the semiconductor's conduction band and not free space. Therefore, there are a couple of differences from the Hydrogen model. One difference is that the electron's mass is conduction band's effective mass. Another difference is that the interactions between the electron and proton are screened by the lattice. As a result, we need to introduce the following substitutions: m_e \\to m_e^* , \\epsilon_0 \\to \\epsilon\\epsilon_0 . We thus estimate the energy of the bound state created by the impurity: E = -\\frac{m_e^*}{m_e \\varepsilon^2} R_E = -0.01 \\text{eV (in Ge)}, with Bohr radius r_B = 4 nm (vs r_B = 0.5 \u00c5 in Hydrogen). The electron is very weakly bound to the impurity! At room temperature (0.026 eV), the donor electron is easily thermally excited into the conduction band. On the other hand, we can add a group III element to reduce the average number of electrons in the system. Group III elements lacks 1 electron and 1 proton and are therefore known as acceptors . We treat the absence of an electron as a hole and the lacking proton as an effective negative charge. As a result, we once again end up with a Hydrogen model, except this time the charges are flipped (hole circles around a negative center). That allows us to use the previous results and to conclude that an acceptor creates a weakly bound state above the valance band.","title":"Adding an impurity to semiconductor"},{"location":"6-semiconductors/6-2-devices/#density-of-states-with-donors-and-acceptors","text":"In order to model multiple donor/acceptor states, we assume that they are all degenerate at the binding energy. Therefore, we model the density of states of donors/acceptors as a Dirac delta function: g_D(E) = N_D \\delta(E- E_D), \\quad g_A(E) = N_A \\delta(E-E_A), where N_D and N_A are donor and acceptor concentrations respectively. The binding energies of the donor and acceptor are defined as E_A and E_D . How good is this Dirac delta approximation? That depends on the concentrations. If we keep on adding impurities, then at some point the weakly bound states will begin to overlap. The overlap will create an effective tight-binding model that leads to a formation of an \"impurity\" band which breaks our approximation. We must therefore prevent the overlap of impurity bound states. From the previous section, we know that the extent of the bound state is roughly 4 nm and thus the distance between impurity atoms cannot exceed that. As a result, the impurity concentration is bounded to N_D \\lesssim (1/4\\textrm{nm})^3 .","title":"Density of states with donors and acceptors"},{"location":"6-semiconductors/6-2-devices/#number-of-carriers","text":"Symbol Meaning n_e Concentration of electrons in the conduction band n_h Concentration of holes in the valance band n_D Concentration of electrons in the donor bound state n_A Concentration of holes in the acceptor bound state N_D Concentration of donor impurities N_A Concentration of acceptor impurities We now have the necessary tools to determine how the Fermi level changes with doping. The algorithm to determine the Fermi level of a semiconductor was outlined in the previous lecture and we continue to use it here. The process is the same up until the third step - charge conservation. The semiconductor now contains impurities that become charged through ionization. For example, if the donor impurity bound state loses an electron - it becomes positively charged. We determine the electron/hole occupation of the donor/acceptor states by applying Fermi-Dirac statistics to their simple Dirac delta density of states: n_D = N_D \\frac{1}{e^{(E_D-E_F)/kT} + 1}, n_A = N_A \\frac{1}{e^{(E_F-E_A)/kT} + 1}. Here we refer to n_D ( n_A ) as the electron(hole) concentration inside donor(acceptor) bound state. With this, the charge balance equation reads: n_e - n_h + n_D - n_A = N_D - N_A. The equation is not an easy one to solve: all of the terms on the lhs depend non-trivially on E_F . In order to solve it, we require several approximations: Firstly, we assume that the Fermi level is far from both bands E_F\u2212E_v \\gg kT and E_c\u2212E_F \\gg kT . The approximation allows us to use the law of mass action from the previous lecture: n_e n_h = N_C N_V e^{-E_g/kT} \\equiv n_i^2. Secondly, we determined that electrons/holes are weakly bound to the impurities. Therefore, at ambient temperatures, we assume that all the impurities are fully ionized and therefore n_D = n_A = 0 . The approximations allow us to simplify the charge balance equation: n_e - n_i^2/n_e = N_D - N_A, which is just the quadratic equation for n_e . When |N_D-N_A| \\gg n_i the semiconductor is extrinsic , so that if N_D > N_A ( n -doped semiconductor), n_e \\approx N_D -N_A and n_h = n_i^2/(N_D-N_A) . If N_D < N_A ( p -doped semiconductor), n_h \\approx N_A -N_D and n_e = n_i^2/(N_A-N_D) . We can now easily find the Fermi level. From the first approximation, we know that the simplified relation between n_{e/h} and E_F is: n_e \\approx N_C e^{-(E_c - E_F)/kT}, n_h \\approx N_V e^{(E_v-E_F)/kT}. We express the lhs with the quadratic equation solution and solve for Fermi level: E_F = E_c - kT\\ln[N_C/(N_D-N_A)], \\textrm{ for } N_D > N_A and E_F = kT\\ln[N_V/(N_A-N_D)], \\textrm{ for } N_A > N_D When is a semiconductor intrinsic, and when it is extrinsic? By definition the semiconductor is intrinsic when |N_D-N_A| \\ll n_i , so kT \\gtrsim E_G/\\log[N_C N_V/(N_D-N_A)^2] .","title":"Number of carriers"},{"location":"6-semiconductors/6-2-devices/#temperature-dependence-of-the-carrier-density-and-fermi-level","text":"It is instructive to consider how E_F , n_e and n_h depend on carrier concentrations. In this case, we consider an n-doped semiconductor, however, the same logic applies to p-doped semiconductors. There are several relevant temperature limits: Intrinsic limit . If the temperature is sufficiently large, then n_i \\gg |N_D-N_A| and therefore n_e = n_h = n_i . Additionally, if holes are heavier than electrons, then E_F has an upturn in this limit. Extrinsic limit . If we decrease the temperature, we decrease the number of intrinsic carriers to the point where most of the charge carriers come form the fully ionized donors. As a result, the number of carriers stays approximately constant in this temperature range. Freeze-out limit . Once the temperature is sufficiently low kT \\ll E_G - E_D , we expect the electrons to \"freeze away\" from the conduction band to the donor band. The charge carriers still come from the donors, however, not all donors are ionized now. Zero temperature . There are no charge carriers in neither conduction nor valance bands. The highest energy electrons are in the donor band and therefore E_F should match the donor band. Exercise check that you can reproduce all the relevant limits in a calculation.","title":"Temperature dependence of the carrier density and Fermi level"},{"location":"6-semiconductors/6-2-devices/#combining-semiconductors-pn-junction","text":"What happens if we bring two differently doped semiconductors together (one of p -type, one of n -type)?","title":"Combining semiconductors: pn-junction"},{"location":"6-semiconductors/6-2-devices/#band-diagram","text":"Previously we dealt with homogeneous materials, now the position coordinate (let's call it x ) starts playing a role. We represent the properties of inhomogeneous materials using the band diagram . The main idea is to plot the dependence of various energies ( E_F , bottom of conduction band E_C , top of the valence band E_V ) as a function of position. Let us build up the band diagram step by step: The main difference between n -type and p -type semiconductors is the location of the Fermi level E_F (see \"n and p\" tab above). The Fermi level of an n -type semiconductor is close to the donor states. On the other hand, the p -type semiconductor has its Fermi level near the acceptor states. At equilibrium (no external fields), we do not expect to see any currents in the system and therefore the Fermi level E_F must be constant across the system (see \"Equilibrium\" tab). To achieve a homogenous Fermi level, we could bring up in energy the p -type region or bring down the n -type region until the Fermi levels are aligned. However, a question arises: what happens at the junction? We can understand the junction with a simple picture. In physics, most of the time we expect things to change continuously . Therefore, we expect that the valance E_V and conduction E_C bands connect continuously in the middle region as shown in the \"Band Bending\" tab. On the contrary, if the bands were to be discontinuous, then an electric field must develop at a single point in the middle region to shift the bands in energy. However, we do not expect such point-like electric fields to develop because electrons can move freely in semiconductors. On a more microscopic level, the electrons at the junction in the n -type semiconductor will move into the p -type semiconductor to recombine with the holes. After the recombination, the n and p -type semiconductors lose an electron and a hole respectively. As a result, a positive ionized donor dopant is not screened anymore and the n -type semiconductor obtains a positive overall charge. Similarly, p -type region obtains a negative charge. Therefore, an electric field develops across the junction. As the recombination process continues, a larger charge density \\rho develops and thus the electric field grows until it is large enough to prevent the electrons/holes from crossing the junction. Inside the region, energy deviates by \\delta \\varphi \\gg kT from the bulk value and thus the density of electrons/holes drops exponentially fast. Therefore, we refer to the region as the depletion region . The charge density \\rho distribution inside of a depletion region is shown below: The typical values of w_n+w_p are \\sim 1 \\mu \\textrm{m} at N_A,\\,N_D \\sim 10^{16} \\textrm{cm}^{-3} , and \\sim 0.1 \\mu \\textrm{m} at N_A,\\,N_D \\sim 10^{18} \\textrm{cm}^{-3} , so it may be much larger than the distance between the dopant atoms.","title":"Band diagram"},{"location":"6-semiconductors/6-2-devices/#pn-junction-diode","text":"What happens if we apply voltage to a junction? Because the conductivity of the p -region and n -region is much larger than that of the depletion region, most of the voltage difference will appear in the depletion region: The number of majority carriers moving across the junction is proportional to their concentration. Increasing the voltage bias \"pushes\" carriers up in energy, it depends exponentially on the voltage. We therefore get the Shockley diode equation: I = I_0 \\left(\\exp(eV/kT) -1\\right)","title":"pn-junction diode"},{"location":"6-semiconductors/6-2-devices/#solar-cell","text":"Light absorbed in the pn -junction creates electron-hole pairs. The eletric field then moves electrons to the n -doped region, holes to the p -doped one, and therefore generates a voltage.","title":"Solar cell"},{"location":"6-semiconductors/6-2-devices/#semiconducting-laser","text":"A heavily doped pn -junction so that the Fermi level is in the conduction/valence band produces an extremely high rate electron-hole recombination with an extremely high rate, and makes the pn -junction function like a laser.","title":"Semiconducting laser"},{"location":"6-semiconductors/6-2-devices/#mosfet-and-quantum-well","text":"See the book for details.","title":"MOSFET and quantum well"},{"location":"6-semiconductors/6-2-devices/#summary","text":"Density of states in a doped semiconductor: Charge balance determines the number of electrons and holes as well as the position of the Fermi level. If dopant concentration is low, then n_e = n_h = n_i \\equiv \\sqrt{N_C N_V}e^{-E_G/2kT} . If dopant concentration is high, then in n -doped semiconductor n_e = N_D - N_A and n_h = n_i^2/n_e (or vice versa in p -doped one). Temperature switches between intrinsic and extrinsic regimes, and controls the carrier density Conductance combines the contributions of electrons and holes, and allows to determine E_G . A pn -junction has a depletion layer in its middle with the potential in a pn -junction having the following shape (where the transition region is made out of two parabolas):","title":"Summary"},{"location":"6-semiconductors/6-2-devices/#exercises","text":"","title":"Exercises"},{"location":"6-semiconductors/6-2-devices/#exercise-1-crossover-between-extrinsic-and-intrinsic-regimes","text":"In the lecture we have identified the intrinsic and extrinsic regimes. Let us now work out what happens when the semiconductor is at the border between these two regimes, and the dopant concentration |N_D - N_A| is comparable to the intrinsic one n_i . Write down the law of mass action and the charge balance condition for a doped semiconductor. Solve this system of equations for n_e and n_h only assuming E_G \\gg k_B T . Verify that your solution reproduces intrinsic regime when |N_D - N_A| \u226a n_i and the extrinsic regime when |N_D - N_A| \u226b n_i","title":"Exercise 1: Crossover between extrinsic and intrinsic regimes"},{"location":"6-semiconductors/6-2-devices/#exercise-2-donor-ionization","text":"Let us examine when the full donor ionization is a good assumption. For that we consider a doped semiconductor in the extrinsic regime. Assume that all dopants are ionized, determine the position of the Fermi level. Write down the concentration of dopants that are not ionized. Determine at what donor concentration one cannot assume anymore that all donors are ionized in germanium at room temperature.","title":"Exercise 2: Donor ionization"},{"location":"6-semiconductors/6-2-devices/#exercise-3-performance-of-a-diode","text":"Consider a pn-junction diode as follows source By Raffamaiden CC BY-SA 3.0 ), Link The current flowing through a diode as a function of applied bias voltage is given by the Shockley diode equation: I(V) = I_s(T)\\left(e^{\\frac{eV}{kT}}-1\\right) where I_s(T) is the saturation current. What is the significance of adding dopant atoms to an intrinsic semiconductor? Can two intrinsic semiconductors joined together make a diode? Discuss which processes carry current in a diode under reverse bias. Based on this, estimate how the saturation current I_s depends on temperature.","title":"Exercise 3: Performance of a diode"},{"location":"6-semiconductors/6-2-devices/#exercise-4-quantum-well-heterojunction-in-detail","text":"Consider a a quantum well formed from a layer of GaAs of thickness L , surrounded by layers of Al _{x} Ga _{1\u2212x} As. source Vectorised by User:Sushant savla from the work by Gianderiu - Quantum well.svg , CC-BY-SA 3.0 . Assume that the band gap of the Al _{x} Ga _{1\u2212x} As is substantially larger than that of GaAs. The electron effective mass in GaAs is 0.068 m_{e} , the hole effective mass is 0.45 m_{e} with m_{e} the mass of the electron. Sketch the band diagram of this quantum well. Write down the Schr\u00f6dinger's equation for electrons and holes Find the energies of electron and holes in the quantum well as a function of k_x, k_y . Calculate the density of states of electron and holes in this quantum well. If we want to design a quantum well with a bandgap 0.1 eV larger than that of bulk GaAs , what thickness L do we need? Why is this structure more useful for making a laser than a normal pn-junction? What would be the advantage of doping the Al _{x} Ga _{1\u2212x} As compared to the GaAs in this quantum well?","title":"Exercise 4: Quantum well heterojunction in detail"},{"location":"solutions/3-3-solutions/","text":"Solutions to exercises \u00b6 Solutions for The specific heat of solids I exercises \u00b6 Solutions for lecture 7 exercises \u00b6 Warm up exercises \u00b6 Check by yourself 2. [m^*] = \\frac{[p^2]}{[E]} = kg 3. m^* = m_e, where m_e is the free electron mass. This is expected because the free elctrons are not subject to a potential If the dispersion relation is parabolic, so in the free electron model. Exercise 1: Next-nearest neighbours chain \u00b6 1. The Schr\u00f6dinger equation is given by: |\\Psi\\rangle = \\sum_n \\phi_n |n\\rangle such that we find E\\phi_n = E_0\\phi_n - t\\phi_{n-1} - t\\phi_{n+1} - t'\\phi_{n-2} - t'\\phi_{n+2} 2. Solving the Schr\u00f6dinger equation yields dispersion: E(k) = E_0 -2t\\cos(ka) -2t'\\cos(2ka) 3. m^* = \\frac{\\hbar^2}{2a^2}\\frac{1}{t\\cos(ka)+4t'\\cos(2ka)} Plot for t=t': k1 = np . linspace ( - pi , - pi / 2 - 0.01 , 300 ); k2 = np . linspace ( - pi / 2 + 0.01 , pi / 2 - 0.01 , 300 ); k3 = np . linspace ( pi / 2 + 0.01 , pi , 300 ); pyplot . plot ( k1 , 1 / ( 5 * np . cos ( k1 )), 'b' ); pyplot . plot ( k2 , 1 / ( 5 * np . cos ( k2 )), 'b' ); pyplot . plot ( k3 , 1 / ( 5 * np . cos ( k3 )), 'b' ); pyplot . xlabel ( r '$k$' ); pyplot . ylabel ( '$m_ {eff} (k)$' ); pyplot . xticks ([ - pi , 0 , pi ],[ r '$-\\pi/a$' , 0 , r '$\\pi/a$' ]); pyplot . yticks ([],[]); pyplot . tight_layout (); 4. Plots for 2t'=t, 4t'=t and 10t'=t: def m ( k , t ): return 1 / ( np . cos ( k ) + 4 * t * np . cos ( 2 * k )) k1 = np . linspace ( - 1.6 , - 0.83 , 300 ); k2 = np . linspace ( - 0.826 , 0.826 , 300 ); k3 = np . linspace ( 0.83 , 1.6 , 300 ); pyplot . plot ( k1 , m ( k1 , 2 ), 'b' ); pyplot . plot ( k2 , m ( k2 , 2 ), 'b' ); pyplot . plot ( k3 , m ( k3 , 2 ), 'b' , label = 't=2t \\' ' ); pyplot . xlabel ( '$k$' ); pyplot . ylabel ( '$m_ {eff} (k)$' ); pyplot . xticks ([ - 1.6 , 0 , 1.6 ],[ r '$-\\pi/a$' , 0 , r '$\\pi/a$' ]); pyplot . yticks ([ 0 ],[]); pyplot . tight_layout (); k1 = np . linspace ( - 1.58 , - 0.81 , 300 ); k2 = np . linspace ( - 0.804 , 0.804 , 300 ); k3 = np . linspace ( 0.81 , 1.58 , 300 ); pyplot . plot ( k1 , m ( k1 , 4 ), 'r' ); pyplot . plot ( k2 , m ( k2 , 4 ), 'r' ); pyplot . plot ( k3 , m ( k3 , 4 ), 'r' , label = 't=4t \\' ' ); k1 = np . linspace ( - 1.575 , - 0.798 , 300 ); k2 = np . linspace ( - 0.790 , 0.790 , 300 ); k3 = np . linspace ( 0.798 , 1.575 , 300 ); pyplot . plot ( k1 , m ( k1 , 10 ), 'k' ); pyplot . plot ( k2 , m ( k2 , 10 ), 'k' ); pyplot . plot ( k3 , m ( k3 , 10 ), 'k' , label = 't=10t \\' ' ); pyplot . legend ();","title":"Solutions to exercises"},{"location":"solutions/3-3-solutions/#solutions-to-exercises","text":"","title":"Solutions to exercises"},{"location":"solutions/3-3-solutions/#solutions-for-the-specific-heat-of-solids-i-exercises","text":"","title":"Solutions for The specific heat of solids I exercises"},{"location":"solutions/3-3-solutions/#solutions-for-lecture-7-exercises","text":"","title":"Solutions for lecture 7 exercises"},{"location":"solutions/3-3-solutions/#warm-up-exercises","text":"Check by yourself 2. [m^*] = \\frac{[p^2]}{[E]} = kg 3. m^* = m_e, where m_e is the free electron mass. This is expected because the free elctrons are not subject to a potential If the dispersion relation is parabolic, so in the free electron model.","title":"Warm up exercises"},{"location":"solutions/3-3-solutions/#exercise-1-next-nearest-neighbours-chain","text":"1. The Schr\u00f6dinger equation is given by: |\\Psi\\rangle = \\sum_n \\phi_n |n\\rangle such that we find E\\phi_n = E_0\\phi_n - t\\phi_{n-1} - t\\phi_{n+1} - t'\\phi_{n-2} - t'\\phi_{n+2} 2. Solving the Schr\u00f6dinger equation yields dispersion: E(k) = E_0 -2t\\cos(ka) -2t'\\cos(2ka) 3. m^* = \\frac{\\hbar^2}{2a^2}\\frac{1}{t\\cos(ka)+4t'\\cos(2ka)} Plot for t=t': k1 = np . linspace ( - pi , - pi / 2 - 0.01 , 300 ); k2 = np . linspace ( - pi / 2 + 0.01 , pi / 2 - 0.01 , 300 ); k3 = np . linspace ( pi / 2 + 0.01 , pi , 300 ); pyplot . plot ( k1 , 1 / ( 5 * np . cos ( k1 )), 'b' ); pyplot . plot ( k2 , 1 / ( 5 * np . cos ( k2 )), 'b' ); pyplot . plot ( k3 , 1 / ( 5 * np . cos ( k3 )), 'b' ); pyplot . xlabel ( r '$k$' ); pyplot . ylabel ( '$m_ {eff} (k)$' ); pyplot . xticks ([ - pi , 0 , pi ],[ r '$-\\pi/a$' , 0 , r '$\\pi/a$' ]); pyplot . yticks ([],[]); pyplot . tight_layout (); 4. Plots for 2t'=t, 4t'=t and 10t'=t: def m ( k , t ): return 1 / ( np . cos ( k ) + 4 * t * np . cos ( 2 * k )) k1 = np . linspace ( - 1.6 , - 0.83 , 300 ); k2 = np . linspace ( - 0.826 , 0.826 , 300 ); k3 = np . linspace ( 0.83 , 1.6 , 300 ); pyplot . plot ( k1 , m ( k1 , 2 ), 'b' ); pyplot . plot ( k2 , m ( k2 , 2 ), 'b' ); pyplot . plot ( k3 , m ( k3 , 2 ), 'b' , label = 't=2t \\' ' ); pyplot . xlabel ( '$k$' ); pyplot . ylabel ( '$m_ {eff} (k)$' ); pyplot . xticks ([ - 1.6 , 0 , 1.6 ],[ r '$-\\pi/a$' , 0 , r '$\\pi/a$' ]); pyplot . yticks ([ 0 ],[]); pyplot . tight_layout (); k1 = np . linspace ( - 1.58 , - 0.81 , 300 ); k2 = np . linspace ( - 0.804 , 0.804 , 300 ); k3 = np . linspace ( 0.81 , 1.58 , 300 ); pyplot . plot ( k1 , m ( k1 , 4 ), 'r' ); pyplot . plot ( k2 , m ( k2 , 4 ), 'r' ); pyplot . plot ( k3 , m ( k3 , 4 ), 'r' , label = 't=4t \\' ' ); k1 = np . linspace ( - 1.575 , - 0.798 , 300 ); k2 = np . linspace ( - 0.790 , 0.790 , 300 ); k3 = np . linspace ( 0.798 , 1.575 , 300 ); pyplot . plot ( k1 , m ( k1 , 10 ), 'k' ); pyplot . plot ( k2 , m ( k2 , 10 ), 'k' ); pyplot . plot ( k3 , m ( k3 , 10 ), 'k' , label = 't=10t \\' ' ); pyplot . legend ();","title":"Exercise 1: Next-nearest neighbours chain"},{"location":"solutions/solutions/","text":"Solutions to exercises \u00b6 Solutions for The specific heat of solids I exercises \u00b6 Preliminary provocations \u00b6 C = 2k_B . Exercise 1: Total heat capacity of a diatomic material \u00b6 Use the formula \\omega = \\sqrt{\\frac{k}{m}} . Energy per atom is given by E = \\frac{N_{^6Li}}{N}\\hbar\\omega_{^6Li}(2 + 1/2) + \\frac{N_{^7Li}}{N}\\hbar\\omega_{^7Li}(4 + 1/2). Energy per atom is given by E = \\frac{N_{^6Li}}{N}\\hbar\\omega_{^6Li}\\left(n_B(\\beta\\hbar\\omega_{^6Li}) + \\frac{1}{2}\\right) + \\frac{N_{^7Li}}{N}\\hbar\\omega_{^7Li}\\left(n_B(\\beta\\hbar\\omega_{^7Li}) + \\frac{1}{2}\\right). Heat capacity per atom is given by C = \\frac{N_{^6Li}}{N}C_{^6Li} + \\frac{N_{^7Li}}{N}C_{^7Li}, Solutions for The specific heat of solids II exercises \u00b6 Preliminary provocations \u00b6 The polarization is related to the direction of the amplitudes of the waves with respect to the direction of the wave. In 3D, there are only 3 different amplitude directions possible. One can convert the integral as follows: \\int k_x k_y \\rightarrow \\int_{0}^{2\\pi} \\mathrm{d} \\theta \\int_{0}^{\\infty} k \\mathrm{d} k = 2\\pi \\int_{0}^{\\infty} k \\mathrm{d} k The Debye frequency \\omega_D . From the definition of the Debye frequency, one can calculate that the wavelength is of the order of the interatomic spacing: \\lambda = (\\frac{4}{3}\\pi)^{1/3} a. Exercise 1: Debye model - concepts. \u00b6 It is clear that n=4 , and thus k = \\frac{2 \\pi n}{L} = \\pm \\frac{4\\pi}{L} . 2. The number of states per k or per frequency. 4. g(\\omega) = \\frac{dN}{d\\omega} = \\frac{dN}{dk}\\frac{dk}{d\\omega} = \\frac{1}{v}\\frac{dN}{dk}. We assume that in d dimensions there are d polarizations. For 1D we have that N = \\frac{L}{2\\pi}\\int_{-k}^{k} dk , hence g(\\omega) = \\frac{L}{\\pi v} . For 2D we have that N = 2\\left(\\frac{L}{2\\pi}\\right)^2\\int d^2k = 2\\left(\\frac{L}{2\\pi}\\right)^2\\int 2\\pi kdk , hence g(\\omega) = \\frac{L^2\\omega}{\\pi v^2} . For 3D we have that N = 3\\left(\\frac{L}{2\\pi}\\right)^3\\int d^3k = 3\\left(\\frac{L}{2\\pi}\\right)^3\\int 4\\pi k^2dk , hence g(\\omega) = \\frac{3L^3\\omega^2}{2\\pi^2v^3} . Exercise 2: Debye model in 2D. \u00b6 See lecture notes. \\begin{align} E &= \\int_{0}^{\\omega_D}g(\\omega)\\hbar\\omega\\left(\\frac{1}{e^{\\beta\\hbar\\omega} - 1} + \\frac{1}{2}\\right)d\\omega \\\\ &= \\frac{L^2}{\\pi v^2\\hbar^2\\beta^3}\\int_{0}^{\\beta\\hbar\\omega_D}\\frac{x^2}{e^{x} - 1}dx + C. \\end{align} High temperature implies \\beta \\rightarrow 0 , hence E = \\frac{L^2}{\\pi v^2\\hbar^2\\beta^3}\\frac{(\\beta\\hbar\\omega_D)^2}{2} + C , and then C = \\frac{k_BL^2\\omega^2_D}{2\\pi v^2} = 2Nk_B . We've used the value for \\omega_D calculated from 2N = \\int_{0}^{\\omega_D}g(\\omega)d\\omega . In the low temperature limit we have that \\beta \\rightarrow \\infty , hence E \\approx \\frac{L^2}{\\pi v^2\\hbar^2\\beta^3}\\int_{0}^{\\infty}\\frac{x^2}{e^{x} - 1}dx + C = \\frac{2\\zeta(3)L^2}{\\pi v^2\\hbar^2\\beta^3} + C . Finally C = \\frac{6\\zeta(3)k^3_BL^2}{\\pi v^2\\hbar^2}T^2 . We used the fact that \\int_{0}^{\\infty}\\frac{x^2}{e^{x} - 1}dx = 2\\zeta(3) where \\zeta is the Riemann zeta function. Exercise 3: Different oscillation modes. \u00b6 g(\\omega) = \\sum_{\\text{polarizations}}\\frac{dN}{dk}\\frac{dk}{d\\omega} = \\left(\\frac{L}{2\\pi}\\right)^3\\sum_{\\text{polarizations}}4\\pi k^2\\frac{dk}{d\\omega} = \\frac{L^3}{2\\pi^2}\\left(\\frac{2}{v_\\perp^3} + \\frac{1}{v_\\parallel^3}\\right)\\omega^2 E = \\int_{0}^{\\omega_D}g(\\omega)\\hbar\\omega\\left(\\frac{1}{e^{\\beta\\hbar\\omega} - 1} + \\frac{1}{2}\\right)d\\omega = \\frac{L^3}{2\\pi^2\\hbar^3\\beta^4}\\left(\\frac{2}{v_\\perp^3} + \\frac{1}{v_\\parallel^3}\\right)\\int_{0}^{\\beta\\hbar\\omega_D}\\frac{x^3}{e^{x} - 1}dx + C. Note that we can get \\omega_D from 3N = \\int_{0}^{\\omega_D}g(\\omega) so everything cancels as usual and we are left with the Dulong-Petit law C = 3Nk_B . In the low temperature limit we have that C \\sim \\frac{2\\pi^2k_B^4L^3}{15\\hbar^3}\\left(\\frac{2}{v_\\perp^3} + \\frac{1}{v_\\parallel^3}\\right)T^3 . We used that \\int_{0}^{\\infty}\\frac{x^3}{e^{x} - 1}dx = \\frac{\\pi^4}{15} . Exercise 4: Anisotropic sound velocities. \u00b6 \\begin{align} E & = 3\\left(\\frac{L}{2\\pi}\\right)^3\\int d^3k\\hbar\\omega(\\mathbf{k})\\left(n_B(\\beta\\hbar\\omega(\\mathbf{k})) + \\frac{1}{2}\\right) \\\\ & = 3\\left(\\frac{L}{2\\pi}\\right)^3\\frac{1}{v_xv_yv_z}\\int d^3\\kappa\\frac{\\hbar\\kappa}{e^{\\beta\\hbar\\kappa} - 1} + C, \\end{align} where we used the substitutions \\kappa_x = k_xv_x,\\kappa_y = k_yv_y, \\kappa_z = k_zv_z . Finally E = \\frac{3\\hbar L^3}{2\\pi^2}\\frac{1}{v_xv_yv_z}\\int_0^{\\kappa_D} d\\kappa\\frac{\\kappa^3}{e^{\\beta\\hbar\\kappa} - 1} + C = \\frac{3L^3}{2\\pi^2\\hbar^3\\beta^4}\\frac{1}{v_xv_yv_z}\\int_0^{\\beta\\hbar\\kappa_D} dx\\frac{x^3}{e^{x} - 1} + C, hence C = \\frac{\\partial E}{\\partial T} = \\frac{6k_B^4L^3T^3}{\\pi^2\\hbar^3}\\frac{1}{v_xv_yv_z}\\int_0^{\\beta\\hbar\\kappa_D} dx\\frac{x^3}{e^{x} - 1} . We see that the result is similar to the one with the linear dispersion, the only difference is the factor 1/v_xv_yv_z instead of 1/v^3 . Solutions for Electrons in metals I exercises \u00b6 Preliminary provocations \u00b6 How does the resistance of a purely 2D material depend on its size? Check that the units of mobility and the Hall coefficient are correct. (As you should always do!) Explain why the scattering times due to different types of scattering events add up in a reciprocal way. Exercise 1: Extracting quantities from basic Hall measurements \u00b6 We apply a magnetic field \\bf B along the z -direction to a planar (two-dimensional) sample that sits in the xy plane. The sample has width W in the y -direction, length L in the x -direction and we apply a current I along the x -direction. What is the relation between the electric field and the electric potential? V_b - V_a = -\\int_{\\Gamma} \\mathbf{E} \\cdot d\\mathbf{\\ell} if \\Gamma is a path from a to b . Suppose we measure a Hall voltage V_H . Express the Hall resistance R_{xy} = V_H/I as a function of magnetic field. Does R_{xy} depend on the geometry of the sample? Also express R_{xy} in terms of the Hall coefficient R_H . Hall voltage is measured across the sample width. Hence, V_H = -\\int_{0}^{W} E_ydy where E_y = -v_xB . R_{xy} = -\\frac{B}{ne} , so it does not depend on the sample geometry. Assuming we control the magnetic field \\mathbf{B} , what quantity can we extract from a measurement of the Hall resistance? Would a large or a small magnetic field give a Hall voltage that is easier to measure? If hall resistance and magnetic field are known, the charge density is calculated from R_{xy} = -\\frac{B}{ne} . As V_x = -\\frac{I_x}{ne}B , a stronger field makes Hall voltages easier to measure. Express the longitudinal resistance R=V/I , where V is the voltage difference over the sample along the x direction, in terms of the longitudinal resistivity \u03c1_{xx} . Suppose we extracted n from a measurement of the Hall resistance, what quantity can we extract from a measurement of the longitudinal resistance? Does the result depend on the geometry of the sample? R_{xx} = \\frac{\\rho_{xx}L}{W} where \\rho_{xx} = \\frac{m_e}{ne^2\\tau} . Therefore, scattering time ( \\tau ) is known and R_{xx} depend upon the sample geometry. Exercise 2: Motion of an electron in a magnetic and an electric field \u00b6 1. m\\frac{d\\bf v}{dt} = -e(\\bf v \\times \\bf B) Magnetic field affects only the velocities along x and y, i.e., v_x(t) and v_y(t) as they are perpendicular to it. Therefore, the equations of motion for the electron are \\frac{dv_x}{dt} = -\\frac{e v_y B_z}{m} \\frac{dv_y}{dt} = \\frac{e v_x B_z}{m} We can compute v_x(t) and v_y(t) by solving the differential equations in 1. From v_x'' = -\\frac{e^2B_z^2}{m^2}v_x and the initial conditions, we find v_x(t) = v_0 \\cos(\\omega_c t) with \\omega_c=eB_z/m . From this we can derive v_y(t)=v_0\\sin(\\omega_c t) . We now calculate the particle position using x(t)=x(0) + \\int_0^t v_x(t')dt' (and similar for y(t) ). From this we can find a relation between the x - and y -coordinates of the particle (x(t) - x_0)^2 + (y(t) - y_0)^2 = \\frac{v_0^2}{\\omega_c^2}. This equation describes a circular motion around the point x_0=x(0), y_0=y(0)+v_0/\\omega , where the characteristic frequency \\omega_c is called the cyclotron frequency. Intuition: \\frac{mv^2}{r} = evB (centripetal force = Lorentz force due to magnetic field). Due to the applied electric field \\bf E in the x -direction, the equations of motion acquire an extra term: m v_x' = -e(E_x + v_yB_z). Differentiating w.r.t. time leads to the same 2nd-order D.E. for v_x as above. However, for v_y we get v_y'' = -\\omega_c^2(v_d+v_y), where we defined v_d=\\frac{E_x}{B_z} . The general solutions are v_y(t) = c_1\\sin(\\omega_c t)+ c_2\\cos(\\omega_c t) -v_d \\\\ v_x(t) = c_3\\sin(\\omega_c t)+ c_4\\cos(\\omega_c t). Using the initial conditions v_x(0)=v_0 and v_y(0)=0 and the 1st order D.E. above, we can show v_y(t) = v_0\\sin(\\omega_c t)+ v_d\\cos(\\omega_c t) -v_d \\\\ v_x(t) = v_d\\sin(\\omega_c t)+ v_0\\cos(\\omega_c t). By integrating the expressions for the velocity we find: (x(t)-x_0)^2 + (y(t) - y_0 + v_d t))^2 = \\frac{v_0^2}{\\omega_c^2}. This represents a cycloid : a circular motion around a point that moves in the y -direction with velocity v_d=\\frac{E_x}{B_z} . Exercise 3: Temperature dependence of resistance in the Drude model \u00b6 Find electron density from n_e = \\frac{Z n N_A}{W} where Z is valence of copper atom, n is density, N_A is Avogadro constant and W is atomic weight. Use \\rho = 1/\\sigma from the lecture notes to calculate scattering time. \\sigma = \\frac{n e^2 \\tau}{m} \\lambda = \\langle v \\rangle\\tau Scattering time \\tau \\propto \\frac{1}{\\sqrt{T}} ; \\rho \\propto \\sqrt{T} In general, \\rho \\propto T as the phonons in the system scales linearly with T (remember high temperature limit of Bose-Einstein factor becomes \\frac{kT}{\\hbar\\omega} leading to \\rho \\propto T ). Inability to explain this linear dependence is a failure of the Drude model. Exercise 4: The Hall conductivity matrix and the Hall coefficient \u00b6 \\rho_{xx} is independent of B and \\rho_{xy} \\propto B 2. \\sigma_{xx} = \\frac{\\rho_{xx}}{\\rho_{xx}^2 + \\rho_{xy}^2} \\sigma_{xy} = \\frac{-\\rho_{yx}}{\\rho_{xx}^2 + \\rho_{xy}^2} This describes a Lorentzian . Solutions for Electrons in metals II exercises \u00b6 Warm-up questions \u00b6 1. E = \\int \\limits_0^{\\infty} g(\\varepsilon) n_{F}(\\beta (\\varepsilon - \\mu)) \\varepsilon \\mathrm{d} \\varepsilon The electronic heat capacity C_e approaches 3N k_B . Thermal smearing is too significant and we can not accurately approximate the fraction of the excited electron with triangles anymore. Thus the Sommerfeld expansion breaks down. Electrons. Exercise 1: potassium \u00b6 Alkali metals mostly have a spherical Fermi surface. Their energy depends only on the magnitude of the Fermi wavevector. Refer to the lecture notes. Electrons are fermions and obey the Pauli exclusion principle. As electrons cannot occupy the same state, they are forced to occupy higher energy states resulting in high Fermi energy and high Fermi temperature. 4. n = \\frac{N}{V} = \\frac{1}{3 \\pi^{2} \\hbar^{3}}\\left(2 m \\varepsilon_{F}\\right)^{3 / 2} 5. n = \\frac{\\rho N_A Z}{M}, where \\rho is the density, N_A is the Avogadro's constant, M is molar mass and Z is the valence of potassium atom. Comparing total and free electron density, only few electrons are available for conduction which is roughly 1 free electron per potassium atom. Exercise 2: a hypothetical material \u00b6 1. E = \\int_{0}^{\\infty}\\varepsilon g(\\varepsilon) n_{F}(\\beta (\\varepsilon - \\mu)) \\textrm{d} \\varepsilon = 2.10^{10}eV^{-\\frac{3}{2}} \\int_{0}^{\\infty}\\frac{\\varepsilon^{\\frac{3}{2}}}{e^\\frac{\\varepsilon-5.2}{k_BT}+1} \\textrm{d} \\varepsilon 2. E = \\frac{4}{5} (5.2)^{\\frac{5}{2}} 10^{10} eV 3. \\begin{align} E(T)-E(T=0) &= \\frac{\\pi^2}{6}(k_B T)^2\\frac{\\partial}{\\partial \\varepsilon}\\left(\\varepsilon g(\\varepsilon)\\right)\\bigg|_{\\varepsilon=\\varepsilon _F}\\\\ &\\approx 8.356 10^8 eV \\end{align} 5. C_v = 1.6713.10^6 eV/K 4, 6. mu = 5.2 kB = 8.617343e-5 T = 1000 #kelvin import numpy as np from scipy import integrate np . seterr ( over = 'ignore' ) # Fermi-Dirac distribution def f ( E , T ): return 1 / ( np . exp (( E - mu ) / ( kB * T )) + 1 ) # Density of states def g ( E ): return 2e10 * np . sqrt ( E ) #integration function def integral ( E , T ): return f ( E , T ) * g ( E ) * E ## Solve integral numerically using scipy's integrate dE = integrate . quad ( integral , 0 , 1e1 , args = ( T ))[ 0 ] - 0.8e10 * 5.2 ** ( 5. / 2 ) dT = 0.001 dEplus = integrate . quad ( integral , 0 , 1e1 , args = ( T + dT ))[ 0 ] - 0.8e10 * 5.2 ** ( 5. / 2 ) dEmin = integrate . quad ( integral , 0 , 1e1 , args = ( T - dT ))[ 0 ] - 0.8e10 * 5.2 ** ( 5. / 2 ) CV = ( dEplus - dEmin ) / ( 2 * dT ); print ( f 'dE = { dE : .4e } eV' ) print ( f 'Cv = { CV : .4e } eV/K' ) Check the source code written in python for solving integral using midpoint rule. Exercise 3: graphene \u00b6 1. import numpy as np import matplotlib.pyplot as plt x = np . linspace ( - 1 , 1 , 100 ) fig , ax = plt . subplots ( figsize = ( 7 , 5 )) ax . plot ( x , x , 'b' ) ax . plot ( x , - x , 'b' ) ax . spines [ 'left' ] . set_position ( 'center' ) ax . spines [ 'bottom' ] . set_position (( 'data' , 0.0 )) # Eliminate upper and right axes ax . spines [ 'right' ] . set_color ( 'none' ) ax . spines [ 'top' ] . set_color ( 'none' ) ax . set_xticks ([]) ax . set_yticks ([]) ax . set_xlabel ( r '$\\mid \\vec k \\mid$' , fontsize = 14 ) ax . set_ylabel ( r '$\\varepsilon$' , fontsize = 18 , rotation = 'horizontal' ) ax . yaxis . set_label_coords ( 0.5 , 1 ) ax . xaxis . set_label_coords ( 1.0 , 0.49 ) 2.The DOS for the positive energies is given by g(\\varepsilon) = 2_s 2_v 2 \\pi \\left(\\frac{L}{2 \\pi}\\right)^2 \\frac{\\varepsilon}{c^2}, where 2_s is the spin degeneracy and 2_v is the valley degeneracy. If we account for the negative energies as well, we obtain g(\\varepsilon) = 2_s 2_v 2 \\pi \\left(\\frac{L}{2 \\pi}\\right)^2 \\frac{|\\varepsilon|}{c^2}. 3. g(\\varepsilon) vs \\varepsilon is a linear plot. Here, the region marked by -k_B T is a triangle whose area gives the number of electrons that can be excited: \\begin{align} n_{ex} &= \\frac{1}{2} g(-k_B T) k_B T\\\\ &= \\frac{L^2 k_B^2T^2}{\\pi c^2}. \\end{align} From this it follows that the energy difference is given by E(T) - E_0 = \\frac{L^2 k_B^3T^3}{\\pi c^2}. 4. C_v(T) = \\frac{\\partial E}{\\partial T} = \\frac{3L^2k_B^3T^2}{\\pi c^2} Solutions for Chemistry 101 exercises \u00b6 Exercise 1: Shell-filling model of atoms \u00b6 See lecture notes. The atomic number of Tungsten is 74: 1\\textrm{s}^2 2\\textrm{s}^2 2\\textrm{p}^6 3\\textrm{s}^23p^64s^23d^{10}4p^65s^24d^{10}5p^66s^24f^{14}5d^4 \\begin{align} \\textrm{Cu} &= [\\textrm{Ar}]4s^23d^9\\\\ \\textrm{Pd} &= [\\textrm{Kr}]5s^24d^8\\\\ \\textrm{Ag} &= [\\textrm{Kr}]5s^24d^9\\\\ \\textrm{Au} &= [\\textrm{Xe}]6s^24f^{14}5d^9 \\end{align} The wheels fall off as V(\\mathbf{r}) \\ne 1/|r| Exercise 2: Application of the LCAO model to the delta-function potential \u00b6 \\psi(x) = \\begin{cases} &\\sqrt{\u03ba}e^{\u03ba(x-x_1)}, x<x_1\\\\ &\\sqrt{\u03ba}e^{-\u03ba(x-x_1)}, x>x_1 \\end{cases} Where \u03ba = \\sqrt{\\frac{-2mE}{\u0127^2}} = \\frac{mV_0}{\u0127^2} . The energy is given by \u03f5_1 = \u03f5_2 = -\\frac{mV_0^2}{2\u0127^2} The wave function of a single delta peak is given by \\psi_1(x) = \\frac{\\sqrt{mV_0}}{\u0127}e^{-\\frac{mV_0}{\u0127^2}|x-x_1|} \\psi_2(x) can be found by replacing x_1 by x_2 H = -\\frac{mV_0^2}{\u0127^2}\\begin{pmatrix} 1/2+\\exp(-\\frac{2mV_0}{\u0127^2}|x_2-x_1|) & \\exp(-\\frac{mV_0}{\u0127^2}|x_2-x_1|)\\\\ \\exp(-\\frac{mV_0}{\u0127^2}|x_2-x_1|) & 1/2+\\exp(-\\frac{2mV_0}{\u0127^2}|x_2-x_1|) \\end{pmatrix} \u03f5_{\\pm} = \\beta(1/2+\\exp(-2\\alpha) \\pm \\exp(-\\alpha)) Where \\beta = -\\frac{mV_0^2}{\u0127^2} and \u03b1 = \\frac{mV_0}{\u0127^2}|x_2-x_1| Exercise 3: Polarization of a hydrogen molecule \u00b6 1. H_{\\mathcal{E}} = ex\\mathcal{E}, 2. \\hat{H} = \\begin{pmatrix} E_0 & -t\\\\ -t & E_0 \\end{pmatrix} +\\begin{pmatrix} \u27e81|ex\\mathcal{E}|1\u27e9 & \u27e81|ex\\mathcal{E}|2\u27e9\\\\ \u27e82|ex\\mathcal{E}|1\u27e9 & \u27e82|ex\\mathcal{E}|2\u27e9 \\end{pmatrix} = \\begin{pmatrix} E_0 - \\gamma & -t\\\\ -t & E_0 + \\gamma \\end{pmatrix}, where \\gamma = e d \\mathcal{E}/2 and have used \u27e81|ex\\mathcal{E}|1\u27e9 = -e d \\mathcal{E}/2\u27e81|1\u27e9 = -e d \\mathcal{E}/2 3. The eigenstates of the Hamiltonian are given by: E_{\\pm} = E_0\\pm\\sqrt{t^2+\\gamma^2} The ground state wave function is: \\begin{split} |\\psi\u27e9 &= \\frac{t}{\\sqrt{(\\gamma+\\sqrt{\\gamma^2+t^2})^2+t^2}}\\begin{pmatrix} \\frac{\\gamma+\\sqrt{t^2+\\gamma^2}}{t}\\\\ 1 \\end{pmatrix}\\\\ |\\psi\u27e9 &= \\frac{\\gamma+\\sqrt{t^2+\\gamma^2}}{\\sqrt{(\\gamma+\\sqrt{\\gamma^2+t^2})^2+t^2}}|1\u27e9+\\frac{t}{\\sqrt{(\\gamma+\\sqrt{\\gamma^2+t^2})^2+t^2}}|2\u27e9 \\end{split} 4. P = -\\frac{2\\gamma^2}{\\mathcal{E}}(\\frac{1}{\\sqrt{\\gamma^2+t^2}})","title":"Solutions to exercises"},{"location":"solutions/solutions/#solutions-to-exercises","text":"","title":"Solutions to exercises"},{"location":"solutions/solutions/#solutions-for-the-specific-heat-of-solids-i-exercises","text":"","title":"Solutions for The specific heat of solids I exercises"},{"location":"solutions/solutions/#preliminary-provocations","text":"C = 2k_B .","title":"Preliminary provocations"},{"location":"solutions/solutions/#exercise-1-total-heat-capacity-of-a-diatomic-material","text":"Use the formula \\omega = \\sqrt{\\frac{k}{m}} . Energy per atom is given by E = \\frac{N_{^6Li}}{N}\\hbar\\omega_{^6Li}(2 + 1/2) + \\frac{N_{^7Li}}{N}\\hbar\\omega_{^7Li}(4 + 1/2). Energy per atom is given by E = \\frac{N_{^6Li}}{N}\\hbar\\omega_{^6Li}\\left(n_B(\\beta\\hbar\\omega_{^6Li}) + \\frac{1}{2}\\right) + \\frac{N_{^7Li}}{N}\\hbar\\omega_{^7Li}\\left(n_B(\\beta\\hbar\\omega_{^7Li}) + \\frac{1}{2}\\right). Heat capacity per atom is given by C = \\frac{N_{^6Li}}{N}C_{^6Li} + \\frac{N_{^7Li}}{N}C_{^7Li},","title":"Exercise 1: Total heat capacity of a diatomic material"},{"location":"solutions/solutions/#solutions-for-the-specific-heat-of-solids-ii-exercises","text":"","title":"Solutions for The specific heat of solids II exercises"},{"location":"solutions/solutions/#preliminary-provocations_1","text":"The polarization is related to the direction of the amplitudes of the waves with respect to the direction of the wave. In 3D, there are only 3 different amplitude directions possible. One can convert the integral as follows: \\int k_x k_y \\rightarrow \\int_{0}^{2\\pi} \\mathrm{d} \\theta \\int_{0}^{\\infty} k \\mathrm{d} k = 2\\pi \\int_{0}^{\\infty} k \\mathrm{d} k The Debye frequency \\omega_D . From the definition of the Debye frequency, one can calculate that the wavelength is of the order of the interatomic spacing: \\lambda = (\\frac{4}{3}\\pi)^{1/3} a.","title":"Preliminary provocations"},{"location":"solutions/solutions/#exercise-1-debye-model-concepts","text":"It is clear that n=4 , and thus k = \\frac{2 \\pi n}{L} = \\pm \\frac{4\\pi}{L} . 2. The number of states per k or per frequency. 4. g(\\omega) = \\frac{dN}{d\\omega} = \\frac{dN}{dk}\\frac{dk}{d\\omega} = \\frac{1}{v}\\frac{dN}{dk}. We assume that in d dimensions there are d polarizations. For 1D we have that N = \\frac{L}{2\\pi}\\int_{-k}^{k} dk , hence g(\\omega) = \\frac{L}{\\pi v} . For 2D we have that N = 2\\left(\\frac{L}{2\\pi}\\right)^2\\int d^2k = 2\\left(\\frac{L}{2\\pi}\\right)^2\\int 2\\pi kdk , hence g(\\omega) = \\frac{L^2\\omega}{\\pi v^2} . For 3D we have that N = 3\\left(\\frac{L}{2\\pi}\\right)^3\\int d^3k = 3\\left(\\frac{L}{2\\pi}\\right)^3\\int 4\\pi k^2dk , hence g(\\omega) = \\frac{3L^3\\omega^2}{2\\pi^2v^3} .","title":"Exercise 1: Debye model - concepts."},{"location":"solutions/solutions/#exercise-2-debye-model-in-2d","text":"See lecture notes. \\begin{align} E &= \\int_{0}^{\\omega_D}g(\\omega)\\hbar\\omega\\left(\\frac{1}{e^{\\beta\\hbar\\omega} - 1} + \\frac{1}{2}\\right)d\\omega \\\\ &= \\frac{L^2}{\\pi v^2\\hbar^2\\beta^3}\\int_{0}^{\\beta\\hbar\\omega_D}\\frac{x^2}{e^{x} - 1}dx + C. \\end{align} High temperature implies \\beta \\rightarrow 0 , hence E = \\frac{L^2}{\\pi v^2\\hbar^2\\beta^3}\\frac{(\\beta\\hbar\\omega_D)^2}{2} + C , and then C = \\frac{k_BL^2\\omega^2_D}{2\\pi v^2} = 2Nk_B . We've used the value for \\omega_D calculated from 2N = \\int_{0}^{\\omega_D}g(\\omega)d\\omega . In the low temperature limit we have that \\beta \\rightarrow \\infty , hence E \\approx \\frac{L^2}{\\pi v^2\\hbar^2\\beta^3}\\int_{0}^{\\infty}\\frac{x^2}{e^{x} - 1}dx + C = \\frac{2\\zeta(3)L^2}{\\pi v^2\\hbar^2\\beta^3} + C . Finally C = \\frac{6\\zeta(3)k^3_BL^2}{\\pi v^2\\hbar^2}T^2 . We used the fact that \\int_{0}^{\\infty}\\frac{x^2}{e^{x} - 1}dx = 2\\zeta(3) where \\zeta is the Riemann zeta function.","title":"Exercise 2: Debye model in 2D."},{"location":"solutions/solutions/#exercise-3-different-oscillation-modes","text":"g(\\omega) = \\sum_{\\text{polarizations}}\\frac{dN}{dk}\\frac{dk}{d\\omega} = \\left(\\frac{L}{2\\pi}\\right)^3\\sum_{\\text{polarizations}}4\\pi k^2\\frac{dk}{d\\omega} = \\frac{L^3}{2\\pi^2}\\left(\\frac{2}{v_\\perp^3} + \\frac{1}{v_\\parallel^3}\\right)\\omega^2 E = \\int_{0}^{\\omega_D}g(\\omega)\\hbar\\omega\\left(\\frac{1}{e^{\\beta\\hbar\\omega} - 1} + \\frac{1}{2}\\right)d\\omega = \\frac{L^3}{2\\pi^2\\hbar^3\\beta^4}\\left(\\frac{2}{v_\\perp^3} + \\frac{1}{v_\\parallel^3}\\right)\\int_{0}^{\\beta\\hbar\\omega_D}\\frac{x^3}{e^{x} - 1}dx + C. Note that we can get \\omega_D from 3N = \\int_{0}^{\\omega_D}g(\\omega) so everything cancels as usual and we are left with the Dulong-Petit law C = 3Nk_B . In the low temperature limit we have that C \\sim \\frac{2\\pi^2k_B^4L^3}{15\\hbar^3}\\left(\\frac{2}{v_\\perp^3} + \\frac{1}{v_\\parallel^3}\\right)T^3 . We used that \\int_{0}^{\\infty}\\frac{x^3}{e^{x} - 1}dx = \\frac{\\pi^4}{15} .","title":"Exercise 3: Different oscillation modes."},{"location":"solutions/solutions/#exercise-4-anisotropic-sound-velocities","text":"\\begin{align} E & = 3\\left(\\frac{L}{2\\pi}\\right)^3\\int d^3k\\hbar\\omega(\\mathbf{k})\\left(n_B(\\beta\\hbar\\omega(\\mathbf{k})) + \\frac{1}{2}\\right) \\\\ & = 3\\left(\\frac{L}{2\\pi}\\right)^3\\frac{1}{v_xv_yv_z}\\int d^3\\kappa\\frac{\\hbar\\kappa}{e^{\\beta\\hbar\\kappa} - 1} + C, \\end{align} where we used the substitutions \\kappa_x = k_xv_x,\\kappa_y = k_yv_y, \\kappa_z = k_zv_z . Finally E = \\frac{3\\hbar L^3}{2\\pi^2}\\frac{1}{v_xv_yv_z}\\int_0^{\\kappa_D} d\\kappa\\frac{\\kappa^3}{e^{\\beta\\hbar\\kappa} - 1} + C = \\frac{3L^3}{2\\pi^2\\hbar^3\\beta^4}\\frac{1}{v_xv_yv_z}\\int_0^{\\beta\\hbar\\kappa_D} dx\\frac{x^3}{e^{x} - 1} + C, hence C = \\frac{\\partial E}{\\partial T} = \\frac{6k_B^4L^3T^3}{\\pi^2\\hbar^3}\\frac{1}{v_xv_yv_z}\\int_0^{\\beta\\hbar\\kappa_D} dx\\frac{x^3}{e^{x} - 1} . We see that the result is similar to the one with the linear dispersion, the only difference is the factor 1/v_xv_yv_z instead of 1/v^3 .","title":"Exercise 4: Anisotropic sound velocities."},{"location":"solutions/solutions/#solutions-for-electrons-in-metals-i-exercises","text":"","title":"Solutions for Electrons in metals I exercises"},{"location":"solutions/solutions/#preliminary-provocations_2","text":"How does the resistance of a purely 2D material depend on its size? Check that the units of mobility and the Hall coefficient are correct. (As you should always do!) Explain why the scattering times due to different types of scattering events add up in a reciprocal way.","title":"Preliminary provocations"},{"location":"solutions/solutions/#exercise-1-extracting-quantities-from-basic-hall-measurements","text":"We apply a magnetic field \\bf B along the z -direction to a planar (two-dimensional) sample that sits in the xy plane. The sample has width W in the y -direction, length L in the x -direction and we apply a current I along the x -direction. What is the relation between the electric field and the electric potential? V_b - V_a = -\\int_{\\Gamma} \\mathbf{E} \\cdot d\\mathbf{\\ell} if \\Gamma is a path from a to b . Suppose we measure a Hall voltage V_H . Express the Hall resistance R_{xy} = V_H/I as a function of magnetic field. Does R_{xy} depend on the geometry of the sample? Also express R_{xy} in terms of the Hall coefficient R_H . Hall voltage is measured across the sample width. Hence, V_H = -\\int_{0}^{W} E_ydy where E_y = -v_xB . R_{xy} = -\\frac{B}{ne} , so it does not depend on the sample geometry. Assuming we control the magnetic field \\mathbf{B} , what quantity can we extract from a measurement of the Hall resistance? Would a large or a small magnetic field give a Hall voltage that is easier to measure? If hall resistance and magnetic field are known, the charge density is calculated from R_{xy} = -\\frac{B}{ne} . As V_x = -\\frac{I_x}{ne}B , a stronger field makes Hall voltages easier to measure. Express the longitudinal resistance R=V/I , where V is the voltage difference over the sample along the x direction, in terms of the longitudinal resistivity \u03c1_{xx} . Suppose we extracted n from a measurement of the Hall resistance, what quantity can we extract from a measurement of the longitudinal resistance? Does the result depend on the geometry of the sample? R_{xx} = \\frac{\\rho_{xx}L}{W} where \\rho_{xx} = \\frac{m_e}{ne^2\\tau} . Therefore, scattering time ( \\tau ) is known and R_{xx} depend upon the sample geometry.","title":"Exercise 1: Extracting quantities from basic Hall measurements"},{"location":"solutions/solutions/#exercise-2-motion-of-an-electron-in-a-magnetic-and-an-electric-field","text":"1. m\\frac{d\\bf v}{dt} = -e(\\bf v \\times \\bf B) Magnetic field affects only the velocities along x and y, i.e., v_x(t) and v_y(t) as they are perpendicular to it. Therefore, the equations of motion for the electron are \\frac{dv_x}{dt} = -\\frac{e v_y B_z}{m} \\frac{dv_y}{dt} = \\frac{e v_x B_z}{m} We can compute v_x(t) and v_y(t) by solving the differential equations in 1. From v_x'' = -\\frac{e^2B_z^2}{m^2}v_x and the initial conditions, we find v_x(t) = v_0 \\cos(\\omega_c t) with \\omega_c=eB_z/m . From this we can derive v_y(t)=v_0\\sin(\\omega_c t) . We now calculate the particle position using x(t)=x(0) + \\int_0^t v_x(t')dt' (and similar for y(t) ). From this we can find a relation between the x - and y -coordinates of the particle (x(t) - x_0)^2 + (y(t) - y_0)^2 = \\frac{v_0^2}{\\omega_c^2}. This equation describes a circular motion around the point x_0=x(0), y_0=y(0)+v_0/\\omega , where the characteristic frequency \\omega_c is called the cyclotron frequency. Intuition: \\frac{mv^2}{r} = evB (centripetal force = Lorentz force due to magnetic field). Due to the applied electric field \\bf E in the x -direction, the equations of motion acquire an extra term: m v_x' = -e(E_x + v_yB_z). Differentiating w.r.t. time leads to the same 2nd-order D.E. for v_x as above. However, for v_y we get v_y'' = -\\omega_c^2(v_d+v_y), where we defined v_d=\\frac{E_x}{B_z} . The general solutions are v_y(t) = c_1\\sin(\\omega_c t)+ c_2\\cos(\\omega_c t) -v_d \\\\ v_x(t) = c_3\\sin(\\omega_c t)+ c_4\\cos(\\omega_c t). Using the initial conditions v_x(0)=v_0 and v_y(0)=0 and the 1st order D.E. above, we can show v_y(t) = v_0\\sin(\\omega_c t)+ v_d\\cos(\\omega_c t) -v_d \\\\ v_x(t) = v_d\\sin(\\omega_c t)+ v_0\\cos(\\omega_c t). By integrating the expressions for the velocity we find: (x(t)-x_0)^2 + (y(t) - y_0 + v_d t))^2 = \\frac{v_0^2}{\\omega_c^2}. This represents a cycloid : a circular motion around a point that moves in the y -direction with velocity v_d=\\frac{E_x}{B_z} .","title":"Exercise 2: Motion of an electron in a magnetic and an electric field"},{"location":"solutions/solutions/#exercise-3-temperature-dependence-of-resistance-in-the-drude-model","text":"Find electron density from n_e = \\frac{Z n N_A}{W} where Z is valence of copper atom, n is density, N_A is Avogadro constant and W is atomic weight. Use \\rho = 1/\\sigma from the lecture notes to calculate scattering time. \\sigma = \\frac{n e^2 \\tau}{m} \\lambda = \\langle v \\rangle\\tau Scattering time \\tau \\propto \\frac{1}{\\sqrt{T}} ; \\rho \\propto \\sqrt{T} In general, \\rho \\propto T as the phonons in the system scales linearly with T (remember high temperature limit of Bose-Einstein factor becomes \\frac{kT}{\\hbar\\omega} leading to \\rho \\propto T ). Inability to explain this linear dependence is a failure of the Drude model.","title":"Exercise 3: Temperature dependence of resistance in the Drude model"},{"location":"solutions/solutions/#exercise-4-the-hall-conductivity-matrix-and-the-hall-coefficient","text":"\\rho_{xx} is independent of B and \\rho_{xy} \\propto B 2. \\sigma_{xx} = \\frac{\\rho_{xx}}{\\rho_{xx}^2 + \\rho_{xy}^2} \\sigma_{xy} = \\frac{-\\rho_{yx}}{\\rho_{xx}^2 + \\rho_{xy}^2} This describes a Lorentzian .","title":"Exercise 4: The Hall conductivity matrix and the Hall coefficient"},{"location":"solutions/solutions/#solutions-for-electrons-in-metals-ii-exercises","text":"","title":"Solutions for Electrons in metals II exercises"},{"location":"solutions/solutions/#warm-up-questions","text":"1. E = \\int \\limits_0^{\\infty} g(\\varepsilon) n_{F}(\\beta (\\varepsilon - \\mu)) \\varepsilon \\mathrm{d} \\varepsilon The electronic heat capacity C_e approaches 3N k_B . Thermal smearing is too significant and we can not accurately approximate the fraction of the excited electron with triangles anymore. Thus the Sommerfeld expansion breaks down. Electrons.","title":"Warm-up questions"},{"location":"solutions/solutions/#exercise-1-potassium","text":"Alkali metals mostly have a spherical Fermi surface. Their energy depends only on the magnitude of the Fermi wavevector. Refer to the lecture notes. Electrons are fermions and obey the Pauli exclusion principle. As electrons cannot occupy the same state, they are forced to occupy higher energy states resulting in high Fermi energy and high Fermi temperature. 4. n = \\frac{N}{V} = \\frac{1}{3 \\pi^{2} \\hbar^{3}}\\left(2 m \\varepsilon_{F}\\right)^{3 / 2} 5. n = \\frac{\\rho N_A Z}{M}, where \\rho is the density, N_A is the Avogadro's constant, M is molar mass and Z is the valence of potassium atom. Comparing total and free electron density, only few electrons are available for conduction which is roughly 1 free electron per potassium atom.","title":"Exercise 1: potassium"},{"location":"solutions/solutions/#exercise-2-a-hypothetical-material","text":"1. E = \\int_{0}^{\\infty}\\varepsilon g(\\varepsilon) n_{F}(\\beta (\\varepsilon - \\mu)) \\textrm{d} \\varepsilon = 2.10^{10}eV^{-\\frac{3}{2}} \\int_{0}^{\\infty}\\frac{\\varepsilon^{\\frac{3}{2}}}{e^\\frac{\\varepsilon-5.2}{k_BT}+1} \\textrm{d} \\varepsilon 2. E = \\frac{4}{5} (5.2)^{\\frac{5}{2}} 10^{10} eV 3. \\begin{align} E(T)-E(T=0) &= \\frac{\\pi^2}{6}(k_B T)^2\\frac{\\partial}{\\partial \\varepsilon}\\left(\\varepsilon g(\\varepsilon)\\right)\\bigg|_{\\varepsilon=\\varepsilon _F}\\\\ &\\approx 8.356 10^8 eV \\end{align} 5. C_v = 1.6713.10^6 eV/K 4, 6. mu = 5.2 kB = 8.617343e-5 T = 1000 #kelvin import numpy as np from scipy import integrate np . seterr ( over = 'ignore' ) # Fermi-Dirac distribution def f ( E , T ): return 1 / ( np . exp (( E - mu ) / ( kB * T )) + 1 ) # Density of states def g ( E ): return 2e10 * np . sqrt ( E ) #integration function def integral ( E , T ): return f ( E , T ) * g ( E ) * E ## Solve integral numerically using scipy's integrate dE = integrate . quad ( integral , 0 , 1e1 , args = ( T ))[ 0 ] - 0.8e10 * 5.2 ** ( 5. / 2 ) dT = 0.001 dEplus = integrate . quad ( integral , 0 , 1e1 , args = ( T + dT ))[ 0 ] - 0.8e10 * 5.2 ** ( 5. / 2 ) dEmin = integrate . quad ( integral , 0 , 1e1 , args = ( T - dT ))[ 0 ] - 0.8e10 * 5.2 ** ( 5. / 2 ) CV = ( dEplus - dEmin ) / ( 2 * dT ); print ( f 'dE = { dE : .4e } eV' ) print ( f 'Cv = { CV : .4e } eV/K' ) Check the source code written in python for solving integral using midpoint rule.","title":"Exercise 2: a hypothetical material"},{"location":"solutions/solutions/#exercise-3-graphene","text":"1. import numpy as np import matplotlib.pyplot as plt x = np . linspace ( - 1 , 1 , 100 ) fig , ax = plt . subplots ( figsize = ( 7 , 5 )) ax . plot ( x , x , 'b' ) ax . plot ( x , - x , 'b' ) ax . spines [ 'left' ] . set_position ( 'center' ) ax . spines [ 'bottom' ] . set_position (( 'data' , 0.0 )) # Eliminate upper and right axes ax . spines [ 'right' ] . set_color ( 'none' ) ax . spines [ 'top' ] . set_color ( 'none' ) ax . set_xticks ([]) ax . set_yticks ([]) ax . set_xlabel ( r '$\\mid \\vec k \\mid$' , fontsize = 14 ) ax . set_ylabel ( r '$\\varepsilon$' , fontsize = 18 , rotation = 'horizontal' ) ax . yaxis . set_label_coords ( 0.5 , 1 ) ax . xaxis . set_label_coords ( 1.0 , 0.49 ) 2.The DOS for the positive energies is given by g(\\varepsilon) = 2_s 2_v 2 \\pi \\left(\\frac{L}{2 \\pi}\\right)^2 \\frac{\\varepsilon}{c^2}, where 2_s is the spin degeneracy and 2_v is the valley degeneracy. If we account for the negative energies as well, we obtain g(\\varepsilon) = 2_s 2_v 2 \\pi \\left(\\frac{L}{2 \\pi}\\right)^2 \\frac{|\\varepsilon|}{c^2}. 3. g(\\varepsilon) vs \\varepsilon is a linear plot. Here, the region marked by -k_B T is a triangle whose area gives the number of electrons that can be excited: \\begin{align} n_{ex} &= \\frac{1}{2} g(-k_B T) k_B T\\\\ &= \\frac{L^2 k_B^2T^2}{\\pi c^2}. \\end{align} From this it follows that the energy difference is given by E(T) - E_0 = \\frac{L^2 k_B^3T^3}{\\pi c^2}. 4. C_v(T) = \\frac{\\partial E}{\\partial T} = \\frac{3L^2k_B^3T^2}{\\pi c^2}","title":"Exercise 3: graphene"},{"location":"solutions/solutions/#solutions-for-chemistry-101-exercises","text":"","title":"Solutions for Chemistry 101 exercises"},{"location":"solutions/solutions/#exercise-1-shell-filling-model-of-atoms","text":"See lecture notes. The atomic number of Tungsten is 74: 1\\textrm{s}^2 2\\textrm{s}^2 2\\textrm{p}^6 3\\textrm{s}^23p^64s^23d^{10}4p^65s^24d^{10}5p^66s^24f^{14}5d^4 \\begin{align} \\textrm{Cu} &= [\\textrm{Ar}]4s^23d^9\\\\ \\textrm{Pd} &= [\\textrm{Kr}]5s^24d^8\\\\ \\textrm{Ag} &= [\\textrm{Kr}]5s^24d^9\\\\ \\textrm{Au} &= [\\textrm{Xe}]6s^24f^{14}5d^9 \\end{align} The wheels fall off as V(\\mathbf{r}) \\ne 1/|r|","title":"Exercise 1: Shell-filling model of atoms"},{"location":"solutions/solutions/#exercise-2-application-of-the-lcao-model-to-the-delta-function-potential","text":"\\psi(x) = \\begin{cases} &\\sqrt{\u03ba}e^{\u03ba(x-x_1)}, x<x_1\\\\ &\\sqrt{\u03ba}e^{-\u03ba(x-x_1)}, x>x_1 \\end{cases} Where \u03ba = \\sqrt{\\frac{-2mE}{\u0127^2}} = \\frac{mV_0}{\u0127^2} . The energy is given by \u03f5_1 = \u03f5_2 = -\\frac{mV_0^2}{2\u0127^2} The wave function of a single delta peak is given by \\psi_1(x) = \\frac{\\sqrt{mV_0}}{\u0127}e^{-\\frac{mV_0}{\u0127^2}|x-x_1|} \\psi_2(x) can be found by replacing x_1 by x_2 H = -\\frac{mV_0^2}{\u0127^2}\\begin{pmatrix} 1/2+\\exp(-\\frac{2mV_0}{\u0127^2}|x_2-x_1|) & \\exp(-\\frac{mV_0}{\u0127^2}|x_2-x_1|)\\\\ \\exp(-\\frac{mV_0}{\u0127^2}|x_2-x_1|) & 1/2+\\exp(-\\frac{2mV_0}{\u0127^2}|x_2-x_1|) \\end{pmatrix} \u03f5_{\\pm} = \\beta(1/2+\\exp(-2\\alpha) \\pm \\exp(-\\alpha)) Where \\beta = -\\frac{mV_0^2}{\u0127^2} and \u03b1 = \\frac{mV_0}{\u0127^2}|x_2-x_1|","title":"Exercise 2: Application of the LCAO model to the delta-function potential"},{"location":"solutions/solutions/#exercise-3-polarization-of-a-hydrogen-molecule","text":"1. H_{\\mathcal{E}} = ex\\mathcal{E}, 2. \\hat{H} = \\begin{pmatrix} E_0 & -t\\\\ -t & E_0 \\end{pmatrix} +\\begin{pmatrix} \u27e81|ex\\mathcal{E}|1\u27e9 & \u27e81|ex\\mathcal{E}|2\u27e9\\\\ \u27e82|ex\\mathcal{E}|1\u27e9 & \u27e82|ex\\mathcal{E}|2\u27e9 \\end{pmatrix} = \\begin{pmatrix} E_0 - \\gamma & -t\\\\ -t & E_0 + \\gamma \\end{pmatrix}, where \\gamma = e d \\mathcal{E}/2 and have used \u27e81|ex\\mathcal{E}|1\u27e9 = -e d \\mathcal{E}/2\u27e81|1\u27e9 = -e d \\mathcal{E}/2 3. The eigenstates of the Hamiltonian are given by: E_{\\pm} = E_0\\pm\\sqrt{t^2+\\gamma^2} The ground state wave function is: \\begin{split} |\\psi\u27e9 &= \\frac{t}{\\sqrt{(\\gamma+\\sqrt{\\gamma^2+t^2})^2+t^2}}\\begin{pmatrix} \\frac{\\gamma+\\sqrt{t^2+\\gamma^2}}{t}\\\\ 1 \\end{pmatrix}\\\\ |\\psi\u27e9 &= \\frac{\\gamma+\\sqrt{t^2+\\gamma^2}}{\\sqrt{(\\gamma+\\sqrt{\\gamma^2+t^2})^2+t^2}}|1\u27e9+\\frac{t}{\\sqrt{(\\gamma+\\sqrt{\\gamma^2+t^2})^2+t^2}}|2\u27e9 \\end{split} 4. P = -\\frac{2\\gamma^2}{\\mathcal{E}}(\\frac{1}{\\sqrt{\\gamma^2+t^2}})","title":"Exercise 3: Polarization of a hydrogen molecule"}]}